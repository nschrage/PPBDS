---
output_yaml:
  - _output.yml
---


<!-- Outline: -->

<!-- 1) Preamble: Intro paragraph to classification in general, both 0/1 and multiple categories. Discuss different approaches, all for solving the same problem. -->

<!-- 2) EDA of cces (meaning just the columns we care about). Start with the usual EDA of the subset of cces data which we are using (only 2018 for now), which will be state, age, gender, race, education.   -->

<!-- 3) Logistic Regression. Go through all our themes. -->

<!-- 4) CART -->

<!-- 5) Random Forest -->

<!-- 6) Comparing the models. Advice. -->

<!-- 7) Multi-category question. Deep learning. -->

<!-- Other stuff: -->

<!-- * tidymodels always. stan_glm from the start. Cover the highlights on Gelman. Divide by 4. predictions are all matter. don't bother with posterior_linpred().  -->

<!-- * Then CART using tidymodels on exactly this data and this problem. Maybe we go slowly, showing how CART works with one variable (numeric and categorical) first. Or maybe we just go straight to the full model.  -->

<!-- * Then random forests using tidymodels on exactly this data and this problem. -->

<!-- * Do we have time to explore Gelman's magic trick: Showing the values of key coefficients when the model is refit to each year separately? That is, how much is age associated with being conservative over time? I hope so! But maybe not. -->

<!-- * Do we have time to do a model which predicts a category with more than 0/1 possible values? I am not sure. If we were going to, we would want to pick something that is not an ordered category, I think. In this data, only race would meet that criteria. Let's revisit this later. -->


<!-- MB: Using augment() for future predictions in this chapter? Works poorly with example -->


# Discrete Response {#discrete-response}

**Binary responses** take on only two values: success ($Y=1$) or failure ($Y=0$), yes ($Y=1$) or no ($Y=0$), et cetera. Binary responses are one of the most common types of data that statisticians encounter.  We are often interested in modeling the probability of success, $p$, based on a set of covariates. As with regression, there are two broad categories of problems: *modeling for prediction* and *modeling for causation*. Although terminology varies across fields, "regression" is generally used for situations in which our *dependent variable* is continuous, as in Chapter \@ref(continuous-response). "Classification" applies to cases in which the dependent variable takes on discrete values, the simplest of which is the binary case.

In this chapter, we will look at three common techniques of **classification** of binary data.  First, we will consider logistic regression, which is similar conceptually to the linear regression models we considered in Chapters \@ref(pitfalls) and \@ref(continuous-response).  Second, we will consider classification and regression trees (CART).  Third, we will discuss random forests. We use the **tidymodels** tools for all examples.

<!-- DK: Need to end with precise question. -->

## Exploratory Data Analysis (EDA)

Begin with our usual libraries:

```{r, message=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(PPBDS.data)
```


Before we start modeling, let's perform some exploratory analysis on the dataset we'll be working with, cces. Cces stands for the Cooperative Congressional Election Study, a study regarding the approval rating of individual voters to their sitting president. Each row captures one voter, some of their demographic information, and how highly they approve (or disaprprove) of the president. Let's first look at the raw data values by either looking at `cces` using RStudio's spreadsheet viewer or by using the `glimpse()` function from the **dplyr** package:

```{r}
glimpse(cces)
```

We will tweak the data by only looking at observations recorded in the year 2018 so that all the responses are about the same president. We will also drop all NA values in the dataset.

```{r}
ch12 <- cces %>%
  filter(year == 2018) %>%
  drop_na()
```

From this, we can gather that there are 16 variables. Notably, there are 60,000 observations even after filtering only for the year 2018. 

Let's also display a random sample of 5 rows of the 60,000 rows. We will be displaying these 5 random observations using a gt() table. Before that, though, we will select the variables that are currently of interest to us. 

```{r, eval = FALSE}
ch12 <- ch12 %>%
  select(state, age, gender, race, education, ideology, approval)

ch12 %>% 
  sample_n(size = 5)
```

```{r, echo = FALSE}
ch12 %>%
  sample_n(5) %>%
  gt() %>%
  cols_label(
    state = md("State"),
    age = md("Age"),
    gender = md("Gender"),
    race = md("Race"),
    education = md("Education"),
    ideology = md("Ideology"),
    approval = md("Approval of President")) %>%
  tab_source_note(md("A random sample of 5 out of the 60,000 voters"))
```

Now that we’ve looked at the raw values in `ch12`, let's make another tweak to the data. Because this chapter will be dealing with logistic regressions, we want to convert the numeric `approval` variable into a binary variable. `approval` is a numeric variable from 1-5 with 5 representing the highest approval of the president. In order to do this, we have to turn approval into a binary variable. 1-2 will be coded to 0 to signify disapproval and 3-5 will be coded to 1 for approval.

```{r}
ch12 <- ch12 %>%
  mutate(approval = case_when(
    approval == 1 ~ 0,
    approval == 2 ~ 0,
    approval == 3 ~ 1,
    approval == 4 ~ 1,
    approval == 5 ~ 1))
```

Now, let’s compute summary statistics. Let’s use the `skim()` function from the `skimr` package.

```{r}
ch12 %>% 
  skim()
```

You'll notice that we are missing data for our ideology and approval variables. The `complete_rate` column tells us that approval has 3% missing observations and ideology has 0.7% missing observations. 

To complete our exploratory data analysis, let's create some data visualizations. 

The primary response variable left in our dataset is `approval`, a (newly) binary variable with 0 representing disapproval of the President and 1 representing approval. So, let's start by looking at the overall distribution of `approval`.

```{r}
ch12 %>%
  ggplot(aes(x = approval)) +
  geom_bar(fill = "red") + 
  labs(y = "Count",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018") 
```

According to this graph, there are roughly 10,000 more voters who disapprove of Trump.
To make things more interesting, let's facet this approval variable by gender and then race.

```{r}
ch12 %>%
  ggplot(aes(x = approval, fill = gender)) +
  geom_bar() + 
  labs(y = "Count",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018 by Gender") +
  facet_wrap(~ gender) + 
  theme(legend.position = "none")
```

It seems that females have higher rates of disapproval. 

```{r}
ch12 %>%
  ggplot(aes(x = approval, fill = race)) +
  geom_bar() + 
  labs(y = "Count",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018 by Race") +
  facet_wrap(~ race) + 
  theme(legend.position = "none") +
  scale_y_log10()
```

Note the log10 scale on the y-axis; this was done so that we can see clearly the distribution for races that have very few observations compared to white individuals. We can see that slight the disapproving majority is present across most races.

Now, let's use our ideology variable. `ideology` has values "Very Liberal", "Liberal", "Moderate", "Conservative", "Very Conservative", and "Not Sure". Let's create a jitterplot with `approval` to see hoow approval varied across political affiliations.

```{r}
ch12 %>%
  ggplot(aes(x = ideology, y = approval)) + 
  geom_jitter() + 
  labs(y = "Presidential Approval",
       x = "Ideology",
       title = "Presidential Approval by Ideology")
```
As one would expect, approval is denser for conservative and very conservative voters while disapproval is denser for liberal and very liberal voters. 

## Logistic regression

### What is logistic regression?

Now that we know our dataset a little better, let's begin our first way of modelling binary/discrete data: logistic regressions. 

Figure \@ref(fig:OLSlogistic) illustrates a data set with a binary (0 or 1) response ($Y$) and a single continuous predictor ($X$).  The blue line is a linear regression to model the probability of a success ($Y=1$) for a given value of $X$. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. Probabilities can only range from 0 up to and including 1 as these represent a 0% and 100% chance of an event happening, respectively.

The red curve is the *logistic regression* curve.  Note that its characteristic "S" shape always produces predicted probabilities between 0 and 1.  Here is the formula for a logistic regression:

Where $p$ is the probability of a "yes" or "success" for a given set of predictors $X$.

<!-- Revisit nomenclature after chapter 5 -->

```{r, OLSlogistic, fig.align="center", out.width="60%", fig.cap='Linear vs. logistic regression models for binary response data.', echo=FALSE, warning=FALSE, message=FALSE}

set.seed(0)
dat <- tibble(x=runif(200, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(200, 1, p),
                  logit=log(p/(1-p)))

ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

<!-- DK: How does the math work here? log(p/1-p) seems, to me, to map 0,1 to 0,infinity. -->

The mathematical function $log\left(\frac{p}{1 - p}\right)$ is called the *logit function* and it transforms variables from the space $(0, 1)$ (like probabilities) to $(-\infty, \infty)$.  The inverse of that function, the *standard logistic function*, is $\frac{1}{1 + e^{-x}}$ and transforms variables from the space $(-\infty, \infty)$ to $(0, 1)$.  From that latter function's name we get the terminology of *logistic regression*.

### One categorical explanatory variable

Let's start our modeling by predicting `approval` with a single categorical explanatory variable. We'll start by modeling our new binary `approval` with the categorical variable `race`.  As we'll see, the syntax for running a logistic regression in R is very similar to that for running a linear regression.  In fact, we'll follow the same basic steps:

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `race_model`.
2. We get the regression table by applying the `tidy()` function from the **broom** package to `race_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

Note that the key difference is that instead of using `lm()`, we are now using `glm()`.  `glm()` operates very similarly to `lm()`, but it has an additional argument: `family`.  To run a logistic regression, we use `family = binomial`. This means that it will be modelled along the red line in Figure \@ref(fig:OLSlogistic) rather than the blue line.

Next, let's fit a model and `tidy()` it:

```{r, eval=FALSE}
race_model <- glm(approval ~ race, family = binomial, data = ch12)
race_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

```{r, echo=FALSE}
race_model <- glm(approval ~ race, family = binomial, data = ch12)
tidy(race_model) %>%
  select(term, estimate) %>%
  gt()

```
The intercept here is the omitted category, Asian.

How can we interpret the coefficients?  Unlike linear regressions, these coefficients aren't directly interpretable.  Recall our logistic regression model equation:

\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]

A one-unit change in $X$ thus is associated with a one-unit change in $log\left(\frac{p}{1 - p}\right)$, where $p$ is the predicted probability of success. It is hard to understand intuitively what this means. We can directly calculate all the possible values of $p$ this model by using the *standard logistic function*:

\[
p = \frac{1}{1 + e^{-(\beta_0+\beta_1X)}} 
\]

We can first use this formula to fill $b_0$ with the intercept (representing Asians) and omit the $b_1$ as we are solving for the probability of an Asian American approving of the President.


\[
p_{pres\_approve} = \frac{1}{1 + e^{-(-1.06)}} = 0.257
\]

We can then fill in the $b_1$ term to calculate the probabilities of all races:
- White: $\frac{1}{1 + e^{-(-1.06 + 0.99)}} = 0.482$
- Black: $\frac{1}{1 + e^{-(-1.06-1.01)}} = 0.112$
- Hispanic: $\frac{1}{1 + e^{-(-1.06 + 0.036)}} = 0.264$
- Middle Eastern: $\frac{1}{1 + e^{-(-1.06 + 0.007)}} = 0.259$
- Native American: $\frac{1}{1 + e^{-(-1.06 + 1.1)}} = 0.51$
- Mixed: $\frac{1}{1 + e^{-(-1.06 + 0.071)}} = 0.271$
- Other: $\frac{1}{1 + e^{-(-1.06 + 1.3)}} = 0.559$

<!-- We can then interpret the effect of moving from one category to another.  For example, the predicted probability of a White voter approving of the President is 0.225 greater than an Asian voter. Note that we could have obtained this through the *divide by 4 rule*:  $0.482 / 4 \approx 0.16$. -->
<!-- MB: Hold off until numeric variable for divide by 4?-->

However, there is a way to calculate these predicted probabilities using R without doing all of the math of the standard logistic function..

We have previously defined the following three concepts for a linear regression:

1. Observed values $y$, or the observed value of the outcome variable
2. Fitted values $\widehat{y}$, or the value on the regression line for a given $x$ value
3. Residuals $y - \widehat{y}$, or the error between the observed value and the fitted value

We obtained these values and other values using the `augment()` function from the **broom** package. Recall too that we used the `.se.fit` column to construct confidence intervals.  We'll see here how we can apply these same concepts to logistic regression.

<!-- MB: .se.fit into cooksd???-->
```{r, eval=FALSE}
regression_points <- race_model %>%
  augment() %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points
```

```{r, echo=FALSE}
regression_points <- augment(race_model) %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race,  .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 60,000 voters)"))
```

The syntax is the same, but the interpretation has to change, since the `.fitted`, `conf.low`, and `conf.high` columns are all on the logit scale.  While we could try to interpret these values, `augment()` has the argument `type.predict = "response"` that allow us to present the results in terms of *predicted probabilities*:

```{r, eval=FALSE}
regression_points <- race_model %>%
  augment(type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points
```

```{r, echo=FALSE}
regression_points <- augment(race_model,
                             type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 60,000 voters)"))
```

Now each of the `.fitted` values is a *predicted probability* of a Democratic victory from our model for a particular district and the confidence intervals are confidence intervals around that predicted probability. You'll notice how the fitted value in this table is the same as the probabilities we calculated by hand using the standard logistical function.

You may be wondering how to interpret the residuals.  The residuals reported by `augment()` for a logistic regression are called *deviance residuals*.  A deviance residual can be calculated for each observation using:

\[
\textrm{d}_i = 
\textrm{sign}(Y_i-\hat{p_i})\sqrt{-2 [ Y_i \text{log} \hat{p_i} + (1 - Y_i) \text{log} (1 - \hat{p_i}) ]}
\]

where $Y_i$ is the actual outcome and $p_i$ is the predicted probability from the logistic regression model.

The sum of the individual deviance residuals is referred to as the **deviance** or **residual deviance**. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred.

However, you can also have `augment()` report residuals as differences between the observed outcome and the predicted probabilities by using `type.residuals = "deviance"`:

<!-- type.residuals from response to deviance -->

```{r, eval=FALSE}
regression_points <- race_model %>%
  augment(type.predict = "response",
          type.residuals = "deviance") %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points
```

```{r, echo=FALSE}
regression_points <- augment(race_model,
                             type.predict = "response",
                             type.residuals = "deviance") %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 60,000 voters)"))
```

Now, the `.resid` value is the difference between the actual outcome (`approval`) and the predicted probability.

### One numerical explanatory variable

We'll now predict `approval` with a single numerical explanatory variable, `age`. 

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `age_model`.
2. We get the regression table by applying the `tidy()` function from the **broom** package to `age_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

```{r, eval=FALSE}
age_model <- glm(approval ~ age, family = binomial, data = ch12)
age_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

```{r, echo=FALSE}
age_model <- glm(approval ~ age, family = binomial, data = ch12)
tidy(age_model,
     conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  gt() %>%
  tab_source_note(md("Logistical Regression Table"))

```

How do we interpret the coefficients in this model?  Since the `age` coefficient is positive, that means that each additional year of age is associated with a higher approval of the President in 2018. 

If we wanted to learn the predicted probabilities for any given value of `age`, we can plug in our values of `age` into the standard logistic function, like so:

\[
p_{dem\_win} = \frac{1}{1 + e^{-(-1.43 + 0.023 \times year)}} 
\]

For example, the predicted probability of a 90-year-old approving of the President is 65.4% while the predicted probability of a 19-year-old approving of the President is 27%.

Note that since this is not a linear function, a one-unit change in `year` will be associated with various one-unit changes in `year`, depending on what `year` you are starting from.  Recall the figure we used to start the chapter:

A one-unit change in $X$ thus is associated with a one-unit change in $log\left(\frac{p}{1 - p}\right)$, where $p$ is the predicted probability of success. It is hard to understand intuitively what this means. We can directly calculate all the possible values of $p$ this model by using the *standard logistic function*:

```{r, echo = FALSE, message = FALSE}
ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

A linear regression line (in blue) has a constant slope, which means that no matter what $x$ you start with, the effect of going from $x$ to $x + 1$ on $y$ is the same number.  However, take a look at the logistic regression curve (in red).  The value of the slope for very high or very low values of $x$ is smaller (approaching 0 as $x$ tends to negative or positive infinity), while the slope in the middle of the curve is highest.  The steepest part of the curve corresponds to that part of the curve where the predicted probability equals 0.5.  That is, the effect of a one-unit change in $x$ is the highest when the predicted probability for that $x$ is close to 0.5 and smallest when the predicted probability for that $x$ is close to 0 or 1.

You can always use R to calculate the predicted probabilities for any value of $x$ and thus calculate the effect of moving from a particular $x$ to $x + 1$.  But this can get complicated.  In particular, once you start employing logistic regression with multiple predictors, the effect of a one-unit change in a predictor $x$ depends not only on $x$, but on the values of all the other predictors in your model!  You can always plug in all the coefficients and values of your predictors into the logistic function to calculate predicted probabilities, but if you don't do that, how can you interpret the coefficients?

Here is where we can use the *divide by 4 rule* that we discussed before.  A logistic regression coefficient divided by 4 is the effect of that variable at the steepest part of the logistic regression curve, which, as we just saw, corresponds to where the predicted probability is 0.5.

Therefore, you can divide a logistic regression coefficient by 4 to get an upper bound on the effect a one-unit change in that predictor will have on the predicted probability of your outcome.  In this case, the approximation tells us that each additional year of age is associated with about a $0.023 / 4 = 0.005$ increase in the predicted probability of a voter approving of the President.

While `race_model` and `age_model` both tell us something interesting, we could learn more with an *interaction model* that includes both of our predictors.

### One numerical and one categorical explanatory variable

We'll now predict `approval` with two variables, `race` and `age`, as well as the interaction between the two.

This time, we'll do it slightly differently. Rather than using `glm()` like we did in the last two examples, we'll instead be using tools from a package called **tidymodels**. Rather than having to use a different function each time we construct a model (choosing between lm(), glm(), and other modelling function), **tidymodels** streamlines the synthax for any model we want to construct.

```{r, message = FALSE}
library(tidymodels)
```

**tidymodels** includes many packages, but we'll start by showing how to use **parsnip** to fit a logistic regression. 

First, in the **tidymodels** workflow, we have to save the *model specification*.  We do that using two functions: `logistic_reg()` and `set_engine()`.

```{r}
logistic_mod <- logistic_reg() %>%
  set_engine("glm")
```

`logistic_reg()` says that we want to fit a logistic regression, and `set_engine("glm")` specifies that we want to do it using `glm()`.  Behind the scenes, **parsnip** uses many other packages to fit its models, but by unifying the syntax, it means that you don't have to memorize how a lot of different functions work.

Note that our new object, `logistic_mod`, doesn't contain our data or a formula.  In order actually to fit our model, we need to feed `logistic_mod` to a function called `fit()`.  `fit()` is the general purpose function in **parsnip** for fitting any model specification.  It takes as its first argument the model specification, but otherwise it operates similarly to `lm()` and `glm()`. We have to wrap `approval` in `factor()`, because `fit()` is more careful than `glm()` in requiring that classification models actually have categorical outcomes.

```{r}
logistic_fit <- fit(logistic_mod,
                    factor(approval) ~ race * age,
                    data = ch12)
```

One we have fit the model, how can we use it?  The `glm` object is still stored in `logistic_fit$fit`, so we can access that and use `tidy()`, just like we did before:

```{r}
logistic_fit$fit %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  gt()
```

As you can see, this generates the same results as when we used `glm()` directly. Now we can see how the effect of `age` varies by `race`. Looking at predicted probabilities of these numbers can put this model in perspective.  Let's use `augment()` to generate the predictions.  Remember that `type.predict = "response"` and `type.residuals = "deviance"` put the fitted values and the residuals on the probability scale.


<!-- MB: This doesn't work, yoou need to find a way to do it in tidymodels. It is a dataframe not a model -->

```{r, eval=FALSE}

interact_model <- logistic_fit$fit %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)

# regression_points <- interact_model %>%
#   augment(type.predict = "response",
#           type.residuals = "deviance") %>%
#   mutate(conf.low = .fitted - 2 * .cooksd,
#          conf.high = .fitted + 2 * .cooksd) %>%
#   select(approval, race, age, .fitted, conf.low, conf.high, .resid)
# regression_points

```

```{r, echo=FALSE}
# 
# regression_points <- augment(interact_model,
#                              type.predict = "response",
#                              type.residuals = "deviance") %>%
#   mutate(conf.low = .fitted - 2 * .cooksd,
#          conf.high = .fitted + 2 * .cooksd) %>%
#   select(approval, race, age, .fitted, conf.low, conf.high, .resid)
# regression_points %>%
#   slice(1:10) %>%
#   knitr::kable(
#     digits = 3,
#     caption = "Regression points (First 10 out of 60,000 voters)",
#     booktabs = TRUE,
#     linesep = ""
#   ) %>%
#   gt()

```

## Classification and regression trees (CART)

```{r, echo=FALSE}
img_path <- "images"
```

### What is CART?

We have learned how to fit models for binary responses using logistic regression.  However, logistic regression is just one of many methods we can use to model binary responses.  CART is another approach, which we'll learn about in this section.  In the next section, we'll learn about random forests.

<!-- Validity/model fitting the world. bias/variance  overfitting/underfitting. goal of the model -->

A **tree** is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as _nodes_. Decision trees predict an outcome variable $Y$ by *partitioning* the predictors.

Decision trees like this are often used in practice. For example, to decide on a person's risk of poor outcome after having a heart attack, doctors use the following:

```{r, echo=FALSE, out.width="50%"}
# source https://www.researchgate.net/profile/Douglas_Walton/publication/228297479/figure/fig1/AS:301828123185152@1448972840090/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png
#knitr::include_graphics(file.path(img_path,"Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png"))
```

(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184^[https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&mirid=1&type=2].)

Here, the binary outcome is whether a patient is "High Risk" or "Low Risk."  We have three predictors: minimum systolic blood pressure over the initial 24-hour period, age, and presence of sinus tachycardia.  The tree presents a series of yes or no questions that allow us to use the predictors to classify a patient's risk level.

**Classification trees**, or decision trees, are used in prediction problems where the outcome is categorical.  (When the outcome is numerical, they are called **regression trees**; hence the acronym **CART**, standing for Classification and Regression Trees.)  The general idea here is to build a decision tree and, at the end of each _node_, obtain a predictor $\hat{y}$. A mathematical way to describe this is to say that we are partitioning the *predictor space* into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$, and then for any predictor $x$ that falls within region $R_j$, we estimate $f(x)$ with the class that is the most common among the data within the partition for which the associated predictor $x_i$ is also in $R_j$.

But how do we decide on which partitions to make  ($R_1, R_2, \ldots, R_J$) and how do we choose $J$, the total number of partitions? Here is where the algorithm gets a bit complicated.

Classification trees create partitions recursively. We start the algorithm with one partition, the entire predictor space (i.e., every observation is classified as 0 or 1). But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. (We will describe how we decide when to stop later.)

Once we select a partition $\mathbf{x}$ to split in order to create the new partitions, we find a predictor $j$ and value $s$ that define two new partitions, which we will call $R_1(j,s)$ and $R_2(j,s)$, that split our observations in the current partition by asking if $x_j$ is bigger than $s$ (or if $x_j$ falls into a particular category $s$, if the predictor $j$ is categorical):

$$
R_1(j,s) = \{\mathbf{x} \mid x_j < s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
$$

Now, after we define the new partitions $R_1$ and $R_2$, and we decide to stop the partitioning process, we compute predictors by taking the most common category of all the observations $y$ for which the associated $\mathbf{x}$ is in $R_1$ and $R_2$. We refer to these two as $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ respectively. 

But how do we pick the predictor $j$ and the value $s$? One of the more popular ways for categorical data is the _Gini Index_.

In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The _Gini Index_ is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define $\hat{p}_{j,k}$ as the proportion of observations in partition $j$ that are of class $k$. The Gini Index is defined as 

$$
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
$$

If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above, since $\hat{p}_{j,k}(1-\hat{p}_{j,k}) = 0$ for all $k$.

Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region. 

But when do we stop partitioning?  Every time we split and define two new partitions, the Gini Index improves. This is because with more partitions, our model has more flexibility to adapt to our data.  However, our model may therefore perform worse when exposed to new data (this problem is called *overfitting*). This connects to our discussion of validity and models, as the conditions used to create the model will be too specific to accurately extrapolate to new data points. To avoid this, the algorithm sets a minimum for how much the Gini Index must improve for another partition to be added. This parameter is referred to as the _complexity parameter_ ($c_p$). The measure of fit must improve by a factor of $c_p$ for the new partition to be added. Large values of $c_p$ will therefore force the algorithm to stop earlier which results in fewer nodes.

<!-- MB: Use overfitting to discuss validity/model fitting the world-->

Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough).  Finally, they can model human decision processes. However, in terms of accuracy, they are rarely the best performing method since they are not very flexible. Random forests, explained in the next section, improve on some of the shortcomings of classification trees.

### One categorical explanatory variable


To create classification trees, we'll use the `decision_tree()` model specification and the `"rpart"` engine.  The syntax is very similar to when we used `logistic_reg()`.  Note that our binary response variable has to be a factor, just like with `logistic_reg()`. 

```{r}
library(parsnip)
tree_mod <- decision_tree() %>%
  set_engine("rpart",
             model = TRUE) %>%
  set_mode("classification")
```

(Note that we added the argument `model = TRUE` to `set_engine()`.  This saves the model frame, which we will need to avoid a warning when we plot the trees later.)

The function `set_mode()` wasn't necessary when we did logistic regression.  Here it clarifies that we want a *classification* tree rather than a *regression* tree.

Now that we have the object `tree_mod`, we can use `fit()` in the **parsnip** package.  We'll start by predicting `approval` with `region`:

```{r}
race_tree <- fit(tree_mod,
                         factor(approval) ~ race, 
                         data = ch12)
```

See how when using **tidymodels**, this is exactly the same as how we would fit a logistic regression, but with our model specification saved in `tree_mod` rather than the model specification we saved in `logistic_fit`.

What was the result of our tree?

```{r}
race_tree
```

It's not especially helpful to look at the results of a tree as text.  In order to visualize the tree, we'll use the `prp()` function in the **rpart.plot** package.  Remember that the model object is stored in `race_tree$fit`.

```{r, message = FALSE}
library(rpart.plot)

race_tree$fit %>%
  prp(extra = 6, varlen = 0, faclen = 0)
```

The arguments `varlen = 0` and `faclen = 0` ensure that the full variable names and factor levels are printed.  The argument `extra = 6` shows the proportion of "yes" outcomes within a given partition. Since we'll be using these same arguments throughout the chapter, we'll create a new function that calls `prp()` but with these options as defaults:

```{r}
prp_ch13 <- function(x, ...) prp(x, extra = 6, varlen = 0, faclen = 0, ...)

race_tree$fit %>%
  prp_ch13()
```

(Note that we added the argument `model = TRUE` to `set_engine()`.  This saves the model frame, which we will need to avoid a warning when we plot the trees later.)

The function `set_mode()` wasn't necessary when we did logistic regression.  Here it clarifies that we want a *classification* tree rather than a *regression* tree.

Now that we have the object `tree_mod`, we can use `fit()` in the **parsnip** package.  We'll start by predicting `approval` with `ideology` as our categorical right side variable. Note that our binary response variable has to be a factor, just like with `logistic_reg()`. 

```{r}
race_tree <- fit(tree_mod,
                 factor(approval) ~ ideology,
                 data = ch12)
```

See how when using **tidymodels**, this is exactly the same as how we would fit a logistic regression, but with our model specification saved in `tree_mod` rather than the model specification we saved in `logistic_fit`.

What was the result of our tree?

```{r}
race_tree
```

It's not helpful to look at the results of a tree as text.  In order to visualize the tree, we'll use the `prp()` function in the **rpart.plot** package.  Remember that the model object is stored in `race_tree$fit`.

```{r, message = FALSE}
library(rpart.plot)

race_tree$fit %>%
  prp(extra = 6, varlen = 0, faclen = 0)
```

The arguments `varlen = 0` and `faclen = 0` ensure that the full variable names and factor levels are printed.  The argument `extra = 6` shows the proportion of "yes" outcomes within a given partition. Since we'll be using these same arguments throughout the chapter, we'll create a new function that calls `prp()` but with these options as defaults:

```{r}
prp_ch12 <- function(x, ...) prp(x, extra = 6, varlen = 0, faclen = 0, ...)

race_tree$fit %>%
  prp_ch12()
```
So, how do we interpret this tree? We are given two partitions; the first is for voters that identify as very liberal, liberal moderate, or not sure, the second is for voters that identify as conservative or very conservative. The first partition classifies its observations as not approving of the President while the second partition classifies its observations as approving of the President.

The decimal numbers mean that 21% of observations in the first node approved of the President while 86% of observatins in the second node approved of the President.

The algorithm is very simple when you have one categorical explanatory variable. It just classifies every observation based on the most common response per category. Take a look at the following table:

```{r}
ch12 %>%
  select(ideology, approval) %>%
  group_by(ideology) %>%
  summarise(average_approval = sum(approval)/n()) %>%
  gt()
```
The two partitions just divide the observations by ideology into a) those ideologies where the majority approve of the President and b) those ideologies where the majority does not approve of the President.

### One numerical explanatory variable

Once we have created our model specification `tree_mod`, it is easy to use it to fit new models with different formulae and data.  We can use the same approach to create a classification tree predicting `approval` with `age`:

```{r}
age_tree <- fit(tree_mod,
                factor(approval) ~ age,
                data = ch12)

age_tree$fit %>%
  prp_ch12()
```

Now, the algorithm creates cutpoints in the `age` variable in order to classify observations.  This tree will classify observations younger than 44 or between 44 and 51 as disapproving of the president. It classifies observations older than 71 as approval of the president. The decimal within each box tells us how many observations within each node approved of the president.

<!-- MB: Complexity parameter -- how do i make this work for my example -->


## Random forests

### What are random forests?

Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.

The first step is _bootstrap aggregation_ or _bagging_. The general idea is to generate many predictors, each using classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees **randomly** different, and the combination of trees is the **forest**. The specific steps are as follows.

<!-- AR: do we explain training set/test set before this? -->

1\. Build $B$ decision trees using the training set. We refer to the fitted models as $T_1, T_2, \dots, T_B$. We later explain how we ensure they are different.

2\. For every observation in the test set, form a prediction $\hat{y}_j$ using tree $T_j$.

3\. For categorical data classification, predict $\hat{y}$ with majority vote (most frequent class among $\hat{y}_1, \dots, \hat{y}_T$).
     
So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let $N$ be the number of observations in the training set. To create $T_j, \, j=1,\ldots,B$ from the training set we do the following:

1\. Create a bootstrap training set by sampling $N$ observations from the training set **with replacement**. This is the first way to induce randomness. 
    
2\. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy. 

### Fitting random forests

<!-- MB: getting some error that I need some package installs for the engine randomForest but every package is installed -->

We will demonstrate by fitting a random forest to the House elections data, predicting `approval` with `state`, `gender`, and `age`.  We will use the `rand_forest()` function to create our model specification, setting the engine to `"randomForest"` and the mode to `"classification"`.

```{r, message=FALSE, warning=FALSE}
library(tidymodels)
library(parsnip)
#forest_mod <- rand_forest() %>%
#  set_engine("randomForest") %>%
#  set_mode("classification")

#approval_forest <- fit(forest_mod,
#                    factor(approval) ~ state + gender + age,
#                    data = ch12)

#approval_forest
```

We see under "OOB estimate of error rate" that this model has an error rate of 36% (or, looking at it the other way, an accuracy of 64%). We can see how the error rate of our algorithm changes as we add trees by looking at `house_forest$fit$err.rate[, "OOB"]`.  By default, `randomForest()` (the engine we specified) grows 500 trees.

```{r}
#tibble(`Error rate` = house_forest$fit$err.rate[, "OOB"],
#       Trees = 1:500) %>%
#  ggplot(aes(x = Trees, y = `Error rate`)) +
#  geom_line() +
#  theme_classic()
```

We can see that in this case, the accuracy improves as we add more trees until about 300 trees where accuracy stabilizes.

Random forests often perform better than other methods. However, a disadvantage of random forests is that we lose interpretability---we don't get anything like the coefficients from a logistic regression or the single tree from CART.

## Machine Learning

<!-- MB: insert whole section INTRO -->
For this chapter, we'll consider x possible models.  We have some intuitions about what should be in a useful model for presidential approval.  First, since the effect of all the predictors likely vary based on whether the president is a Democrat or Republican, all predictors should be interacted with `pres_gop`.  Also, party identification is likely a large predictor of presidential approval, and thus should be included.  But what about the other variables?  Let's consider the following combinations:

1. `race` alone
2. `race` and `age`
3. `state` plus demographic variables (`race`, `female`, `educ`, and `age`)
4. Same as above, but with all two-way interactions between the demographic variables

Of course, these are a small subset of the possible models we could consider, either with the variables we have selected or with the larger set of all the variables in the `cces`.  But we'll use these as examples for the machine learning techniques in this chapter; if you'd like, you can use the methods we learn here to test additional models.

Let's save these as `formula` objects in R, so we can easily access them later.  We'll start with the simplest model we'll consider, as `basic_form`:

```{r}
basic_form <- formula(approval ~ age * ideology)
```

Next, we can use `update()` to create the more complicated formulas.  `update()` takes as its first argument a formula and as its second argument the additions you want to make.  To keep all the predictors from the first formula and add more, you will start with `~ . + ` and then add more predictors, like so:

```{r}
ideo_form <- update(basic_form,
                    ~ age * ideology)

demo_form <- update(basic_form,
                    ~ . + race * gender + 
                      age * gender  + 
                      education * gender + 
                      state * gender)

demo_interact_form <- update(basic_form,
                             ~ . + race * education * gender + 
                               gender * age * race)
```

Since the last model is the same as `demo_interact_form` but with `ideo5 * pres_gop` added, we'll use `update(demo_interact_form)` rather than `update(basic_form)` here:

```{r}
#full_form <- update(demo_interact_form,
#                    ~ . + ideo5 * pres_gop)
```

Now we have five `formula` objects we can use to fit models.

So we can access them easily, we'll save them in a tibble and give them easy-to-remember names:

```{r}
#cces_formulas <- tibble(formula = c(basic_form,
#                                    ideo_form,
#                                    demo_form,
#                                    demo_interact_form,
#                                    full_form),
#                       group = c("Basic model",
#                                 "Ideology model",
#                                 "Demographic model",
#                                 "Demographic interaction model",
#                                 "Full model"))
```

## **parsnip**: build the model

This step is really three, using only the [**parsnip** package](https://tidymodels.github.io/parsnip/). In this setp, we can choose the *model*, the *engine* to run the model in R, and, for some models, the *mode*.  Here, our model will be linear regression, the engine `lm`, and the mode "regression" (the only possible mode for a linear regression).

```{r}
lm_spec <- 
  
  # Pick model
  
  linear_reg() %>%
  
  # Set engine
  
  set_engine("lm") %>%
  
  # Set mode
  
  set_mode("regression") 

lm_spec
```
