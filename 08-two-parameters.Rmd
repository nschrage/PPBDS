---
output_yaml:
  - output.yml
---


# Two Parameters {#two-parameters}

```{r, include=FALSE}
library(gt)
library(tufte)
```


<!-- Priorities: -->

<!-- 0) Add plots to EDA for trains. -->

<!-- 1) Fix Rubin Table. Show at least Actual Rubin Table. Show Reduced Rubin Table. -->

<!-- 2) Start on att_end section, including two Rubin Tables.  -->

<!-- Outline of the chapter -->

<!-- 0) Open with a question. -->

<!-- 1) EDA for trains -->

<!-- 2) Review of material from Chapter 7 -->

<!-- 3) age by party (modeling for prediction).  -->

<!-- 4) att_end by treatment (modeling for causation). No use of bootstrap. -->

<!-- 5) Models requiring the bootstrap. Ratio of average age of democrats versus republicans. -->

<!-- 6) Prediction. Use the models to answer your question. -->



We have now introduced almost all of the key themes in the book. The last five chapters are about increasing the complexity of the models we use. In this chapter, we estimate models which involve two parameters.

## EDA for `trains`

<!-- DK: Still need to standardize the method we use for providing references. -->

Let's do some exploratory data analysis (EDA) of the `trains` data set from the **PPBDS.data** package. Recall the discussion from Chapter \@ref(rubin-causal-model). Enos (2014) randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. Exposure to Spanish-speakers -- the `treatment` -- influenced attitudes toward immigration. These reactions were measured through changes in answers to three survey questions. Let's load the libraries we will need in this chapter and look at the data.

```{r, message=FALSE}
library(PPBDS.data)
library(tidyverse)
library(broom)
library(rsample)
library(skimr)
```

```{r}
glimpse(trains)
```

Here, we can see variables that indicate each respondent's gender, political affiliations, age, and income. Additionally, we have variables that indicate whether a subject was in the control or treatment group, and the their attitudes toward immigration both before or after the treatment. You can type `?trains` to read the help page for more information about each variable. Let's restrict attention to a subset of the variables.

```{r}
ch8 <- trains %>% 
  select(age, att_end, party, treatment)
```

It is always smart to look at a some random samples of the data:

```{r}
ch8 %>% 
  sample_n(5)
```

`att_end` is a measure of person's attitude toward immigration, a higher number means to a more conservative, i.e., a more exclusionary stance on immigration into the United States. Running `glimpse()` is another way of exploring a data set.

```{r}
ch8 %>% 
  glimpse()
```

Pay attention to the variable "type." Do they make sense? Perhaps. But there are certainly grounds for suspicion. Why are `age` and `att_end` doubles rather than integers? Looking at the values, it appears that they are not recorded more accurately than simple integers. Why is `party` a character variable and `treament` a factor variable? It could be that these are intentional choices made by the creator of the tibble, i.e., us. These could be mistakes. Or, most likely, these choices are a mixture of sensible and arbitrary. Regardless, it is your responsibility to notice them. You can't make a good model without looking closely at the data which you are using.

`skim` from the **skimr** package is the best way to get an overview of a tibble.

```{r}
ch8 %>% 
  skim()
```

`skim()` shows us what the different values of `treatment` are because it is a factor. Unfortunately, it does not do the same for character variables like `party`. The ranges for `age` and `att_end` seem reasonable. Recall that participants were asked three questions about immigration issues, each of which allowed for an answer which was a strength of agreement between 1 and 5, with higher values indicating more agreement with conservative viewpoints. So, the most liberal possible value is 3 and the most conservative is 15.

<!-- add a plot -->


## Review of One Parameter Estimation


<!--  Speak with Cass about adding a Rubin Table here. -->

Recall how we constructed bootstrap distributions for a parameter by sampling with replacement in Chapter \@ref(one-parameter). Through this bootstrapping process, we were able to construct *confidence intervals* for a parameter. Remember, confidence intervals are a tool to help us identify *parameter uncertainty*, which is the degree to which we are confident in these estimates of the *unknown* (and often unknowable) numbers. 

We will once again be using the `bootstraps()` function from the **rsample** package. You might think that since we're shifting to calculating two parameters of comparison, we might need to conduct the entire bootstrapping process twice, once for each parameter. However, it's much, much easier than that! We can actually calculate as many parameters as we need to for each replicate instead of doing a separate bootstrap for each parameter we want to track.

Let's review the material from  Chapter \@ref(one-parameter) by calculating the confidence interval for both the mean and the 90th percentile of age for Boston commuters. We will build up the code section by section.

First, we select out the only variable we need, with is `age`, and then create an objects of class `bootstraps`.


```{r}
trains %>% 
  select(age) %>% 
  bootstraps(1000)
```

We discussed these objects in Chapter \@ref(one-parameter). The key in working with them is the use of `analysis()` from the **rsample** package to pull out the underlying data.

```{r}
trains %>% 
  select(age) %>% 
  bootstraps(1000) %>% 
  mutate(boot = map(splits, ~ analysis(.)))
```

`boot` is now a list-column in the tibble. This workflow --- keep adding new columns to the tibble created by the initial call to `bootstraps()` --- is common. Let's add three more columns.

```{r}
trains %>% 
  select(age) %>% 
  bootstraps(1000) %>% 
  mutate(boot = map(splits, ~ analysis(.))) %>%
  mutate(ages = map(boot, ~ pull(., age))) %>% 
  mutate(age_mean = map_dbl(ages, ~ mean(.))) %>% 
  mutate(age_90th = map_dbl(ages, ~ quantile(., probs = 0.9)))
```

We could simplify this code by combining all the `mutate()` steps into a single line, create one function which does the `analyst()`, `pull()`, `mean()` and `quantile()` steps together. However, when building up code like this, we prefer to go line-by-line, at least until everything is working. We could also "feed" this pipe directly into a call to `ggplot()`. However, it is better practice to save the needed results in an object and then use that object for plotting, especially if creating the data takes some time. For replication purposes, we use `set.seed()`. 

```{r}
set.seed(9)

x <- trains %>% 
  select(age) %>% 
  bootstraps(1000) %>% 
  mutate(ages = map(splits, ~ analysis(.) %>% 
                              pull(., age))) %>%
  mutate(age_mean = map_dbl(ages, ~ mean(.))) %>% 
  mutate(age_90th = map_dbl(ages, ~ quantile(., probs = 0.9))) %>% 
  select(age_mean, age_90th)
```

We no longer need to re-run this code each time we adjust the plot.


```{r}
x %>% 
  ggplot() +
    geom_histogram(aes(x = age_mean), binwidth = .1, fill = "red") +
    geom_histogram(aes(x = age_90th), binwidth = .1, fill = "blue") +
    labs(x = "Estimate in Years",
        title = "Posterior Distribution for the Mean and the 90th Percentile of 
         Age for Boston Commuters")
```

Remember that some parameters, like the mean, are easy to calculate with built in functions like `lm()` while others require the use of the bootstrap. Note how similar the `lm()` solution is:

```{r}
trains %>% 
  lm(age ~ 1, data = .) %>%
  tidy(conf.int = TRUE) %>% 
  select(conf.low, estimate, conf.high)
```

The 95% confidence interval calculated via `lm()` is, more or less, the same as the one we calculate "by hand" using the bootstrap: `r round(quantile(x$age_mean, probs = c(0.025, 0.975)), 1)`.



<!-- DK: Should also add a "validity" or "representativeness" discussion about what population this estimate applies to. All people? All people in Boston? All Boston commuters? Do we need to get into prediction stuff? -->

## `age` as a function of `party`

<!-- No use of bootstrap. Make clear the distinction between "modeling for prediction" and "modeling for causation." The first is only about comparisons between categories. The second is stronger, a claim about changing an individual; it is a claim about potential outcomes. -->

But what if, instead of having no other information about our sample beyond their ages, we also knew their political party? How would we make a model which included this other information? This is a model for prediction, rather than a model for causal inference, because `age` can not be changed, regardless what happens to `party`. That is, someone's age is the same regardless of the party of which they are a member.

```{r}

trains %>%
  ggplot(aes(x = party, y = age)) + 
  geom_jitter(width = 0.2) + 
  labs(x = "Party",
       y = "Age",
       title = "Age by Party Affiliation in Trains Dataset")

```

The simplest model would be to guess that a person's age, conditional on their party membership, is equal to the mean age of people in that party.

```{r}
trains %>% 
  group_by(party) %>% 
  summarize(age_average = mean(age))
```

If a new Boston commuter named Jake showed up at the station, what would be a good guess as to their age? If you don't know anything else about this person, then guessing the mean age of the sample, `r round(mean(trains$age), 1)` makes sense, as we learned in Chapter \@ref(one-parameter). But, if we know that the person is a Republican, than this guess would be (slightly) too high. Among the population of Boston commuters, (it seems that) Republican's are younger than average. So, conditional on knowing that Jake is a Republican, we should guess that his age is  `r round(mean(trains$age[trains$party == "Republican"]), 1)`. 

However, instead of calculating these averages by hand, it is more convenient to use `lm()`, a function for creating linear statistical models. Start by estimating a simple model:

<!-- DK: Would now be a time to start using tidymodels syntax so that we could avoid the horribleness of the dot? -->

```{r}
trains %>% 
  lm(data = ., age ~ party - 1)
```

### Model Structure

The initials "lm" stand for "linear model." We are creating a model in which age is a function of party. The "-1" term causes the model to provide us with the mean for each group. The coefficient estimates are the same as the mean values for age within each party. 

There are three parts of understanding any model structure: whether it is causal or predictive, the mathematical formula, and realism. This model is predictive and has nothing to do with causality. Rather than modelling how the explanatory variable will change the outcome variable, we are using the explanatory variable (party) to simply predict the outcome variable (age). The difference between these two model types becomes clear when we construct a Rubin's Causal Model Table: 

```{r}

# Using real sampled ages but making my own tibble to add names

tibble(subject = c("Emily", "Dominic", "John", "Cassidy", "Miro"),
       age = c("34", "53", "22", "36", "30")) %>%
  gt() %>%
  cols_label(subject = md("Name"),
             age = md("Age")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  fmt_markdown(columns = TRUE)

```

This table is very simple, which is the point. There is no causal effect as we are simply trying to predict the immutable variable of age.

This mathematical formula will also help us better understand the model:
$$ y = b_0 + b_1 x $$
$$ y = (41.16) + (42.6 - 41.16) x $$
When the individual is a Republican (x=0), their age is predicted to be 41.16. When the individual is a Republican (x=1), their age is predicted to be 42.6. 

$$ y_i = b_0 + b_1 x_i + \epsilon_i $$
If you know the age and party affiliation of any single individual, you can solve for the epsilson, or the residual, of that observation. This is how much the individual's age varies from the prediction of their age in the model. The age of any single observation is known as the *fitted value*. 

The final aspect of model structure is realism. Does this model reflect the real world accurately?

<!-- More to say about realism? -->

### Parameter Uncertainty

How uncertain are our estimates for age? Again, we use the same tools we have used before:

```{r}
trains %>% 
  lm(data = ., age ~ party - 1) %>% 
  tidy(conf.int = TRUE) %>% 
  select(term, conf.low, estimate, conf.high)
```

The confidence intervals that we've generated display the upper and lower bounds of the mean age of Democrat and Republican commuters on this train platform. This touches on the idea of **parameter uncertainty**: even if a model is constructed in a logical way, we are unable to know our parameters exactly, the parameters in this case being the mean ages of the two parties. Confidence intervals are one way to capture this uncertainty, so that we don't over-confidently put forward one value rather than a range. 

```{r}
trains %>%
  group_by(party) %>%
  summarise(n())
```

You may notice how the range of the confidence interval for Republicans is larger. This may be because far fewer Republicans were surveyed than Democrats -- 19 to 96. Parameter estimates are always exclusively based on the data that the model is fed, so factors like sample size can impact our parameter uncertainty. With more data to work with, we can more accurately capture the true mean age of the Democrat commuters. 

### Unmodeled Variation

If a new Democrat entered the train station, how easy would it be to guess their age with our model? Could you confidently say that they would be within the Democrat interval bounds of 40 and 45?

$$ y_i = b_0 + b_1 x_i + \epsilon_i $$
One shouldn't be too confident in that prediction. The new Democrat's age comes in two parts: the beta (determined by their political party) and their epsilon/residual (how much noise there is in this observation that offsets it from the beta prediction). While the beta values have been determined by our model, the epsilon value represents *unmodeled variation* that could likely put the value outside of these bounds. 

<!-- DK: I went fast above, but that is all the key material. Still need to add a Rubin Table example, highlighting that this is modeling for prediction. There is only one column for Y because it is impossible to change someone's age, whatever manipulations you make to party. -->

<!-- DK: Is the below text from previous versions worth keeping? -->

<!-- In the original dataset — and therefore, most likely for the majority of the bootstrapped samples — the number of democratic subjects vastly outnumber the number of republican subjects. Why does this lead to wider confidence intervals? -->

<!-- Intuitively, we can think about the matter like this: since we have more data to work with for the democratic party, we are more confident that the mean income measurement from each bootstrapped sample is representative of the *true mean income of that sample* — remember, this "true value" is what we're trying to approximate through the bootstrapping process. This idea is more obvious on an extreme scale: if we were given one billion individuals' incomes, we'd feel more comfortable approximating the world's average income from that dataset than if we only had two individual's incomes to work with. These two individuals could be impoverished, or they could be billionaires, and we wouldn't reliably make any conclusions regarding world income based on those two individuals alone. Because there are far more democrats in the original sample than there are republicans, we are therefore more confident that the democrats' true average income is represented by the democratic subset of the data than we are of the republicans' true average income being accurately represented by the republican subset. -->

<!-- The math behind the confidence INTERVALS specifically is a little more complex, but the premise behind it is simple. First, recall that bootstrapping with replacement creates different assortments of the data due to the possibility of data points being omitted from each sample, or values being included more than once. If I was sampling with replacement from the list of 1, 2, 3, 4, 5, One replicate might include "1" three times, while another might include "1" zero times. -->

<!-- It is because of this possibility for omission/duplicate inclusion that subsets with small sample sizes are at the mercy of being misrepresented by sampling with replacement. In small sample sizes, individual points carry significant weight. As a result, we are more likely to get extreme values — values that are unreasonably small or large — the smaller the size of the subset. -->

<!-- As it relates to the `trains` dataset, the subset with the larger sample size, the democrat contingent, is less susceptible from this because each data point carries far less weight — each one gets drowned out by the many other democrats in the dataset, and removing or duplicating a single or even a few data points won't drastically shift measurements, such as mean and median, for most of the bootstrap samples. However, for the smaller republican subset, even a single omission or double inclusion of a data point through bootstrapping with replacement could generate extreme values for these measurements. -->


<!-- In short: If we have fewer data points, bootstrap parameter calculations are more likely to be further from the true values, and our confidence intervals will be wider as a result. It's important to understand the mechanics behind these relationships as we continue our work with confidence intervals going forward. -->


## Estimate other things

<!-- Often functions of these parameters, which are, after all, just functions of the bootstrap samples. Consider the difference in income between treated and control. Again, this is a case in which we know every number. There is no missing data. So, we can calculate exactly the average income for control and the average income for treated, as we did above. With those two parameters estimated, we can calculate the average difference between the two --- or the difference between the averages, which is the same thing because all this is linear. But what about the uncertainty of that difference? Again, the bootstrap comes to our rescue. What does the 95% confidence interval of the difference look like? Does it include zero? Would we expect it to? How does randomization of treatment/control figure in? -->

## Estimate trickier things

<!-- Explain how the above is a bit of a cheat since linear functions are special: the average difference is the same as the difference between the averages. This is an important point. But there are lots of things which are not like that! Consider the income ratio between the median treated and median control. This is something which, a priori, I might be as interested in as the difference in means. But the ratio of the medians is not the same thing as the median of the ratios! This is a key point. In order to calculate this, we need to, again, take the bootstrap and, in each bootstrap sample, calculate the median income of the treated, the median income of the controls, and record that ratio. We then save that ratio and look at its bootstrap distribution at the end of the exercise. -->

<!-- EM: I feel like having "Estimate other things" and "Estimate trickier things" are redundant as chapter sections because they are mostly addressed through the sections that sandwich them.  -->





## Using the predict() function

We will introduce a new function here, `predict()`, which takes in a model as input and, given a specified parameter, will predict the value and confidence intervals of any given data point:

```{r}

trains.lm <- lm(data = trains, income ~ party)
newdata <- tibble(party = c("Republican", "Democrat"))

predict(trains.lm, newdata, interval = "prediction")

```

Note that for both democrats and republicans, the prediction interval is significantly wider than the confidence interval. This discrepancy illustrates the *unmodeled variation* of the dataset: while the mean values, as indicated by the "fit" column, are still the best guess you can make for any given subject, we are less confident in the value range of that individual subject than we are of the value range of the mean. *The unmodeled variation widens the prediction intervals relative to the confidence intervals.*

Recall our discussion regarding the wider confidence intervals for republican subjects than democrat subjects (*would like to attach a link here that takes reader up to that part of the chapter*). Although the republican and democrat average incomes had vastly different confidence intervals — the republican convidence intervals were much wider — they have very similar prediction intervals. Although the *parameter uncertainty* of these two parameters were very different, the *unmodeled variation* for both parameters was roughly the same. Even though the data points were equally spread out for both democrat and republican income, were were able to more confidently pinpoint a true value for democrat income because we had more data to work with.

#### Validity

We just spent a ton of time analyzing the relationship between party and income in Enos' study. Although we have already accounted for parameter uncertainty (the confidence intervals) and unmodeled variation uncertainty (the prediction intervals), we have not accounted for the *validity* of his model — that is, whether the ideas he wishes to capture are truly represented by the data he collects.

For example, take the `income` variable. How do you answer this question if you, having made a fortune years ago, are currently earning an income of zero? How do you answer it if you are one-half of a married couple — is it both your incomes combined, your incomes divided by two, or some other calculation? Enos really means to distinguish between poorer and richer subjects through this measure, but the ambiguity in the question "what is your income" makes us doubt whether the income figures truly help us distinguish between these two types of people.

Another variable which, troublingly, raises concerns regarding validity is the `att_end` variable. Through this variable, Enos hopes to measure how people's political attitudes change, and the implications this could have for the political landscape of the United States— however, what he is truly measuring is how people respond to his specific survey questions, which required responses on a scale of 1-5 and were aggregated and scaled to a total out of 15. What if he asked people to respond on a scale of 1-3, or a scale of 1-30? We do not mean to say that his choices were unreasonable — rather, we think it is important to note that he would have gotten very different answers using different reasonable choices. **If you're running Trump's 2020 campaign, you don't care about how people filled out Enos' specific survey. You care about the statistics that data claims to represent**.

It is always important to keep the problem of validity in the back of our minds as we investigate data sets and how the data within these sets are collected. Given the many factors — those we are aware of and those we are not aware of — that go into any kind of data collection, there is no way to guarantee 100 percent validity.

#### Unknown unknowns

Yet another area of uncertainty that might not be immediately obvious is the possibiity of unknown unknowns — that is, all the things that might change with time. How do we know that our model — which relies heavily on figures like income and party affiliation — will still be able to draw conclusions about America 50 years from now? Mass migrations, political revolutions, and world wars are just a few of the things that could render all the conclusions we draw from the `trains` dataset moot. And, other than expanding our confidence intervals to the point of uselessness, there's no way to account for the unknown unknowns. 

In short, *while we can accept the model as representative of the United States at or around 2012 (the year in which Enos conducted his study), we cannot reasonably extrapolate that to the far future*.


It is always important to keep the problem of validity in the back of our minds as we investigate data sets and how the data within these sets are collected. Given the many factors — those we are aware of and those we are not aware of — that go into any kind of data collection, there is no way to guarantee 100 percent validity.



## `att_end` as a function of `treatment`

Let's establish the two types of models we've studied thus far. What we explored above was a predictive model: with someone's party affiliation, we can make a better guess as to what their age is. 

However, that's just about all we can do with this relationship. Recall that the trains dataset was originally introduced in the context of the Rubin Causal Model back in Chapter \@ref(rubin-causal-model). That model cannot be applied to the relationship between party and income because these values were independently decided outside the experiment — although we were able to identify correlations between those two variables, the combinations of party and income were predetermined and not randomly assigned in a way that allows us to establish a causal relationship.

If that's difficult to wrap your head around, here's big-idea reason for why we wouldn't look at the party-income relationship as causal: if an individual suddenly decided to change party affiliations — say, he or she decided to vote for Biden in the 2020 election despite having voted for Trump in 2016 — we wouldn't suddenly expect his or her income to, solely because of this change in party affiliation, spike or plummet. There are many other factors at play when determining these two variables that make identifying a causal relationship impossible.

In conclusion, while we can explore correlations or trends between party and income, we cannot label these correlations or trends as causal. However, `treatment` was a variable that was randomly assigned to subjects, and we have a variable that was measured after this random assignment, `att_end`. With this setup, we can calculate the Average Treatment Effect, or the average difference in `att_end` for treated subjects and control subjects. 

```{r}

mean_ends <- trains %>% 
  group_by(treatment) %>% 
  summarize(mean_att_end = mean(att_end))

```

```{r, include = FALSE}

ATE <- mean_ends$mean_att_end[2] - mean_ends$mean_att_end[1]

```

Here, we see that the average treatment effect is `r ATE`; this is the average change in att_end caused by treatment (if this statistic looks unfamiliar, refer back to chapter 3 for a refresher on ATE). But how confident are we in this ATE? We can use `lm()` again to generate confidence intervals for this parameter (note that, as mean is a linear measure, we can use `lm()` no problem!):

```{r}
#This is necessary so we can see the offset as caused by treatmentTreated, not treatmentControl.

trains$treatment <-fct_relevel(trains$treatment, "Control", "Treated")

lm(data = trains, att_end ~-1 + treatment) %>% 
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)

#To do: add row numbers, figure out a way to mutate columns as to include both values — maybe a function could be useful here?)

```

The confidence intervals represent our confidence in the average treatment effect of the `trains` dataset being representative of the true average treatment effect on Boston commuters.

Now, we'll create the RCM table of possible outcomes, as we did back in ch3:

<!-- EM: The Rubin tables below should be structurally different from the ones you constructed for age and party affiliation because age~party is merely predictive while att_end~treatment is CAUSAL. This explains there being TWO outcome columns for the att_end~treament model instead of only ONE outcome column, like we have for the age~party Rubin Table. Make sure that this distinction is clear — burn it into the reader's mind. -->

```{r}

#Adding question marks where necessary

trains_RCM <- trains %>% 
  pivot_wider(names_from = treatment, values_from = att_end) %>%
  slice(1:7) %>%
  mutate(subject = 1:7) %>%
  replace_na(list(Treated = "?", Control = "?")) %>% 
  select(subject, Control, Treated)

#Mathematical notation

trains_RCM %>%
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                Control = md("$$Y_c(u)$$"),
                Treated = md("$$Y_t(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header(title = "Att_end of First Ten Respondents of Train Dataset")


```

Recall that the Rubin Causal Model centers on filling in the unknowns, which are indicated by the "?"s in the table. There's one way we could fill in the missing values — we could use our approximations we generated with our `lm()` model: that is, for each unknown Treated value, we use 10, and for each unknown Control value, we use 8.45. This is certainly a reasonable prediction.

There's one way we could fill in the missing values — we could use our approximations we generated with our `lm()` model: that is, for each unknown Treated value, we use 10, and for each unknown Control value, we use 8.45. We also attach the uncertainty attached for these predictions.

```{r, echo = FALSE}

tibble(subject = 1:7,
       Control = c("8.45 (7.76-9.14)", "8.45 (7.76-9.14)", "8.45 (7.76-9.14)", "8.45 (7.76-9.14)", "5", "8.45 (7.76-9.14)", "13"),
       Treated = c("11", "10", "5", "11", "10 (9.2-10.8)", "13", "10 (9.2-10.8)")) %>% 
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                Control = md("$$Y_c(u)$$"),
                Treated = md("$$Y_t(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header(title = "Att_end of First Ten Respondents of Train Dataset")

```

Recall our earlier conversation regarding the difference between confidence intervals; an argument could be made that, taking individual variation into account, prediction intervals would be more accurate indicators of uncertainty than confidence intervals. We can construct the above table using prediction intervals instead, generated using the `predict()` function:

```{r}


lm(data = trains, att_end ~ treatment) %>%
predict(tibble(treatment = c("Treated", "Control")), 
        interval = "prediction")

```

And we can reconstruct the table by filling in the unknowns with the predictions (note that these are unchanged) and their associated levels of uncertainty (which HAVE changed to accomodate for individual variation within the data):

<!-- EM: Note that some of the uncertainty boundaries exceed 3 or 15, which were the minimum and maximum values for att_end, respectively. This should be made apparent in a margin note for the tables that have uncertainty bounds below 3 or above 15. -->

``` {r}

trains.lm <- lm(data = trains, att_end ~ treatment)
newdata <- tibble(treatment = c("Treated", "Control"))

predict(trains.lm, newdata, interval = "prediction")

```

And we can reconstruct the table by filling in the unknowns with the predictions (note that these are unchanged) and their associated levels of uncertainty (which HAVE changed to accomodate for individual variation within the data):

```{r, echo = FALSE}

tibble(subject = 1:7,
       Control = c("8.45 (2.9-14.0)", "8.45 (2.9-14.0)", "8.45 (2.9-14.0)", "8.45 (2.9-14.0)", "5", "8.45 (2.9-14.0)", "13"),
       Treated = c("11", "10", "5", "11", "10 (4.4-15.6)", "13", "10 (4.4-15.6)")) %>% 
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                Control = md("$$Y_c(u)$$"),
                Treated = md("$$Y_t(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header(title = "Att_end of First Ten Respondents of Train Dataset") %>%
  tab_footnote(footnote = "Note: minimum att_end is 3 and maximum att_end is 15",
               locations = cells_column_labels(
      columns = T))
```

Regardless of whether we use confidence or prediction intervals for these unknowns, however, the table looks a bit funky: take a look at subject 3. According to this model, subject 3's attitude under control would be HIGHER than his/her attitude under treatment, which corresponds to a negative treatment effect. This would mean that he/she would become more LIBERAL after undergoing treatment, which flies in the face of the ATE we calculated earlier. Another example of this can be seen with subject 7, whose predicted `att_end` under treatment is lower than his/her `att_end` under control (also signifying a shift towards liberal attitudes regarding immigration). 

Clearly, using the same control and treatment values to fill in the unknowns doesn't capture the entire picture. While using `lm()` to fill in the `att_end` unknowns towards the center of the distribution, it makes less sense for values towards the outskirts of the dataset. So, how can we predict outcomes in a way that equally addresses data towards the center of the distribution, and data that isn't?

One such way is by attaching the ATE in the appropriate direction to fill in the unknowns. Here, we use lm() to calculate confidence intervals for the ATE, which we previously calculated to be `r ATE`, and add and subtract those from the known values to fill in the unknown values:

```{r, echo = FALSE}

#This is necessary so we can see the offset as caused by treatmentTreated, not treatmentControl.

trains$treatment <- fct_relevel(trains$treatment, "Control", "Treated")

lm(data = trains, att_end ~ treatment) %>% 
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)


tibble(subject = 1:7,
       Control = c("9.45 (8.42, 10.48)", "8.45 (7.42, 9.48)", "3.45 (2.42, 4.48)", "9.45 (8.42, 10.48)", "5", "11.45 (10.42, 12.48)", "13"),
       Treated = c("11", "10", "5", "11", "6.55 (5.52, 7.58)", "13", "14.55 (13.52, 15.58)")) %>%
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                Control = md("$$Y_c(u)$$"),
                Treated = md("$$Y_t(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header(title = "Att_end of First Ten Respondents of Train Dataset")

```

<!-- EM: Unsure how to tie in probability -->

### Uncertainties

There are various sources of uncertainties and problems that have addressed in previous chapters and again in this chapter. It's important that you always have these in the back of your mind when working with data:

1) *Validity*. At the surface level, parameter names might not invoke too much thought — but once you really sit down and think about the story the numbers are trying to tell, it's not always the case that the numbers deserve to tell that story. In our earlier discussion regarding `income` and `att_end`, the conclusions we draw from these data might be misleading just because the way they were collected or computed inevitably change these conclusions. Even when there's nothing WRONG with a setup, we need to keep in mind that the smallest differences 

2) When we dive deeper into the data, we run into the issue of *parameter uncertainty*, which is the uncertainty associated with our analysis of summary parameters, such as mean, median, and range. In our above analysis with the `trains` dataset, we account for parameter uncertainty using confidence intervals.

3) As great as confidence intervals are, they don't capture the uncertainty of using summary statistics to predict parameter values of random individuals. We are more sure, for example, about the range of possible incomes for the mean income of American democrats than we are about the range of possible incomes for a single American democrat, whose income could stray far from the mean democrat income. This *unmodeled variation*, or variation between data points that is not fully captured by bootstrapping, deserves much consideration when dealing with data.

4) *Unknown unknowns* are an especially unavoidable source of uncertainty. The world is changing, and people 50 years from now might act differently and have vastly different lifestyles and characteristics; in the case of the `trains` dataset, it wouldn't be unreasonable to expect completely different combinations of income and party affiliation. That would render our mean income, median ratio, and prediction intervals for both those measures useless.

## Two parameter models which require the bootstrap


## Prediction

<!-- How do we connect this to the probability chapter? I am still thinking about this. Basic idea is that, what would we estimate would happen with a new experiment, which might include one or 10 or 1,000 new people? Just like coin-tossing from chapter 5. We need to make this connection very explicit. Read through chapter 5 and discuss.  Posterior distribution, given the data we have, of an unknown parameter.  -->

<!-- Should we make an explicit connection to the probability chapter by adding another dimension to that work? We have two coins, each with their own p. Build the joint distribution for them, with one on the x-axis and one on the y-axis. You need a near graphic for each value of the H number of heads total that you get from flipping them each N times. Then, you run the experiment, and you select the correct graphic. That shows you the best guess joint distribution. This is confusing!  -->

<!-- Do we want to play the prediction game? If so, how? -->

So far, we have not clearly discussed what we are using these models for. Why bother to create these models? What is their *purpose*? In many cases, the most important use of a model is for prediction. We use them to make predictions about data which we have not yet seen.



