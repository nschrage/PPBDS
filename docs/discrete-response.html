<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 12 Discrete Response | Gov 50: Data" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="davidkane9/PPBDS" />



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 12 Discrete Response | Gov 50: Data">

<title>Chapter 12 Discrete Response | Gov 50: Data</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Gov 50: Data<p><p class="author"></p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html"></a>
<a href="preamble.html">Preamble</a>
<a href="shopping-week.html">Shopping Week</a>
<a href="visualization.html"><span class="toc-section-number">1</span> Visualization</a>
<a href="wrangling.html"><span class="toc-section-number">2</span> Wrangling</a>
<a href="rubin-causal-model.html"><span class="toc-section-number">3</span> Rubin Causal Model</a>
<a href="functions.html"><span class="toc-section-number">4</span> Functions</a>
<a href="probability.html"><span class="toc-section-number">5</span> Probability</a>
<a href="one-parameter.html"><span class="toc-section-number">6</span> One Parameter</a>
<a href="two-parameters.html"><span class="toc-section-number">7</span> Two Parameters</a>
<a href="three-parameters.html"><span class="toc-section-number">8</span> Three Parameters</a>
<a href="n-parameters.html"><span class="toc-section-number">9</span> N Parameters</a>
<a href="pitfalls.html"><span class="toc-section-number">10</span> Pitfalls</a>
<a href="continuous-response.html"><span class="toc-section-number">11</span> Continuous Response</a>
<a id="active-page" href="discrete-response.html"><span class="toc-section-number">12</span> Discrete Response</a><ul class="toc-sections">
<li class="toc"><a href="#exploratory-data-analysis-eda"> Exploratory Data Analysis (EDA)</a></li>
<li class="toc"><a href="#the-question"> The Question</a></li>
<li class="toc"><a href="#logistic-regression"> Logistic regression</a></li>
<li class="toc"><a href="#classification-and-regression-trees-cart"> Classification and regression trees (CART)</a></li>
<li class="toc"><a href="#random-forests"> Random forests</a></li>
<li class="toc"><a href="#comparing-models"> Comparing Models</a></li>
<li class="toc"><a href="#NA"> The Question Pt. 2</a></li>
</ul>
<a href="appendices.html">Appendices</a>
<a href="tools.html">Tools</a>
<a href="shiny.html">Shiny</a>
<a href="maps.html">Maps</a>
<a href="animation.html">Animation</a>
<a href="references.html">References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="discrete-response" class="section level1">
<h1>
<span class="header-section-number">Chapter 12</span> Discrete Response</h1>
<p><strong>Binary responses</strong> take on only two values: success (<span class="math inline">\(Y=1\)</span>) or failure (<span class="math inline">\(Y=0\)</span>), yes (<span class="math inline">\(Y=1\)</span>) or no (<span class="math inline">\(Y=0\)</span>), et cetera. Binary responses are one of the most common types of data that statisticians encounter. We are often interested in modeling the probability of success, <span class="math inline">\(p\)</span>, based on a set of covariates. As with regression, there are two broad categories of problems: <em>modeling for prediction</em> and <em>modeling for causation</em>. Although terminology varies across fields, “regression” is generally used for situations in which our <em>dependent variable</em> is continuous, as in Chapter <a href="continuous-response.html#continuous-response">11</a>. “Classification” applies to cases in which the dependent variable takes on discrete values, the simplest of which is the binary case.</p>
<p>In this chapter, we will look at three common techniques of <strong>classification</strong> of binary data. First, we will consider logistic regression, which is similar conceptually to the linear regression models we considered in Chapters <a href="pitfalls.html#pitfalls">10</a> and <a href="continuous-response.html#continuous-response">11</a>. Second, we will consider classification and regression trees (CART). Third, we will discuss random forests. We use the <strong>tidymodels</strong> tools for all examples. At the end, we will compare the performances of all three models.</p>
<p>Which of these models will be the most accurate? Keep this question in mind as you read through this chapter.</p>
<div id="exploratory-data-analysis-eda" class="section level2">
<h2>
<span class="header-section-number">12.1</span> Exploratory Data Analysis (EDA)</h2>
<p>Begin with our usual libraries:</p>
<div class="sourceCode" id="cb1007"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1007-1"><a href="discrete-response.html#cb1007-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1007-2"><a href="discrete-response.html#cb1007-2"></a><span class="kw">library</span>(broom)</span>
<span id="cb1007-3"><a href="discrete-response.html#cb1007-3"></a><span class="kw">library</span>(skimr)</span>
<span id="cb1007-4"><a href="discrete-response.html#cb1007-4"></a><span class="kw">library</span>(PPBDS.data)</span>
<span id="cb1007-5"><a href="discrete-response.html#cb1007-5"></a><span class="kw">library</span>(tidymodels)</span></code></pre></div>
<p>Before we start modeling, let’s perform some exploratory analysis on the dataset we’ll be working with, cces. Cces stands for the Cooperative Congressional Election Study, a study regarding the approval rating of individual voters to their sitting president. Each row captures one voter, some of their demographic information, and how highly they approve (or disaprprove) of the president. Let’s first look at the raw data values by either looking at <code>cces</code> using RStudio’s spreadsheet viewer or by using the <code>glimpse()</code> function from the <strong>dplyr</strong> package:</p>
<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1008-1"><a href="discrete-response.html#cb1008-1"></a><span class="kw">glimpse</span>(cces)</span></code></pre></div>
<pre><code>## Rows: 452,755
## Columns: 12
## $ year        &lt;int&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 200…
## $ state       &lt;chr&gt; "North Carolina", "Ohio", "New Jersey", "Illinois", "New …
## $ gender      &lt;chr&gt; "Female", "Male", "Female", "Female", "Male", "Female", "…
## $ age         &lt;int&gt; 32, 49, 54, 34, 20, 27, 47, 20, 77, 19, 53, 55, 38, 72, 4…
## $ race        &lt;chr&gt; "White", "White", "White", "Black", "White", "White", "Wh…
## $ marstat     &lt;chr&gt; "Divorced", "Married", "Divorced", "Single / Never Marrie…
## $ ideology    &lt;fct&gt; Liberal, Moderate, Liberal, Liberal, Liberal, Liberal, Co…
## $ education   &lt;fct&gt; High School Graduate, Post-Grad, High School Graduate, 4-…
## $ news        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ econ        &lt;chr&gt; "Gotten Worse / Somewhat Worse", "Gotten Much Worse", "Go…
## $ approval_ch &lt;chr&gt; "Strongly Disapprove", "Strongly Disapprove", "Strongly D…
## $ approval    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 4, 1, 4, 5, 1, 1, 1, 3, 1, …</code></pre>
<p>We will tweak the data by only looking at observations recorded in the year 2018 so that all the responses are about the same president. We’ll also select the variables that are currently of interest to us. Finally, because this chapter will be dealing with logistic regressions, we want to convert the numeric <code>approval</code> variable into a binary variable. <code>approval</code> is a numeric variable from 1-5 with 5 representing the highest approval of the president. In order to do this, we have to turn approval into a binary variable. 1-2 will be coded to 0 to signify disapproval and 3-5 will be coded to 1 for approval. We will also cast <code>approval</code> as a factor variable rather than a number, which is useful information for models.</p>
<div class="sourceCode" id="cb1010"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1010-1"><a href="discrete-response.html#cb1010-1"></a>ch12 &lt;-<span class="st"> </span>cces <span class="op">%&gt;%</span></span>
<span id="cb1010-2"><a href="discrete-response.html#cb1010-2"></a><span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2018</span>) <span class="op">%&gt;%</span></span>
<span id="cb1010-3"><a href="discrete-response.html#cb1010-3"></a><span class="st">  </span><span class="kw">select</span>(state, age, gender, race, education, ideology, approval) <span class="op">%&gt;%</span></span>
<span id="cb1010-4"><a href="discrete-response.html#cb1010-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">as.factor</span>(<span class="kw">case_when</span>(</span>
<span id="cb1010-5"><a href="discrete-response.html#cb1010-5"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1010-6"><a href="discrete-response.html#cb1010-6"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">2</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1010-7"><a href="discrete-response.html#cb1010-7"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">3</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb1010-8"><a href="discrete-response.html#cb1010-8"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">4</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb1010-9"><a href="discrete-response.html#cb1010-9"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">5</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>)))</span></code></pre></div>
<p>From this, we can gather that there are 16 variables. Notably, there are 60,000 observations even after filtering only for the year 2018.</p>
<p>Let’s also display a random sample of 5 rows of the 60,000 rows.</p>
<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1011-1"><a href="discrete-response.html#cb1011-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1011-2"><a href="discrete-response.html#cb1011-2"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 7
##   state          age gender race     education    ideology     approval
##   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;fct&gt;        &lt;fct&gt;        &lt;fct&gt;   
## 1 Pennsylvania    27 Female White    No HS        Very Liberal 0       
## 2 Connecticut     43 Female Hispanic 4-Year       Very Liberal 0       
## 3 California      49 Male   White    Post-Grad    Liberal      0       
## 4 Florida         76 Female White    Some College Moderate     1       
## 5 Minnesota       24 Female White    4-Year       Not Sure     0</code></pre>
<p>Now, let’s compute summary statistics. Let’s use the <code>skim()</code> function from the <code>skimr</code> package.</p>
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="discrete-response.html#cb1013-1"></a>ch12 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1013-2"><a href="discrete-response.html#cb1013-2"></a><span class="st">  </span><span class="kw">skim</span>()</span></code></pre></div>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:unnamed-chunk-785">TABLE 12.1: </span>Data summary</span><!--</caption>--></p>
<table><tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">60000</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">7</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">factor</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody></table>
<p><strong>Variable type: character</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">state</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">20</td>
<td align="right">0</td>
<td align="right">51</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">gender</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">race</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">15</td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">education</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">6</td>
<td align="left">Hig: 16617, 4-Y: 14256, Som: 12631, Pos: 8316</td>
</tr>
<tr class="even">
<td align="left">ideology</td>
<td align="right">419</td>
<td align="right">0.99</td>
<td align="left">FALSE</td>
<td align="right">6</td>
<td align="left">Mod: 17302, Con: 12052, Lib: 10929, Ver: 7434</td>
</tr>
<tr class="odd">
<td align="left">approval</td>
<td align="right">33</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">0: 33743, 1: 26224</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">48</td>
<td align="right">18</td>
<td align="right">18</td>
<td align="right">32</td>
<td align="right">48</td>
<td align="right">62</td>
<td align="right">95</td>
<td align="left">▇▇▇▅▁</td>
</tr></tbody>
</table>
<p>You’ll notice that we are missing data for our ideology and approval variables. The <code>complete_rate</code> column tells us that approval has 3% missing observations and ideology has 0.7% missing observations. Let’s use the function <code>drop_na()</code> to get rid of these missing observations so they don’t interfere with our models later in the chapter:</p>
<div class="sourceCode" id="cb1014"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1014-1"><a href="discrete-response.html#cb1014-1"></a>ch12 &lt;-<span class="st"> </span>cces <span class="op">%&gt;%</span></span>
<span id="cb1014-2"><a href="discrete-response.html#cb1014-2"></a><span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2018</span>) <span class="op">%&gt;%</span></span>
<span id="cb1014-3"><a href="discrete-response.html#cb1014-3"></a><span class="st">  </span><span class="kw">select</span>(state, age, gender, race, education, ideology, approval) <span class="op">%&gt;%</span></span>
<span id="cb1014-4"><a href="discrete-response.html#cb1014-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">as.factor</span>(<span class="kw">case_when</span>(</span>
<span id="cb1014-5"><a href="discrete-response.html#cb1014-5"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1014-6"><a href="discrete-response.html#cb1014-6"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">2</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1014-7"><a href="discrete-response.html#cb1014-7"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">3</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb1014-8"><a href="discrete-response.html#cb1014-8"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">4</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb1014-9"><a href="discrete-response.html#cb1014-9"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">5</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1014-10"><a href="discrete-response.html#cb1014-10"></a><span class="st">  </span><span class="kw">drop_na</span>()</span></code></pre></div>
<p>To complete our exploratory data analysis, let’s create some data visualizations.</p>
<p>The primary response variable left in our dataset is <code>approval</code>, a (newly) binary variable with 0 representing disapproval of the President and 1 representing approval. So, let’s start by looking at the overall distribution of <code>approval</code>.</p>
<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1015-1"><a href="discrete-response.html#cb1015-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1015-2"><a href="discrete-response.html#cb1015-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> approval)) <span class="op">+</span></span>
<span id="cb1015-3"><a href="discrete-response.html#cb1015-3"></a><span class="st">    </span><span class="kw">geom_bar</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1015-4"><a href="discrete-response.html#cb1015-4"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb1015-5"><a href="discrete-response.html#cb1015-5"></a>         <span class="dt">x =</span> <span class="st">"Presidential Approval"</span>,</span>
<span id="cb1015-6"><a href="discrete-response.html#cb1015-6"></a>         <span class="dt">title =</span> <span class="st">"Presidential Approval in 2018"</span>) </span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-787-1.png" width="672"></p>
<p>According to this graph, there are roughly 10,000 more voters who disapprove of Trump. To make things more interesting, let’s look at <code>approval</code> across gender and then race.</p>
<div class="sourceCode" id="cb1016"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1016-1"><a href="discrete-response.html#cb1016-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1016-2"><a href="discrete-response.html#cb1016-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> approval, <span class="dt">fill =</span> gender)) <span class="op">+</span></span>
<span id="cb1016-3"><a href="discrete-response.html#cb1016-3"></a><span class="st">  </span><span class="kw">geom_bar</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1016-4"><a href="discrete-response.html#cb1016-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb1016-5"><a href="discrete-response.html#cb1016-5"></a>       <span class="dt">x =</span> <span class="st">"Presidential Approval"</span>,</span>
<span id="cb1016-6"><a href="discrete-response.html#cb1016-6"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval in 2018 by Gender"</span>) <span class="op">+</span></span>
<span id="cb1016-7"><a href="discrete-response.html#cb1016-7"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>gender) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1016-8"><a href="discrete-response.html#cb1016-8"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">"none"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-788-1.png" width="672"></p>
<p>It seems that females have higher rates of disapproval of the President than males have.</p>
<div class="sourceCode" id="cb1017"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1017-1"><a href="discrete-response.html#cb1017-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1017-2"><a href="discrete-response.html#cb1017-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> approval, <span class="dt">x =</span> race, <span class="dt">y =</span> age)) <span class="op">+</span></span>
<span id="cb1017-3"><a href="discrete-response.html#cb1017-3"></a><span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="st">"fill"</span>, <span class="dt">stat=</span><span class="st">"identity"</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1017-4"><a href="discrete-response.html#cb1017-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Percentage"</span>,</span>
<span id="cb1017-5"><a href="discrete-response.html#cb1017-5"></a>       <span class="dt">x =</span> <span class="st">"Presidential Approval"</span>,</span>
<span id="cb1017-6"><a href="discrete-response.html#cb1017-6"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval in 2018 by Race"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-789-1.png" width="672"></p>
<p>This segmented bar graph shows us the percentage of each race that approved of the president.We can see that the disapproving majority in the overall data is present across most races.</p>
<p>Now, let’s use our state variable. Let’s create a scatterplot with <code>approval</code> to see how the rate of approval varied across states.</p>
<div class="sourceCode" id="cb1018"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1018-1"><a href="discrete-response.html#cb1018-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1018-2"><a href="discrete-response.html#cb1018-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">as.integer</span>(approval)) <span class="op">%&gt;%</span></span>
<span id="cb1018-3"><a href="discrete-response.html#cb1018-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">case_when</span>(</span>
<span id="cb1018-4"><a href="discrete-response.html#cb1018-4"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1018-5"><a href="discrete-response.html#cb1018-5"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">2</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1018-6"><a href="discrete-response.html#cb1018-6"></a><span class="st">  </span><span class="kw">group_by</span>(state) <span class="op">%&gt;%</span></span>
<span id="cb1018-7"><a href="discrete-response.html#cb1018-7"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">avg_approval =</span> <span class="kw">sum</span>(approval)<span class="op">/</span><span class="kw">n</span>()) <span class="op">%&gt;%</span></span>
<span id="cb1018-8"><a href="discrete-response.html#cb1018-8"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> avg_approval, <span class="dt">y =</span> <span class="kw">reorder</span>(state, avg_approval))) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1018-9"><a href="discrete-response.html#cb1018-9"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1018-10"><a href="discrete-response.html#cb1018-10"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"State"</span>,</span>
<span id="cb1018-11"><a href="discrete-response.html#cb1018-11"></a>       <span class="dt">x =</span> <span class="st">"Approval Rate of President"</span>,</span>
<span id="cb1018-12"><a href="discrete-response.html#cb1018-12"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval by State"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-790-1.png" width="672"></p>
</div>
<div id="the-question" class="section level2">
<h2>
<span class="header-section-number">12.2</span> The Question</h2>
<p>Now that we’ve seen some EDA for the general dataset, let’s narrow our focus. When creating models, it is important to have a specific question in mind that you can use your model to answer. For our chapter, that question will be “How do Blacks and Hispanic voters vary in their support of the President?” At the end of this chapter, we’ll use the three models we created to address answer this. Before then, however, we’ll do some exploratory data analysis on this part of the data.</p>
<p>First, we’ll filter <code>race</code> to two groups: Black or Hispanic. We’ll also code approval to be an integer, which will help when doing the math for our graphs.</p>
<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1019-1"><a href="discrete-response.html#cb1019-1"></a>ch12 &lt;-<span class="st"> </span>ch12 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1019-2"><a href="discrete-response.html#cb1019-2"></a><span class="st">  </span><span class="kw">filter</span>(race <span class="op">==</span><span class="st"> "Black"</span> <span class="op">|</span><span class="st"> </span>race <span class="op">==</span><span class="st"> "Hispanic"</span>) <span class="op">%&gt;%</span></span>
<span id="cb1019-3"><a href="discrete-response.html#cb1019-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">as.integer</span>(approval)) <span class="op">%&gt;%</span></span>
<span id="cb1019-4"><a href="discrete-response.html#cb1019-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">case_when</span>(</span>
<span id="cb1019-5"><a href="discrete-response.html#cb1019-5"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1019-6"><a href="discrete-response.html#cb1019-6"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">2</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>))</span></code></pre></div>
<p>Now, let’s look at a basic comparison of their rates of approval.</p>
<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1020-1"><a href="discrete-response.html#cb1020-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1020-2"><a href="discrete-response.html#cb1020-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> approval, <span class="dt">fill =</span> race)) <span class="op">+</span></span>
<span id="cb1020-3"><a href="discrete-response.html#cb1020-3"></a><span class="st">  </span><span class="kw">geom_bar</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1020-4"><a href="discrete-response.html#cb1020-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb1020-5"><a href="discrete-response.html#cb1020-5"></a>       <span class="dt">x =</span> <span class="st">"Presidential Approval"</span>,</span>
<span id="cb1020-6"><a href="discrete-response.html#cb1020-6"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval in 2018 for Black and Hispanic voters"</span>) <span class="op">+</span></span>
<span id="cb1020-7"><a href="discrete-response.html#cb1020-7"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>race)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-792-1.png" width="672"></p>
<p>We can get two important pieces of information from this graph. The first observation is that, although it seems there are a few more black voters, these populations are very similar in size. The second is there is slightly stronger disapproval of the President among Black voters.</p>
<p>However, we must do further analyses to better understand Black and Hispanic voters. Looking at how different covariates affect approval rate within the group is a great next step. Let’s start with age:</p>
<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1021-1"><a href="discrete-response.html#cb1021-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1021-2"><a href="discrete-response.html#cb1021-2"></a><span class="st">  </span><span class="kw">group_by</span>(age, race) <span class="op">%&gt;%</span></span>
<span id="cb1021-3"><a href="discrete-response.html#cb1021-3"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">approval_rate =</span> <span class="kw">sum</span>(approval)<span class="op">/</span><span class="kw">n</span>()) <span class="op">%&gt;%</span></span>
<span id="cb1021-4"><a href="discrete-response.html#cb1021-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> approval_rate, <span class="dt">group =</span> race, <span class="dt">color =</span> race)) <span class="op">+</span></span>
<span id="cb1021-5"><a href="discrete-response.html#cb1021-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb1021-6"><a href="discrete-response.html#cb1021-6"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1021-7"><a href="discrete-response.html#cb1021-7"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Age"</span>,</span>
<span id="cb1021-8"><a href="discrete-response.html#cb1021-8"></a>       <span class="dt">y =</span> <span class="st">"Approval Rate"</span>,</span>
<span id="cb1021-9"><a href="discrete-response.html#cb1021-9"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval Rate by Age"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-793-1.png" width="672"></p>
<p>You can see that the difference in the approval ratings increases with age. Older Hispanic voters approve more strongly of the President while older Black voters approve of the President at even lower rates than younger Black voters. This graph shows us why covariates are important. Even adding just one – age – added a new complexity to our understanding of question at hand.</p>
<p>Now, let’s add another variable by throwing gender in the mix.</p>
<div class="sourceCode" id="cb1022"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1022-1"><a href="discrete-response.html#cb1022-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1022-2"><a href="discrete-response.html#cb1022-2"></a><span class="st">  </span><span class="kw">group_by</span>(age, race, gender) <span class="op">%&gt;%</span></span>
<span id="cb1022-3"><a href="discrete-response.html#cb1022-3"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">approval_rate =</span> <span class="kw">sum</span>(approval)<span class="op">/</span><span class="kw">n</span>()) <span class="op">%&gt;%</span></span>
<span id="cb1022-4"><a href="discrete-response.html#cb1022-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> approval_rate, <span class="dt">group =</span> race, <span class="dt">color =</span> race)) <span class="op">+</span></span>
<span id="cb1022-5"><a href="discrete-response.html#cb1022-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb1022-6"><a href="discrete-response.html#cb1022-6"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1022-7"><a href="discrete-response.html#cb1022-7"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>gender)<span class="op">+</span></span>
<span id="cb1022-8"><a href="discrete-response.html#cb1022-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Age"</span>,</span>
<span id="cb1022-9"><a href="discrete-response.html#cb1022-9"></a>       <span class="dt">y =</span> <span class="st">"Approval Rate"</span>,</span>
<span id="cb1022-10"><a href="discrete-response.html#cb1022-10"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval Rate by Age and Gender"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-794-1.png" width="672"></p>
<p>Now that we’ve faceted by gender, we can see that the trends from the previous graph don’t vary much from male to female.</p>
<div id="themes" class="section level3">
<h3>
<span class="header-section-number">12.2.1</span> Themes</h3>
<p>As stated at the beginning of the chapter, this chapter will look into three different types of models, all modelling presidential approval against several explanatory variables. Before we do so, let’s take a moment to discuss our themes of prudence, courage, temperance, and justice.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="other/images/prudence.jpg" alt=" " width="1280"><!--
<p class="caption marginnote">--><!--</p>--><!--</div>--></span>
</p>
<p><em>Prudence</em> addresses the matter of <em>validity</em>. Validity is the measure of how well our data captures the concept we are setting out to analyze and the relevance of the estimand for which we are modelling. So, is <code>approval</code> a good measure of what we are trying to estimate? The most obvious problem is that the <code>approval</code> variable began as a 5-point scale from “Strongly Disapprove” to “Strongly Approve”. Converting this to a binary variable of 0 and 1 may not be a valid measure of presidential approval – think to yourself, would our data be the same if we asked the exact same respondents to respond to a “Yes” or “No” question rather than transforming their data? Finally, does stating your approval or disapproval of the president on a survey capture what we are trying to study? Perhaps so, but in a practical application, it may be true that whether or not an individual donated money to the President’s campaign or if they voted for him in 2016 more accurately captures what we are setting out to study. Finally, are our right hand side variables valid? For example, the 5 point scale of <code>ideology</code> from “Strongly Liberal” to “Strongly Conservative” seems rather subjective.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="other/images/justice.jpg" alt=" " width="960"><!--
<p class="caption marginnote">--><!--</p>--><!--</div>--></span>
</p>
<p><em>Justice</em> deals with the theme of <em>model structure</em>. When establishing your model structure, there are a few steps. Firstly, will our model be predictive or causal? This chapter deals with passing our data through three different model types to predict presidential approval. We are not trying to measure the causal effect of any specific treatment variable, so these will be predictive models. Next, we must make a mathematical formula for our models. Because we are doing several different model types, our formula can be abstracted to…</p>
<p><span class="math display">\[y_i = f(x_{1i}, x_{2i}, ... )\]</span></p>
<p><span class="math inline">\(f()\)</span> represents the different models we will pass our data into, <span class="math inline">\(x_i\)</span> represents our data points, and <span class="math inline">\(y_i\)</span> represents <code>approval</code>. Finally, we will construct a preceptor table for this dataset.</p>
<p>Finally, as a part of model structure, let’s look at a preceptor table of our data.</p>
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#adqghggars .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#adqghggars .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#adqghggars .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#adqghggars .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#adqghggars .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#adqghggars .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#adqghggars .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#adqghggars .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#adqghggars .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#adqghggars .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#adqghggars .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#adqghggars .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#adqghggars .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#adqghggars .gt_from_md > :first-child {
  margin-top: 0;
}

#adqghggars .gt_from_md > :last-child {
  margin-bottom: 0;
}

#adqghggars .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#adqghggars .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#adqghggars .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#adqghggars .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#adqghggars .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#adqghggars .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#adqghggars .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#adqghggars .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#adqghggars .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#adqghggars .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#adqghggars .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#adqghggars .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#adqghggars .gt_left {
  text-align: left;
}

#adqghggars .gt_center {
  text-align: center;
}

#adqghggars .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#adqghggars .gt_font_normal {
  font-weight: normal;
}

#adqghggars .gt_font_bold {
  font-weight: bold;
}

#adqghggars .gt_font_italic {
  font-style: italic;
}

#adqghggars .gt_super {
  font-size: 65%;
}

#adqghggars .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="adqghggars" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><table class="gt_table">
<thead class="gt_header">
<tr>
<th colspan="2" class="gt_heading gt_title gt_font_normal" style>Preceptor Table of Approval</th>
    </tr>
<tr>
<th colspan="2" class="gt_heading gt_subtitle gt_font_normal gt_bottom_border" style></th>
    </tr>
</thead>
<thead class="gt_col_headings"><tr>
<th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1"><strong>ID</strong></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1"><strong>Approval</strong></th>
    </tr></thead>
<tbody class="gt_table_body">
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>1</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>0</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>2</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>1</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>...</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>...</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>10,052</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>?</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>...</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>...</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>120,892</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>0</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>20,400,100</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>0</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>...</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>...</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>200,627,000</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>?</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>...</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>...</p>
</div></td>
    </tr>
<tr>
<td class="gt_row gt_center" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;"><div class="gt_from_md">
<p>N</p>
</div></td>
      <td class="gt_row gt_center"><div class="gt_from_md">
<p>1</p>
</div></td>
    </tr>
</tbody>
</table></div>
<p>The purpose of a Preceptor Table is to capture all of the data such that if the question marks were filled out, one wouldn’t need to construct a model. The rows in this Preceptor Table represent the full population of N American voters that the cces sampled from for this dataset. For this reason, some have recorded data while other do not. If all of the “?”s were filled out, we would not need to make a predictive model for approval because we know everyone’s value for approval.</p>
<p>Our discussion of <em>courage</em>, or the <em>data-generating mechanism</em>, will be brief, as we will discuss our three model types in-depth throughout the chapter. We will begin with a logistical regression using <code>glm()</code>, like in previous chapters. Then we will use a regression tree, which creates partitions based on the right hand side variables and has a prediction of <code>approval</code> for each partition. Finally, we will use random forests, which are created by generating and averaging many different regression trees. Each of these models will use the binary variable of <code>approval</code> as their left hand variable.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="other/images/temperance.jpg" alt=" " width="960"><!--
<p class="caption marginnote">--><!--</p>--><!--</div>--></span>
</p>
<p>Finally, <em>temperance</em> (or <em>humility</em>). Because the world is extremely complex and ever-changing, our models are never as good as they appear. We will hold off on our full discussion of <em>temperance</em> until the end of the chapter when we’ve completed the models. Until then, keep in mind these questions: Is our model <em>representative</em> of the greater world? How accurately does our model predict <em>unknown unknowns</em>? What are the flaws in our modelling process?</p>
</div>
</div>
<div id="logistic-regression" class="section level2">
<h2>
<span class="header-section-number">12.3</span> Logistic regression</h2>
<div id="what-is-logistic-regression" class="section level3">
<h3>
<span class="header-section-number">12.3.1</span> What is logistic regression?</h3>
<p>Now that we know our dataset a little better, let’s begin our first way of modelling binary/discrete data: logistic regressions.</p>
<p>Figure <a href="discrete-response.html#fig:OLSlogistic">12.1</a> illustrates a data set with a binary (0 or 1) response (<span class="math inline">\(Y\)</span>) and a single continuous predictor (<span class="math inline">\(X\)</span>). The blue line is a linear regression to model the probability of a success (<span class="math inline">\(Y=1\)</span>) for a given value of <span class="math inline">\(X\)</span>. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. Probabilities can only range from 0 up to and including 1 as these represent a 0% and 100% chance of an event happening, respectively.</p>
<p>The red curve is the <em>logistic regression</em> curve. Note that its characteristic “S” shape always produces predicted probabilities between 0 and 1. Here is the formula for a logistic regression:</p>
<p>Where <span class="math inline">\(p\)</span> is the probability of a “yes” or “success” for a given set of predictors <span class="math inline">\(X\)</span>.</p>
<!-- Revisit nomenclature after chapter 5 -->
<div class="figure" style="text-align: center">
<span id="fig:OLSlogistic"></span>
<p class="caption marginnote shownote">
FIGURE 12.1: Linear vs. logistic regression models for binary response data.
</p>
<img src="book_temp_files/figure-html/OLSlogistic-1.png" alt="Linear vs. logistic regression models for binary response data." width="60%">
</div>
<!-- DK: How does the math work here? log(p/1-p) seems, to me, to map 0,1 to 0,infinity. -->
<p>The mathematical function <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span> is called the <em>logit function</em> and it transforms variables from the space <span class="math inline">\((0, 1)\)</span> (like probabilities) to <span class="math inline">\((-\infty, \infty)\)</span>. The inverse of that function, the <em>standard logistic function</em>, is <span class="math inline">\(\frac{1}{1 + e^{-x}}\)</span> and transforms variables from the space <span class="math inline">\((-\infty, \infty)\)</span> to <span class="math inline">\((0, 1)\)</span>. From that latter function’s name we get the terminology of <em>logistic regression</em>.</p>
</div>
<div id="one-categorical-explanatory-variable" class="section level3">
<h3>
<span class="header-section-number">12.3.2</span> One categorical explanatory variable</h3>
<p>Let’s start our modeling by predicting <code>approval</code> with a single categorical explanatory variable. We’ll start by modeling our binary <code>approval</code> with the categorical variable <code>race</code>. Like last chapter, we’ll be using tools from the <strong>tidymodels</strong> package. Rather than having to use a different function each time we construct a model (choosing between lm(), glm(), and other modelling function), <strong>tidymodels</strong> streamlines the synthax for any model we want to construct.</p>
<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1023-1"><a href="discrete-response.html#cb1023-1"></a><span class="kw">library</span>(tidymodels)</span></code></pre></div>
<p>First, in the <strong>tidymodels</strong> workflow, we have to save the <em>model specification</em>. We do that using two functions: <code>logistic_reg()</code> and <code>set_engine()</code>.</p>
<div class="sourceCode" id="cb1024"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1024-1"><a href="discrete-response.html#cb1024-1"></a>race_mod &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb1024-2"><a href="discrete-response.html#cb1024-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"glm"</span>)</span></code></pre></div>
<p><code>logistic_reg()</code> says that we want to fit a logistic regression, and <code>set_engine("glm")</code> specifies that we want to do it using <code>glm()</code>. By unifying the syntax, it means that you don’t have to memorize how a lot of different functions work to use different model types, from glm to random forests that we’ll use later in the chapter. <code>glm()</code> operates very similarly to <code>lm()</code>, but will instead model along the red line in Figure <a href="discrete-response.html#fig:OLSlogistic">12.1</a> rather than the blue line when using a logistic outcome variable.</p>
<p>Note that our new object, <code>race_mod</code>, doesn’t contain our data or a formula. In order actually to fit our model, we need to feed <code>race_mod</code> to a function called <code>fit()</code>. <code>fit()</code> takes as its first argument the model specification, but otherwise it operates similarly to <code>lm()</code> and <code>glm()</code>. We have to wrap <code>approval</code> in <code>factor()</code>, because <code>fit()</code> is more careful than <code>glm()</code> in requiring that classification models actually have categorical outcomes.</p>
<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1025-1"><a href="discrete-response.html#cb1025-1"></a>race_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(race_mod,</span>
<span id="cb1025-2"><a href="discrete-response.html#cb1025-2"></a>                    <span class="kw">factor</span>(approval) <span class="op">~</span><span class="st"> </span>race,</span>
<span id="cb1025-3"><a href="discrete-response.html#cb1025-3"></a>                    <span class="dt">data =</span> ch12)</span></code></pre></div>
<p>One we have fit the model, how can we use it? The <code>glm</code> object is still stored in <code>race_fit$fit</code>, so we can access that and use <code>tidy()</code>, just like we did before:</p>
<div class="sourceCode" id="cb1026"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1026-1"><a href="discrete-response.html#cb1026-1"></a>race_fit<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb1026-2"><a href="discrete-response.html#cb1026-2"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1026-3"><a href="discrete-response.html#cb1026-3"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high) <span class="op">%&gt;%</span></span>
<span id="cb1026-4"><a href="discrete-response.html#cb1026-4"></a><span class="st">  </span><span class="kw">gt</span>()</span></code></pre></div>
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#zmaszwdpqy .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#zmaszwdpqy .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#zmaszwdpqy .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#zmaszwdpqy .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#zmaszwdpqy .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#zmaszwdpqy .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#zmaszwdpqy .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#zmaszwdpqy .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#zmaszwdpqy .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#zmaszwdpqy .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#zmaszwdpqy .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#zmaszwdpqy .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#zmaszwdpqy .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#zmaszwdpqy .gt_from_md > :first-child {
  margin-top: 0;
}

#zmaszwdpqy .gt_from_md > :last-child {
  margin-bottom: 0;
}

#zmaszwdpqy .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#zmaszwdpqy .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#zmaszwdpqy .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#zmaszwdpqy .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#zmaszwdpqy .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#zmaszwdpqy .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#zmaszwdpqy .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#zmaszwdpqy .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#zmaszwdpqy .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#zmaszwdpqy .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#zmaszwdpqy .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#zmaszwdpqy .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#zmaszwdpqy .gt_left {
  text-align: left;
}

#zmaszwdpqy .gt_center {
  text-align: center;
}

#zmaszwdpqy .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#zmaszwdpqy .gt_font_normal {
  font-weight: normal;
}

#zmaszwdpqy .gt_font_bold {
  font-weight: bold;
}

#zmaszwdpqy .gt_font_italic {
  font-style: italic;
}

#zmaszwdpqy .gt_super {
  font-size: 65%;
}

#zmaszwdpqy .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="zmaszwdpqy" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><table class="gt_table">
<thead class="gt_col_headings"><tr>
<th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">term</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">estimate</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">conf.low</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">conf.high</th>
    </tr></thead>
<tbody class="gt_table_body">
<tr>
<td class="gt_row gt_left">(Intercept)</td>
      <td class="gt_row gt_right">-0.887</td>
      <td class="gt_row gt_right">-0.990</td>
      <td class="gt_row gt_right">-0.79</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceBlack</td>
      <td class="gt_row gt_right">-0.799</td>
      <td class="gt_row gt_right">-0.924</td>
      <td class="gt_row gt_right">-0.67</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceHispanic</td>
      <td class="gt_row gt_right">0.032</td>
      <td class="gt_row gt_right">-0.086</td>
      <td class="gt_row gt_right">0.15</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceMiddle Eastern</td>
      <td class="gt_row gt_right">0.086</td>
      <td class="gt_row gt_right">-0.337</td>
      <td class="gt_row gt_right">0.49</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceMixed</td>
      <td class="gt_row gt_right">0.045</td>
      <td class="gt_row gt_right">-0.107</td>
      <td class="gt_row gt_right">0.20</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceNative American</td>
      <td class="gt_row gt_right">1.037</td>
      <td class="gt_row gt_right">0.824</td>
      <td class="gt_row gt_right">1.25</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceOther</td>
      <td class="gt_row gt_right">1.209</td>
      <td class="gt_row gt_right">1.011</td>
      <td class="gt_row gt_right">1.41</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceWhite</td>
      <td class="gt_row gt_right">0.869</td>
      <td class="gt_row gt_right">0.765</td>
      <td class="gt_row gt_right">0.97</td>
    </tr>
</tbody>
</table></div>
<p>The intercept here is the omitted category, Asian.</p>
<p>How can we interpret the coefficients? Unlike linear regressions, these coefficients aren’t directly interpretable. Recall our logistic regression model equation:</p>
<p><span class="math display">\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]</span></p>
<p>A one-unit change in <span class="math inline">\(X\)</span> thus is associated with a one-unit change in <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span>, where <span class="math inline">\(p\)</span> is the predicted probability of success. It is hard to understand intuitively what this means. We can directly calculate all the possible values of <span class="math inline">\(p\)</span> this model by using the <em>standard logistic function</em>:</p>
<p><span class="math display">\[
p = \frac{1}{1 + e^{-(\beta_0+\beta_1X)}} 
\]</span></p>
<p>We can first use this formula to fill <span class="math inline">\(b_0\)</span> with the intercept (representing Asians) and omit the <span class="math inline">\(b_1\)</span> as we are solving for the probability of an Asian American approving of the President.</p>
<p><span class="math display">\[
p_{pres\_approve} = \frac{1}{1 + e^{-(-1.06)}} = 0.257
\]</span></p>
<p>We can then fill in the <span class="math inline">\(b_1\)</span> term to calculate the probabilities of all races:
- White: <span class="math inline">\(\frac{1}{1 + e^{-(-1.06 + 0.99)}} = 0.482\)</span>
- Black: <span class="math inline">\(\frac{1}{1 + e^{-(-1.06-1.01)}} = 0.112\)</span>
- Hispanic: <span class="math inline">\(\frac{1}{1 + e^{-(-1.06 + 0.036)}} = 0.264\)</span>
- Middle Eastern: <span class="math inline">\(\frac{1}{1 + e^{-(-1.06 + 0.007)}} = 0.259\)</span>
- Native American: <span class="math inline">\(\frac{1}{1 + e^{-(-1.06 + 1.1)}} = 0.51\)</span>
- Mixed: <span class="math inline">\(\frac{1}{1 + e^{-(-1.06 + 0.071)}} = 0.271\)</span>
- Other: <span class="math inline">\(\frac{1}{1 + e^{-(-1.06 + 1.3)}} = 0.559\)</span></p>
<!-- We can then interpret the effect of moving from one category to another.  For example, the predicted probability of a White voter approving of the President is 0.225 greater than an Asian voter. Note that we could have obtained this through the *divide by 4 rule*:  $0.482 / 4 \approx 0.16$. -->
<!-- MB: Hold off until numeric variable for divide by 4?-->
<p>However, there is a way to calculate these predicted probabilities using R without doing all of the math of the standard logistic function..</p>
<p>We have previously defined the following three concepts for a linear regression:</p>
<ol style="list-style-type: decimal">
<li>Observed values <span class="math inline">\(y\)</span>, or the observed value of the outcome variable</li>
<li>Fitted values <span class="math inline">\(\widehat{y}\)</span>, or the value on the regression line for a given <span class="math inline">\(x\)</span> value</li>
<li>Residuals <span class="math inline">\(y - \widehat{y}\)</span>, or the error between the observed value and the fitted value</li>
</ol>
<p>We obtained these values and other values using the <code>augment()</code> function from the <strong>broom</strong> package. Recall too that we used the <code>.se.fit</code> column to construct confidence intervals. We’ll see here how we can apply these same concepts to logistic regression.</p>
<!-- MB: This entire part has to be redone in tidymodels because augment doesn't work anymore

regression_points <- race_fit %>%
  augment() %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points


regression_points <- race_fit %>%
  augment(type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points


regression_points <- augment(race_fit,
                             type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 60,000 voters)"))


Now each of the `.fitted` values is a *predicted probability* of a Democratic victory from our model for a particular district and the confidence intervals are confidence intervals around that predicted probability. You'll notice how the fitted value in this table is the same as the probabilities we calculated by hand using the standard logistical function.

You may be wondering how to interpret the residuals.  The residuals reported by `augment()` for a logistic regression are called *deviance residuals*.  A deviance residual can be calculated for each observation using:

\[
\textrm{d}_i = 
\textrm{sign}(Y_i-\hat{p_i})\sqrt{-2 [ Y_i \text{log} \hat{p_i} + (1 - Y_i) \text{log} (1 - \hat{p_i}) ]}
\]

where $Y_i$ is the actual outcome and $p_i$ is the predicted probability from the logistic regression model.

The sum of the individual deviance residuals is referred to as the **deviance** or **residual deviance**. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred.

However, you can also have `augment()` report residuals as differences between the observed outcome and the predicted probabilities by using `type.residuals = "deviance"`:


regression_points <- race_fit %>%
  augment(type.predict = "response",
          type.residuals = "deviance") %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points

regression_points <- augment(race_fit,
                             type.predict = "response",
                             type.residuals = "deviance") %>%
  mutate(conf.low = .fitted - 2 * .cooksd,
         conf.high = .fitted + 2 * .cooksd) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 60,000 voters)"))


Now, the `.resid` value is the difference between the actual outcome (`approval`) and the predicted probability.

-->
</div>
<div id="one-numerical-explanatory-variable" class="section level3">
<h3>
<span class="header-section-number">12.3.3</span> One numerical explanatory variable</h3>
<p>We’ll now predict <code>approval</code> with a single numerical explanatory variable, <code>age</code> using <strong>tidymodels</strong>. We’ll begin, once again, by setting our engine to <code>glm</code>.</p>
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="discrete-response.html#cb1027-1"></a>age_mod &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb1027-2"><a href="discrete-response.html#cb1027-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"glm"</span>)</span></code></pre></div>
<p>Now, we’ll use <code>fit</code> to add our formula to the model.</p>
<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1028-1"><a href="discrete-response.html#cb1028-1"></a>age_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(age_mod,</span>
<span id="cb1028-2"><a href="discrete-response.html#cb1028-2"></a>               <span class="kw">factor</span>(approval) <span class="op">~</span><span class="st"> </span>age,</span>
<span id="cb1028-3"><a href="discrete-response.html#cb1028-3"></a>               <span class="dt">data =</span> ch12)</span></code></pre></div>
<p>We’ll use <code>tidy()</code> to view the <code>glm</code> object is still stored in <code>race_fit$fit</code>.</p>
<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1029-1"><a href="discrete-response.html#cb1029-1"></a>age_fit<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb1029-2"><a href="discrete-response.html#cb1029-2"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1029-3"><a href="discrete-response.html#cb1029-3"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high) <span class="op">%&gt;%</span></span>
<span id="cb1029-4"><a href="discrete-response.html#cb1029-4"></a><span class="st">  </span><span class="kw">gt</span>()</span></code></pre></div>
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#gftpbmcjfg .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#gftpbmcjfg .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#gftpbmcjfg .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#gftpbmcjfg .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#gftpbmcjfg .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#gftpbmcjfg .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#gftpbmcjfg .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#gftpbmcjfg .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#gftpbmcjfg .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#gftpbmcjfg .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#gftpbmcjfg .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#gftpbmcjfg .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#gftpbmcjfg .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#gftpbmcjfg .gt_from_md > :first-child {
  margin-top: 0;
}

#gftpbmcjfg .gt_from_md > :last-child {
  margin-bottom: 0;
}

#gftpbmcjfg .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#gftpbmcjfg .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#gftpbmcjfg .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#gftpbmcjfg .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#gftpbmcjfg .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#gftpbmcjfg .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#gftpbmcjfg .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#gftpbmcjfg .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#gftpbmcjfg .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#gftpbmcjfg .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#gftpbmcjfg .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#gftpbmcjfg .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#gftpbmcjfg .gt_left {
  text-align: left;
}

#gftpbmcjfg .gt_center {
  text-align: center;
}

#gftpbmcjfg .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#gftpbmcjfg .gt_font_normal {
  font-weight: normal;
}

#gftpbmcjfg .gt_font_bold {
  font-weight: bold;
}

#gftpbmcjfg .gt_font_italic {
  font-style: italic;
}

#gftpbmcjfg .gt_super {
  font-size: 65%;
}

#gftpbmcjfg .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="gftpbmcjfg" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><table class="gt_table">
<thead class="gt_col_headings"><tr>
<th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">term</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">estimate</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">conf.low</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">conf.high</th>
    </tr></thead>
<tbody class="gt_table_body">
<tr>
<td class="gt_row gt_left">(Intercept)</td>
      <td class="gt_row gt_right">-1.20</td>
      <td class="gt_row gt_right">-1.250</td>
      <td class="gt_row gt_right">-1.15</td>
    </tr>
<tr>
<td class="gt_row gt_left">age</td>
      <td class="gt_row gt_right">0.02</td>
      <td class="gt_row gt_right">0.019</td>
      <td class="gt_row gt_right">0.02</td>
    </tr>
</tbody>
</table></div>
<p>How do we interpret the coefficients in this model? Since the <code>age</code> coefficient is positive, that means that each additional year of age is associated with a higher approval of the President in 2018.</p>
<p>If we wanted to learn the predicted probabilities for any given value of <code>age</code>, we can plug in our values of <code>age</code> into the standard logistic function, like so:</p>
<p><span class="math display">\[
p_{dem\_win} = \frac{1}{1 + e^{-(-1.43 + 0.023 \times year)}} 
\]</span></p>
<p>For example, the predicted probability of a 90-year-old approving of the President is 65.4% while the predicted probability of a 19-year-old approving of the President is 27%.</p>
<p>Note that since this is not a linear function, a one-unit change in <code>year</code> will be associated with various one-unit changes in <code>year</code>, depending on what <code>year</code> you are starting from. Recall the figure we used to start the chapter:</p>
<p>A one-unit change in <span class="math inline">\(X\)</span> thus is associated with a one-unit change in <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span>, where <span class="math inline">\(p\)</span> is the predicted probability of success. It is hard to understand intuitively what this means. We can directly calculate all the possible values of <span class="math inline">\(p\)</span> this model by using the <em>standard logistic function</em>:</p>
<p><img src="book_temp_files/figure-html/unnamed-chunk-808-1.png" width="672"></p>
<p>A linear regression line (in blue) has a constant slope, which means that no matter what <span class="math inline">\(x\)</span> you start with, the effect of going from <span class="math inline">\(x\)</span> to <span class="math inline">\(x + 1\)</span> on <span class="math inline">\(y\)</span> is the same number. However, take a look at the logistic regression curve (in red). The value of the slope for very high or very low values of <span class="math inline">\(x\)</span> is smaller (approaching 0 as <span class="math inline">\(x\)</span> tends to negative or positive infinity), while the slope in the middle of the curve is highest. The steepest part of the curve corresponds to that part of the curve where the predicted probability equals 0.5. That is, the effect of a one-unit change in <span class="math inline">\(x\)</span> is the highest when the predicted probability for that <span class="math inline">\(x\)</span> is close to 0.5 and smallest when the predicted probability for that <span class="math inline">\(x\)</span> is close to 0 or 1.</p>
<p>You can always use R to calculate the predicted probabilities for any value of <span class="math inline">\(x\)</span> and thus calculate the effect of moving from a particular <span class="math inline">\(x\)</span> to <span class="math inline">\(x + 1\)</span>. But this can get complicated. In particular, once you start employing logistic regression with multiple predictors, the effect of a one-unit change in a predictor <span class="math inline">\(x\)</span> depends not only on <span class="math inline">\(x\)</span>, but on the values of all the other predictors in your model! You can always plug in all the coefficients and values of your predictors into the logistic function to calculate predicted probabilities, but if you don’t do that, how can you interpret the coefficients?</p>
<p>Here is where we can use the <em>divide by 4 rule</em> that we discussed before. A logistic regression coefficient divided by 4 is the effect of that variable at the steepest part of the logistic regression curve, which, as we just saw, corresponds to where the predicted probability is 0.5.</p>
<p>Therefore, you can divide a logistic regression coefficient by 4 to get an upper bound on the effect a one-unit change in that predictor will have on the predicted probability of your outcome. In this case, the approximation tells us that each additional year of age is associated with about a <span class="math inline">\(0.023 / 4 = 0.005\)</span> increase in the predicted probability of a voter approving of the President.</p>
<p>While <code>race_model</code> and <code>age_model</code> both tell us something interesting, we could learn more with an <em>interaction model</em> that includes multiple predictors.</p>
</div>
<div id="multiple-explanatory-variables" class="section level3">
<h3>
<span class="header-section-number">12.3.4</span> Multiple explanatory variables</h3>
<p>We’ll now predict <code>approval</code> with three variables, <code>race</code>, <code>age</code> and <code>gender</code>, as well as the interaction between the <code>race</code> and <code>age</code>.</p>
<p>This time, we’ll do it slightly differently. We’ll still be using <strong>tidymodels</strong>, but this time we will split the data into a testing set and training set. The training dataset will be used to create our model and the testing dataset will be used to test our final model’s performance. This is a typical part of the tidymodels workflow; it was omitted in our previous two examples because we were not testing the performance of either models. The <code>initial_split()</code> function reserves 25% of the data for testing by default.</p>
<div class="sourceCode" id="cb1030"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1030-1"><a href="discrete-response.html#cb1030-1"></a>log_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ch12)</span>
<span id="cb1030-2"><a href="discrete-response.html#cb1030-2"></a></span>
<span id="cb1030-3"><a href="discrete-response.html#cb1030-3"></a></span>
<span id="cb1030-4"><a href="discrete-response.html#cb1030-4"></a>log_training &lt;-<span class="st"> </span>log_split <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">training</span>()</span>
<span id="cb1030-5"><a href="discrete-response.html#cb1030-5"></a>log_testing &lt;-<span class="st"> </span>log_split <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">testing</span>()</span></code></pre></div>
<p>Then, let’s set the <em>model specification</em> to be a logistic regression using <code>glm</code>.</p>
<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1031-1"><a href="discrete-response.html#cb1031-1"></a>logistic_mod &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb1031-2"><a href="discrete-response.html#cb1031-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"glm"</span>)</span></code></pre></div>
<p>For our multi-variable logistic regression, we’ll be using 3 different explanatory variables, as well as the interaction between 2 of those variables, <code>race</code> and <code>age</code>. We will begin by defining the formula of our model. In our last two examples, we constructed our formulas directly in <code>fit()</code>. This time, we will do it seperately, as the formula will be used seperately later when cross-validating our results.</p>
<div class="sourceCode" id="cb1032"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1032-1"><a href="discrete-response.html#cb1032-1"></a>log_formula &lt;-<span class="st"> </span><span class="kw">formula</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>race <span class="op">+</span><span class="st"> </span>race<span class="op">:</span>age)</span></code></pre></div>
<p>Finally, let’s fit our model. <code>fit()</code> will take in our model specification <code>logistic_mod</code>, our formula, and our training set of data. We are reserving our testing set for later.</p>
<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1033-1"><a href="discrete-response.html#cb1033-1"></a>logistic_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(logistic_mod, log_formula, log_training)</span>
<span id="cb1033-2"><a href="discrete-response.html#cb1033-2"></a></span>
<span id="cb1033-3"><a href="discrete-response.html#cb1033-3"></a>logistic_fit<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb1033-4"><a href="discrete-response.html#cb1033-4"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1033-5"><a href="discrete-response.html#cb1033-5"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high) <span class="op">%&gt;%</span></span>
<span id="cb1033-6"><a href="discrete-response.html#cb1033-6"></a><span class="st">  </span><span class="kw">gt</span>()</span></code></pre></div>
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#tveibhrfhl .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#tveibhrfhl .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#tveibhrfhl .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#tveibhrfhl .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#tveibhrfhl .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#tveibhrfhl .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#tveibhrfhl .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#tveibhrfhl .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#tveibhrfhl .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#tveibhrfhl .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#tveibhrfhl .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#tveibhrfhl .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#tveibhrfhl .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#tveibhrfhl .gt_from_md > :first-child {
  margin-top: 0;
}

#tveibhrfhl .gt_from_md > :last-child {
  margin-bottom: 0;
}

#tveibhrfhl .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#tveibhrfhl .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#tveibhrfhl .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#tveibhrfhl .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#tveibhrfhl .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#tveibhrfhl .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#tveibhrfhl .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#tveibhrfhl .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#tveibhrfhl .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#tveibhrfhl .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#tveibhrfhl .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#tveibhrfhl .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#tveibhrfhl .gt_left {
  text-align: left;
}

#tveibhrfhl .gt_center {
  text-align: center;
}

#tveibhrfhl .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#tveibhrfhl .gt_font_normal {
  font-weight: normal;
}

#tveibhrfhl .gt_font_bold {
  font-weight: bold;
}

#tveibhrfhl .gt_font_italic {
  font-style: italic;
}

#tveibhrfhl .gt_super {
  font-size: 65%;
}

#tveibhrfhl .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="tveibhrfhl" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><table class="gt_table">
<thead class="gt_col_headings"><tr>
<th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">term</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">estimate</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">conf.low</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">conf.high</th>
    </tr></thead>
<tbody class="gt_table_body">
<tr>
<td class="gt_row gt_left">(Intercept)</td>
      <td class="gt_row gt_right">-1.7485</td>
      <td class="gt_row gt_right">-2.085</td>
      <td class="gt_row gt_right">-1.4169</td>
    </tr>
<tr>
<td class="gt_row gt_left">age</td>
      <td class="gt_row gt_right">0.0205</td>
      <td class="gt_row gt_right">0.012</td>
      <td class="gt_row gt_right">0.0291</td>
    </tr>
<tr>
<td class="gt_row gt_left">genderMale</td>
      <td class="gt_row gt_right">0.2867</td>
      <td class="gt_row gt_right">0.247</td>
      <td class="gt_row gt_right">0.3262</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceBlack</td>
      <td class="gt_row gt_right">0.7170</td>
      <td class="gt_row gt_right">0.304</td>
      <td class="gt_row gt_right">1.1322</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceHispanic</td>
      <td class="gt_row gt_right">0.1678</td>
      <td class="gt_row gt_right">-0.225</td>
      <td class="gt_row gt_right">0.5632</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceMiddle Eastern</td>
      <td class="gt_row gt_right">0.3929</td>
      <td class="gt_row gt_right">-0.754</td>
      <td class="gt_row gt_right">1.4938</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceMixed</td>
      <td class="gt_row gt_right">0.0272</td>
      <td class="gt_row gt_right">-0.464</td>
      <td class="gt_row gt_right">0.5166</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceNative American</td>
      <td class="gt_row gt_right">0.5718</td>
      <td class="gt_row gt_right">-0.227</td>
      <td class="gt_row gt_right">1.3553</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceOther</td>
      <td class="gt_row gt_right">1.2824</td>
      <td class="gt_row gt_right">0.548</td>
      <td class="gt_row gt_right">2.0154</td>
    </tr>
<tr>
<td class="gt_row gt_left">raceWhite</td>
      <td class="gt_row gt_right">0.8079</td>
      <td class="gt_row gt_right">0.470</td>
      <td class="gt_row gt_right">1.1503</td>
    </tr>
<tr>
<td class="gt_row gt_left">age:raceBlack</td>
      <td class="gt_row gt_right">-0.0393</td>
      <td class="gt_row gt_right">-0.050</td>
      <td class="gt_row gt_right">-0.0289</td>
    </tr>
<tr>
<td class="gt_row gt_left">age:raceHispanic</td>
      <td class="gt_row gt_right">-0.0052</td>
      <td class="gt_row gt_right">-0.015</td>
      <td class="gt_row gt_right">0.0048</td>
    </tr>
<tr>
<td class="gt_row gt_left">age:raceMiddle Eastern</td>
      <td class="gt_row gt_right">-0.0095</td>
      <td class="gt_row gt_right">-0.037</td>
      <td class="gt_row gt_right">0.0175</td>
    </tr>
<tr>
<td class="gt_row gt_left">age:raceMixed</td>
      <td class="gt_row gt_right">-0.0021</td>
      <td class="gt_row gt_right">-0.014</td>
      <td class="gt_row gt_right">0.0097</td>
    </tr>
<tr>
<td class="gt_row gt_left">age:raceNative American</td>
      <td class="gt_row gt_right">0.0048</td>
      <td class="gt_row gt_right">-0.012</td>
      <td class="gt_row gt_right">0.0222</td>
    </tr>
<tr>
<td class="gt_row gt_left">age:raceOther</td>
      <td class="gt_row gt_right">-0.0063</td>
      <td class="gt_row gt_right">-0.021</td>
      <td class="gt_row gt_right">0.0084</td>
    </tr>
<tr>
<td class="gt_row gt_left">age:raceWhite</td>
      <td class="gt_row gt_right">-0.0047</td>
      <td class="gt_row gt_right">-0.013</td>
      <td class="gt_row gt_right">0.0040</td>
    </tr>
</tbody>
</table></div>
<p>This table may at first seem a bit intimidating, but once we understand how to use the coefficients, the model becomes more simple to interpret.</p>
<p>You may notice that the coefficient associated with each race and age has changed significantly from <code>race_model</code> and <code>age_model</code>. The reason these numbers are significantly different is because our model now incorporates gender, age, and how age changes by gender. An observation’s fitted value is now the sum of their race’s coefficient, whether or not they are a male, and the age coefficient corresponding to their race. For example, the fitted value of a 21-year-old white man is addition of raceWhite + the genderMale + 21 * raceWhite:age. The fitted value of an 45-year-old Asian woman is the addition of (Intercept) + 45 * age; this model uses Asian as the default race, which is why we use the (Intercept) and age coefficients.</p>
</div>
<div id="k-fold-cross-validation" class="section level3">
<h3>
<span class="header-section-number">12.3.5</span> K-Fold Cross-Validation</h3>
<p>We can check whether the model we created is accurate by using cross-validation. Recall that when we first started constructing this model, we used <code>initial_split()</code> to reserves 25% of the data for testing and 75% of the data for creating the model. By doing so, the model avoids the issue of overtraining.</p>
<p>The function <code>vfold_cv()</code> splits your training set into <code>v</code> smaller sections. We will be using <code>v = 5</code> so that the small, individual sections are still large enough to train the model. The first four folds will be used to train the model, and the last fold is always used to assess the accuracy of the model.</p>
<div class="sourceCode" id="cb1034"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1034-1"><a href="discrete-response.html#cb1034-1"></a>log_folds &lt;-<span class="st"> </span>log_training <span class="op">%&gt;%</span></span>
<span id="cb1034-2"><a href="discrete-response.html#cb1034-2"></a><span class="st">  </span><span class="kw">vfold_cv</span>(<span class="dt">v =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>Now, we create the recipe for our cross-validation. A recipe is the same thing as a formula, except it includes the dataset that the formula is enacted upon (<code>log_training</code> in our case).</p>
<div class="sourceCode" id="cb1035"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1035-1"><a href="discrete-response.html#cb1035-1"></a>log_recipe &lt;-<span class="st"> </span><span class="kw">recipe</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>race, <span class="dt">data =</span> log_training) </span></code></pre></div>
<p>You’ll notice that our interaction term is missing from this recipe. In order to add interaction terms to a recipe, you must take the additional step of using <code>step_interact</code>.</p>
<div class="sourceCode" id="cb1036"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1036-1"><a href="discrete-response.html#cb1036-1"></a>log_recipe &lt;-<span class="st"> </span>log_recipe <span class="op">%&gt;%</span></span>
<span id="cb1036-2"><a href="discrete-response.html#cb1036-2"></a><span class="st">  </span><span class="kw">step_interact</span>(<span class="dt">terms =</span> <span class="op">~</span><span class="st"> </span>age<span class="op">:</span>race)</span></code></pre></div>
<p>Finally, we’ll use <code>fit_resamples()</code> to run our logistic model on the different folds we created in the training set. <code>fit_resamples()</code> takes in our model specification, our recipe, and the folds we created. This will give us our <code>accuracy</code> and <code>roc_auc</code> values.</p>
<div class="sourceCode" id="cb1037"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1037-1"><a href="discrete-response.html#cb1037-1"></a><span class="kw">fit_resamples</span>(logistic_mod,</span>
<span id="cb1037-2"><a href="discrete-response.html#cb1037-2"></a>              log_recipe,</span>
<span id="cb1037-3"><a href="discrete-response.html#cb1037-3"></a>              log_folds) <span class="op">%&gt;%</span></span>
<span id="cb1037-4"><a href="discrete-response.html#cb1037-4"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.608     5 0.00394
## 2 roc_auc  binary     0.655     5 0.00335</code></pre>
<!-- MB: interpret these values -->
<p>Now that we saw how the training set performed, it’s time to use our testing set. We do so by applying <code>log_testing</code> to <code>predict()</code>.</p>
<div class="sourceCode" id="cb1039"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1039-1"><a href="discrete-response.html#cb1039-1"></a>logistic_mod <span class="op">%&gt;%</span></span>
<span id="cb1039-2"><a href="discrete-response.html#cb1039-2"></a><span class="st">  </span><span class="kw">fit</span>(log_formula, <span class="dt">data =</span> log_training) <span class="op">%&gt;%</span></span>
<span id="cb1039-3"><a href="discrete-response.html#cb1039-3"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> log_testing)</span></code></pre></div>
<pre><code>## # A tibble: 14,887 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 0          
##  2 1          
##  3 1          
##  4 1          
##  5 1          
##  6 0          
##  7 0          
##  8 1          
##  9 0          
## 10 1          
## # … with 14,877 more rows</code></pre>
<p>Here, the model created by the training set predicts whether the observations in testing set would approve or disapprove of the President. While this is an interesting set of predictions, it doesn’t tell us much about how well our model performed. So, we need to solve for the rmse, or the mean squared error.</p>
<p>To extract the rmse, we set the “truth” to <code>approval</code> so this function can compare our predicted values to the true values.</p>
<div class="sourceCode" id="cb1041"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1041-1"><a href="discrete-response.html#cb1041-1"></a>logistic_mod <span class="op">%&gt;%</span></span>
<span id="cb1041-2"><a href="discrete-response.html#cb1041-2"></a><span class="st">  </span><span class="kw">fit</span>(log_formula, <span class="dt">data =</span> log_training) <span class="op">%&gt;%</span></span>
<span id="cb1041-3"><a href="discrete-response.html#cb1041-3"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> log_testing) <span class="op">%&gt;%</span></span>
<span id="cb1041-4"><a href="discrete-response.html#cb1041-4"></a><span class="st">  </span><span class="kw">bind_cols</span>(log_testing) <span class="op">%&gt;%</span></span>
<span id="cb1041-5"><a href="discrete-response.html#cb1041-5"></a><span class="st">  </span><span class="kw">rmse</span>(<span class="dt">truth =</span> <span class="kw">as.numeric</span>(approval), <span class="dt">estimate =</span> <span class="kw">as.numeric</span>(.pred_class))</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       0.627</code></pre>
<p>The mean squared error is about 0.62.</p>
<!-- MB: interpret this value -->
</div>
</div>
<div id="classification-and-regression-trees-cart" class="section level2">
<h2>
<span class="header-section-number">12.4</span> Classification and regression trees (CART)</h2>
<div id="what-is-cart" class="section level3">
<h3>
<span class="header-section-number">12.4.1</span> What is CART?</h3>
<p>Logistic regression is just one of many methods we can use to model binary responses. CART is another approach, which we’ll learn about in this section.</p>
<p>A <strong>tree</strong> is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as <em>nodes</em>. Decision trees predict an outcome variable <span class="math inline">\(Y\)</span> by <em>partitioning</em> the predictors.</p>
<p><strong>Classification trees</strong>, or decision trees, are used in prediction problems where the outcome is categorical. When the outcome is numerical, they are called <strong>regression trees</strong>; hence the acronym <strong>CART</strong>, standing for Classification and Regression Trees. The general idea here is to build a decision tree and, at the end of each <em>node</em>, obtain a predictor <span class="math inline">\(\hat{y}\)</span>. In this case, <span class="math inline">\(\hat{y}\)</span> would identify the likelihood of a voter in that node approving of the President.</p>
<p>But how do we decide on which partitions to make (<span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>) and how do we choose <span class="math inline">\(J\)</span>, the total number of partitions? Here is where the algorithm gets a bit complicated.</p>
<p>Classification trees create partitions recursively. We start the algorithm with one partition in which every observation is classified as either 0 or 1. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on.</p>
<p>Now, after we define the new partitions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, and we decide to stop the partitioning process, we compute predictors by taking the most common category of all the observations <span class="math inline">\(y\)</span> for which the associated <span class="math inline">\(\mathbf{x}\)</span> is in <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We refer to these two as <span class="math inline">\(\hat{y}_{R_1}\)</span> and <span class="math inline">\(\hat{y}_{R_2}\)</span> respectively.</p>
<!--MB: make this a margin note?
But how do we pick the predictor $j$ and the value $s$? One of the more popular ways for categorical data is the _Gini Index_. In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The _Gini Index_ is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define $\hat{p}_{j,k}$ as the proportion of observations in partition $j$ that are of class $k$. The Gini Index is defined as $\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})$ -->
<!-- If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above, since $\hat{p}_{j,k}(1-\hat{p}_{j,k}) = 0$ for all $k$. -->
<p>Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.</p>
<!-- But when do we stop partitioning?  Every time we split and define two new partitions, the Gini Index improves. This is because with more partitions, our model has more flexibility to adapt to our data.  However, our model may therefore perform worse when exposed to new data (this problem is called *overfitting*). This connects to our discussion of validity and models, as the conditions used to create the model will be too specific to accurately extrapolate to new data points. To avoid this, the algorithm sets a minimum for how much the Gini Index must improve for another partition to be added. This parameter is referred to as the _complexity parameter_ ($c_p$). The measure of fit must improve by a factor of $c_p$ for the new partition to be added. Large values of $c_p$ will therefore force the algorithm to stop earlier which results in fewer nodes. -->
<p>Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model human decision processes. However, in terms of accuracy, they are rarely the best performing method since they are not very flexible. Random forests, explained in the next section, improve on some of the shortcomings of classification trees.</p>
<p>One limitation of CART is its lack of fitted values. Unlike a <code>glm()</code>, you can’t clean predicted probabilities or point estimates from CART. Rather, you simply get a prediction of 0 or 1 for whatever observation you pass through the tree.</p>
</div>
<div id="multivariate-cart" class="section level3">
<h3>
<span class="header-section-number">12.4.2</span> Multivariate CART</h3>
<p>Before creating any model, let’s split our data into a training and testing set, just like we did with our multivariate logistical regression.</p>
<div class="sourceCode" id="cb1043"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1043-1"><a href="discrete-response.html#cb1043-1"></a>cart_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ch12)</span>
<span id="cb1043-2"><a href="discrete-response.html#cb1043-2"></a></span>
<span id="cb1043-3"><a href="discrete-response.html#cb1043-3"></a>cart_training &lt;-<span class="st"> </span>cart_split <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">training</span>()</span>
<span id="cb1043-4"><a href="discrete-response.html#cb1043-4"></a>cart_testing &lt;-<span class="st"> </span>cart_split <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">testing</span>()</span></code></pre></div>
<p>To create a CART using multiple variables, we’ll use the <code>decision_tree()</code> model specification and the <code>"rpart"</code> engine. The syntax is very similar to when we used <code>logistic_reg()</code>. Note that our binary response variable has to be a factor, just like with <code>logistic_reg()</code>.</p>
<div class="sourceCode" id="cb1044"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1044-1"><a href="discrete-response.html#cb1044-1"></a>tree_mod &lt;-<span class="st"> </span><span class="kw">decision_tree</span>() <span class="op">%&gt;%</span></span>
<span id="cb1044-2"><a href="discrete-response.html#cb1044-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"rpart"</span>,</span>
<span id="cb1044-3"><a href="discrete-response.html#cb1044-3"></a>             <span class="dt">model =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1044-4"><a href="discrete-response.html#cb1044-4"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">"classification"</span>)</span></code></pre></div>
<p>Note that we added the argument <code>model = TRUE</code> to <code>set_engine()</code>. This saves the model frame, which we will need to avoid a warning when we plot the trees later.</p>
<p>The function <code>set_mode()</code> wasn’t necessary when we did logistic regression. Here it clarifies that we want a <em>classification</em> tree rather than a <em>regression</em> tree.</p>
<p>Next, let’s create our formula for this tree. We will use the same formula that we used in the logistical regression.</p>
<div class="sourceCode" id="cb1045"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1045-1"><a href="discrete-response.html#cb1045-1"></a>cart_formula &lt;-<span class="st"> </span><span class="kw">formula</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>race)</span></code></pre></div>
<p>Now that we have the object <code>tree_mod</code> and our formula, we can use <code>fit()</code> to construct our tree.</p>
<div class="sourceCode" id="cb1046"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1046-1"><a href="discrete-response.html#cb1046-1"></a>cart_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(tree_mod,</span>
<span id="cb1046-2"><a href="discrete-response.html#cb1046-2"></a>                 cart_formula,</span>
<span id="cb1046-3"><a href="discrete-response.html#cb1046-3"></a>                 cart_training)</span></code></pre></div>
<p>As you can see, <strong>tidymodels</strong> has the same workflow for creating a CART as it does for fitting a logistic regression, but with our model specification saved in <code>tree_mod</code> rather than the model specification we saved in <code>logistic_mod</code>.</p>
<p>What was the result of our tree?</p>
<div class="sourceCode" id="cb1047"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1047-1"><a href="discrete-response.html#cb1047-1"></a>cart_fit</span></code></pre></div>
<pre><code>## parsnip model object
## 
## Fit time:  296ms 
## n= 44661 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 44661 20000 0 (0.56 0.44)  
##   2) race=Asian,Black,Hispanic,Middle Eastern,Mixed 10391  2500 0 (0.76 0.24) *
##   3) race=Native American,Other,White 34270 17000 0 (0.50 0.50)  
##     6) age&lt; 42 11752  4700 0 (0.60 0.40) *
##     7) age&gt;=42 22518 10000 1 (0.45 0.55) *</code></pre>
<p>It’s not helpful to look at the results of a tree as text. In order to visualize the tree, we’ll use the <code>prp()</code> function in the <strong>rpart.plot</strong> package. Remember that the model object is stored in <code>cart_fit$fit</code>.</p>
<div class="sourceCode" id="cb1049"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1049-1"><a href="discrete-response.html#cb1049-1"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb1049-2"><a href="discrete-response.html#cb1049-2"></a></span>
<span id="cb1049-3"><a href="discrete-response.html#cb1049-3"></a>cart_fit<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb1049-4"><a href="discrete-response.html#cb1049-4"></a><span class="st">  </span><span class="kw">prp</span>(<span class="dt">extra =</span> <span class="dv">6</span>, <span class="dt">varlen =</span> <span class="dv">0</span>, <span class="dt">faclen =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-825-1.png" width="672"></p>
<p>The arguments <code>varlen = 0</code> and <code>faclen = 0</code> ensure that the full variable names and factor levels are printed. The argument <code>extra = 6</code> shows the proportion of “yes” outcomes within a given partition. Since we’ll be using these same arguments throughout the chapter, we’ll create a new function that calls <code>prp()</code> but with these options as defaults:</p>
<div class="sourceCode" id="cb1050"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1050-1"><a href="discrete-response.html#cb1050-1"></a>prp_ch12 &lt;-<span class="st"> </span><span class="cf">function</span>(x, ...) <span class="kw">prp</span>(x, <span class="dt">extra =</span> <span class="dv">6</span>, <span class="dt">varlen =</span> <span class="dv">0</span>, <span class="dt">faclen =</span> <span class="dv">0</span>, ...)</span>
<span id="cb1050-2"><a href="discrete-response.html#cb1050-2"></a></span>
<span id="cb1050-3"><a href="discrete-response.html#cb1050-3"></a>cart_fit<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb1050-4"><a href="discrete-response.html#cb1050-4"></a><span class="st">  </span><span class="kw">prp_ch12</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-826-1.png" width="672"></p>
<p>So, how do we interpret this tree? We are given two partitions; the first is for race. The observations that identify as any of the named races are classified as disapproving of the President, denoted by the 0 on that node. All other racial identities proceed to the next partition. If these people are younger than 44, they are also classified as disapproving of the president. Voters 44 and older are classsified as approving of the President.</p>
<p>The decimal numbers within each box identify how many observations in that node approved of the President. This means that only 20% of observations in the first node approved of the President, 38% in the second node, and 54% in the third node.</p>
<p>You may notice that this tree did not factor in gender.
<!-- MB: Trees cannot handle interaction terms, --></p>
</div>
<div id="k-fold-cross-validation-1" class="section level3">
<h3>
<span class="header-section-number">12.4.3</span> K-Fold Cross-Validation</h3>
<p>Once again, we can check whether the model we created is accurate by using cross-validation.</p>
<p>We’ll use <code>vfold_cv()</code> to create <code>v = 5</code> splits in the training set, using the final fold to asses the model’s accuracy.</p>
<div class="sourceCode" id="cb1051"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1051-1"><a href="discrete-response.html#cb1051-1"></a>cart_folds &lt;-<span class="st"> </span>cart_training <span class="op">%&gt;%</span></span>
<span id="cb1051-2"><a href="discrete-response.html#cb1051-2"></a><span class="st">  </span><span class="kw">vfold_cv</span>(<span class="dt">v =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>Then, we’ll create our recipe and plug it into <code>fit_resamples()</code> along with <code>tree_mod</code> and <code>cart_folds</code>. We’ll use <code>collect_metrics()</code> to get our accuracy values.</p>
<!-- MB: Added interaction term here, but not in the tree above -->
<div class="sourceCode" id="cb1052"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1052-1"><a href="discrete-response.html#cb1052-1"></a>cart_recipe &lt;-<span class="st"> </span><span class="kw">recipe</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>race <span class="op">+</span><span class="st"> </span>gender, <span class="dt">data =</span> cart_training) </span>
<span id="cb1052-2"><a href="discrete-response.html#cb1052-2"></a></span>
<span id="cb1052-3"><a href="discrete-response.html#cb1052-3"></a>cart_recipe &lt;-<span class="st"> </span>cart_recipe <span class="op">%&gt;%</span></span>
<span id="cb1052-4"><a href="discrete-response.html#cb1052-4"></a><span class="st">  </span><span class="kw">step_interact</span>(<span class="dt">terms =</span> <span class="op">~</span><span class="st"> </span>age<span class="op">:</span>race)</span>
<span id="cb1052-5"><a href="discrete-response.html#cb1052-5"></a></span>
<span id="cb1052-6"><a href="discrete-response.html#cb1052-6"></a><span class="kw">fit_resamples</span>(tree_mod,</span>
<span id="cb1052-7"><a href="discrete-response.html#cb1052-7"></a>              cart_recipe,</span>
<span id="cb1052-8"><a href="discrete-response.html#cb1052-8"></a>              cart_folds) <span class="op">%&gt;%</span></span>
<span id="cb1052-9"><a href="discrete-response.html#cb1052-9"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.610     5 0.00234
## 2 roc_auc  binary     0.620     5 0.00644</code></pre>
<!-- Interpret accuracy values -->
<p>Now that we saw how the training set performed, we’ll use our testing set. We do so by applying <code>cart_testing</code> to <code>predict()</code>.</p>
<div class="sourceCode" id="cb1054"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1054-1"><a href="discrete-response.html#cb1054-1"></a>tree_mod <span class="op">%&gt;%</span></span>
<span id="cb1054-2"><a href="discrete-response.html#cb1054-2"></a><span class="st">  </span><span class="kw">fit</span>(cart_formula, <span class="dt">data =</span> cart_training) <span class="op">%&gt;%</span></span>
<span id="cb1054-3"><a href="discrete-response.html#cb1054-3"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> cart_testing)</span></code></pre></div>
<pre><code>## # A tibble: 14,887 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 1          
##  2 1          
##  3 1          
##  4 1          
##  5 1          
##  6 1          
##  7 1          
##  8 1          
##  9 1          
## 10 1          
## # … with 14,877 more rows</code></pre>
<p>Once again, the model predicts whether the observations in testing set would approve or disapprove of the President based on the model created with the training set.</p>
<p>To extract the rmse, we set the “truth” to <code>approval</code> so this function can compare our predicted values to the true values.</p>
<div class="sourceCode" id="cb1056"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1056-1"><a href="discrete-response.html#cb1056-1"></a>tree_mod <span class="op">%&gt;%</span></span>
<span id="cb1056-2"><a href="discrete-response.html#cb1056-2"></a><span class="st">  </span><span class="kw">fit</span>(cart_formula, <span class="dt">data =</span> cart_training) <span class="op">%&gt;%</span></span>
<span id="cb1056-3"><a href="discrete-response.html#cb1056-3"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> cart_testing) <span class="op">%&gt;%</span></span>
<span id="cb1056-4"><a href="discrete-response.html#cb1056-4"></a><span class="st">  </span><span class="kw">bind_cols</span>(cart_testing) <span class="op">%&gt;%</span></span>
<span id="cb1056-5"><a href="discrete-response.html#cb1056-5"></a><span class="st">  </span><span class="kw">rmse</span>(<span class="dt">truth =</span> <span class="kw">as.numeric</span>(approval), <span class="dt">estimate =</span> <span class="kw">as.numeric</span>(.pred_class))</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       0.628</code></pre>
<p>The mean squared error is about 0.616.</p>
</div>
<div id="predicting-new-data" class="section level3">
<h3>
<span class="header-section-number">12.4.4</span> Predicting New Data</h3>
<p>Let’s now use our tree to make some predictions. When used on trees, the <code>predict()</code> function takes in the tree object and new observations containing the explanatory variables. In this case, we’ll be passing in a 54-year-old Asian voter, a 41-year-old Black voter, a 56-year-old White voter, and a 70-year-old Native American voter.</p>
<div class="sourceCode" id="cb1058"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1058-1"><a href="discrete-response.html#cb1058-1"></a>newdata &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">race =</span> <span class="kw">c</span>(<span class="st">"Asian"</span>, <span class="st">"Black"</span>, <span class="st">"White"</span>, <span class="st">"Native American"</span>),</span>
<span id="cb1058-2"><a href="discrete-response.html#cb1058-2"></a>                  <span class="dt">age =</span> <span class="kw">c</span>(<span class="dv">54</span>, <span class="dv">41</span>, <span class="dv">56</span>, <span class="dv">70</span>),</span>
<span id="cb1058-3"><a href="discrete-response.html#cb1058-3"></a>                  <span class="dt">gender =</span> <span class="kw">c</span>(<span class="st">"Male"</span>, <span class="st">"Female"</span>, <span class="st">"Male"</span>, <span class="st">"Female"</span>))</span>
<span id="cb1058-4"><a href="discrete-response.html#cb1058-4"></a></span>
<span id="cb1058-5"><a href="discrete-response.html#cb1058-5"></a><span class="kw">predict</span>(cart_fit, <span class="dt">new_data =</span> newdata) </span></code></pre></div>
<pre><code>## # A tibble: 4 x 1
##   .pred_class
##   &lt;fct&gt;      
## 1 0          
## 2 0          
## 3 1          
## 4 1</code></pre>
<p>As you can see, the tree predicted that our first two new voters would disapprove of the President while our second two would approve based on their age and race.</p>
<div class="sourceCode" id="cb1060"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1060-1"><a href="discrete-response.html#cb1060-1"></a><span class="co">#library(rstanarm)</span></span>
<span id="cb1060-2"><a href="discrete-response.html#cb1060-2"></a><span class="co">#posterior_linpred(cart_fit, newdata)</span></span></code></pre></div>
</div>
</div>
<div id="random-forests" class="section level2">
<h2>
<span class="header-section-number">12.5</span> Random forests</h2>
<div id="what-are-random-forests" class="section level3">
<h3>
<span class="header-section-number">12.5.1</span> What are random forests?</h3>
<p>Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by <em>averaging</em> multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</p>
<p>The first step is <em>bootstrap aggregation</em> or <em>bagging</em>. The general idea is to generate many predictors, each using classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees <strong>randomly</strong> different, and the combination of trees is the <strong>forest</strong>. The specific steps are as follows.</p>
<ol style="list-style-type: decimal">
<li><p>Build decision trees using a portion of the data called the training set. We refer to the fitted models as <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>. We later explain how we ensure they are different.</p></li>
<li><p>For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span>.</p></li>
<li><p>For categorical data classification, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_T\)</span>).</p></li>
</ol>
<p>So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let <span class="math inline">\(N\)</span> be the number of observations in the training set. To create <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> from the training set we do the following:</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set <strong>with replacement</strong>. This is the first way to induce randomness.</p></li>
<li><p>A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.</p></li>
</ol>
</div>
<div id="fitting-random-forests" class="section level3">
<h3>
<span class="header-section-number">12.5.2</span> Fitting random forests</h3>
<!-- MB: http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/#evaluate-the-model-on-the-test-set
https://s3.amazonaws.com/assets.datacamp.com/production/course_7215/slides/chapter3.pdf
-->
<p>We will demonstrate by fitting a random forest to the House elections data. Before we even write our model formula, we need to split the data to create a <em>training set</em> and a <em>testing set</em>. The training dataset will be used to create our model and the testing dataset will be used to test our final model’s performance.</p>
<div class="sourceCode" id="cb1061"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1061-1"><a href="discrete-response.html#cb1061-1"></a>forest_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ch12, </span>
<span id="cb1061-2"><a href="discrete-response.html#cb1061-2"></a>                                <span class="dt">prop =</span> <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)</span>
<span id="cb1061-3"><a href="discrete-response.html#cb1061-3"></a></span>
<span id="cb1061-4"><a href="discrete-response.html#cb1061-4"></a>forest_train &lt;-<span class="st"> </span><span class="kw">training</span>(forest_split)</span>
<span id="cb1061-5"><a href="discrete-response.html#cb1061-5"></a>forest_test &lt;-<span class="st"> </span><span class="kw">testing</span>(forest_split)</span></code></pre></div>
<p>Next, we will use the <code>rand_forest()</code> function to create our model specification, setting the engine to <code>"randomForest"</code> and the mode to <code>"classification"</code>.</p>
<div class="sourceCode" id="cb1062"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1062-1"><a href="discrete-response.html#cb1062-1"></a><span class="kw">library</span>(randomForest)</span>
<span id="cb1062-2"><a href="discrete-response.html#cb1062-2"></a></span>
<span id="cb1062-3"><a href="discrete-response.html#cb1062-3"></a>forest_mod &lt;-<span class="st"> </span></span>
<span id="cb1062-4"><a href="discrete-response.html#cb1062-4"></a><span class="st">  </span><span class="kw">rand_forest</span>() <span class="op">%&gt;%</span></span>
<span id="cb1062-5"><a href="discrete-response.html#cb1062-5"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"randomForest"</span>) <span class="op">%&gt;%</span></span>
<span id="cb1062-6"><a href="discrete-response.html#cb1062-6"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">"classification"</span>) </span></code></pre></div>
<p>We will now create the formula for our forest.</p>
<div class="sourceCode" id="cb1063"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1063-1"><a href="discrete-response.html#cb1063-1"></a>forest_formula &lt;-<span class="st"> </span><span class="kw">formula</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>race <span class="op">+</span><span class="st"> </span>race<span class="op">:</span>age)</span></code></pre></div>
<p>Finally, let’s fit our model. <code>fit()</code> will take in our model specification <code>logistic_mod</code>, our formula, and our training set of data. We are reserving our testing set for later.</p>
<div class="sourceCode" id="cb1064"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1064-1"><a href="discrete-response.html#cb1064-1"></a>forest_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(forest_mod, forest_formula, forest_train)</span></code></pre></div>
<p>If we graph the “OOB estimate of error rate”, we can see that this model settles around an error rate of just under 40% (or, looking at it the other way, an accuracy of 60%). We can see how the error rate of our algorithm changes as we add trees by looking at <code>forest_fit$fit$err.rate[, "OOB"]</code>. By default, <code>randomForest()</code> (the engine we specified) grows 500 trees. –&gt;</p>
<div class="sourceCode" id="cb1065"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1065-1"><a href="discrete-response.html#cb1065-1"></a><span class="kw">tibble</span>(<span class="st">`</span><span class="dt">Error rate</span><span class="st">`</span> =<span class="st"> </span>forest_fit<span class="op">$</span>fit<span class="op">$</span>err.rate[, <span class="st">"OOB"</span>], <span class="dt">Trees =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">500</span>) <span class="op">%&gt;%</span></span>
<span id="cb1065-2"><a href="discrete-response.html#cb1065-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Trees, <span class="dt">y =</span> <span class="st">`</span><span class="dt">Error rate</span><span class="st">`</span>)) <span class="op">+</span></span>
<span id="cb1065-3"><a href="discrete-response.html#cb1065-3"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1065-4"><a href="discrete-response.html#cb1065-4"></a><span class="st">  </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-837-1.png" width="672"></p>
<p>We can see that in this case, the accuracy improves as we add more trees until about 50 trees where accuracy stabilizes. Random forests often perform better than other methods. However, a disadvantage of random forests is that we lose interpretability—we don’t get anything like the coefficients from a logistic regression or the single tree from CART.</p>
</div>
<div id="k-fold-cross-validation-2" class="section level3">
<h3>
<span class="header-section-number">12.5.3</span> K-Fold Cross-Validation</h3>
<p>Just like with CART and logistic regression, we will use cross-validation to check the accuracy of our random forest. Once again, we’ll create the recipe and add our interaction term seperately.</p>
<div class="sourceCode" id="cb1066"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1066-1"><a href="discrete-response.html#cb1066-1"></a>forest_recipe &lt;-<span class="st"> </span><span class="kw">recipe</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>race, </span>
<span id="cb1066-2"><a href="discrete-response.html#cb1066-2"></a>                        <span class="dt">data =</span> forest_train)</span>
<span id="cb1066-3"><a href="discrete-response.html#cb1066-3"></a></span>
<span id="cb1066-4"><a href="discrete-response.html#cb1066-4"></a>forest_recipe &lt;-<span class="st"> </span>forest_recipe <span class="op">%&gt;%</span></span>
<span id="cb1066-5"><a href="discrete-response.html#cb1066-5"></a><span class="st">  </span><span class="kw">step_interact</span>(<span class="dt">terms =</span> <span class="op">~</span><span class="st"> </span>age<span class="op">:</span>race)</span>
<span id="cb1066-6"><a href="discrete-response.html#cb1066-6"></a></span>
<span id="cb1066-7"><a href="discrete-response.html#cb1066-7"></a><span class="co">#approval_workflow &lt;- workflow() %&gt;%</span></span>
<span id="cb1066-8"><a href="discrete-response.html#cb1066-8"></a>  <span class="co"># add the recipe</span></span>
<span id="cb1066-9"><a href="discrete-response.html#cb1066-9"></a><span class="co">#  add_recipe(approval_recipe) %&gt;%</span></span>
<span id="cb1066-10"><a href="discrete-response.html#cb1066-10"></a>  <span class="co"># add the model</span></span>
<span id="cb1066-11"><a href="discrete-response.html#cb1066-11"></a><span class="co">#  add_model(forest_mod)</span></span></code></pre></div>
<p>We will also create our folds using <code>vfold_cv()</code>.</p>
<div class="sourceCode" id="cb1067"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1067-1"><a href="discrete-response.html#cb1067-1"></a>forest_folds &lt;-<span class="st"> </span>forest_train <span class="op">%&gt;%</span></span>
<span id="cb1067-2"><a href="discrete-response.html#cb1067-2"></a><span class="st">  </span><span class="kw">vfold_cv</span>(<span class="dt">v =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>Now we use <code>fit_resamples()</code> to see how it accurately the model performed on the fifth and final fold of the training set. We will save the output of <code>fit_resamples()</code> in forest_metrics to use later.</p>
<div class="sourceCode" id="cb1068"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1068-1"><a href="discrete-response.html#cb1068-1"></a>forest_metrics &lt;-<span class="st"> </span><span class="kw">fit_resamples</span>(forest_mod,</span>
<span id="cb1068-2"><a href="discrete-response.html#cb1068-2"></a>                                forest_recipe,</span>
<span id="cb1068-3"><a href="discrete-response.html#cb1068-3"></a>                                forest_folds) </span>
<span id="cb1068-4"><a href="discrete-response.html#cb1068-4"></a></span>
<span id="cb1068-5"><a href="discrete-response.html#cb1068-5"></a>forest_metrics <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.611     5 0.00286
## 2 roc_auc  binary     0.652     5 0.00266</code></pre>
<!-- MB: Interpret -->
<p>Now that we saw how the training set performed, we’ll use our testing set. We do so by applying <code>forest_test</code> to <code>predict()</code>.</p>
<div class="sourceCode" id="cb1070"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1070-1"><a href="discrete-response.html#cb1070-1"></a><span class="co">#forest_mod %&gt;%</span></span>
<span id="cb1070-2"><a href="discrete-response.html#cb1070-2"></a><span class="co">#  fit(forest_formula, data = forest_train) %&gt;%</span></span>
<span id="cb1070-3"><a href="discrete-response.html#cb1070-3"></a><span class="co">#  predict(new_data = forest_test)</span></span></code></pre></div>
<p>To extract the rmse, we again set the “truth” to <code>approval</code> so this function can compare our predicted values to the true values.</p>
<div class="sourceCode" id="cb1071"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1071-1"><a href="discrete-response.html#cb1071-1"></a><span class="co">#forest_mod %&gt;%</span></span>
<span id="cb1071-2"><a href="discrete-response.html#cb1071-2"></a><span class="co">#  fit(forest_formula, data = forest_train) %&gt;%</span></span>
<span id="cb1071-3"><a href="discrete-response.html#cb1071-3"></a><span class="co">#  predict(new_data = forest_test) %&gt;%</span></span>
<span id="cb1071-4"><a href="discrete-response.html#cb1071-4"></a><span class="co">#  bind_cols(forest_test) %&gt;%</span></span>
<span id="cb1071-5"><a href="discrete-response.html#cb1071-5"></a><span class="co">#  rmse(truth = as.numeric(approval), estimate = as.numeric(.pred_class))</span></span></code></pre></div>
<p>The mean squared error is about 0.619.</p>
<!-- We can use `collect_predictions()` to see exactly how many of the predictions on the test set were correct. It appears that 9051 were correct (the addition of the (0,0) and (1,1) cells), while 5403 were incorrect. -->
<!-- ```{r} -->
<!-- test_predictions <- forest_metrics %>% collect_predictions() -->
<!-- test_predictions %>%  -->
<!--   conf_mat(truth = approval, estimate = .pred_class) -->
<!-- ``` -->
<!-- We could also plot distributions of the predicted probability distributions for voter. -->
<!-- ```{r} -->
<!-- test_predictions %>% -->
<!--   ggplot() + -->
<!--   geom_density(aes(x = .pred_1, fill = approval),  -->
<!--                alpha = 0.5) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- final_model <- fit(approval_workflow, ch12) -->
<!-- ``` -->
</div>
<div id="predicting-new-data-1" class="section level3">
<h3>
<span class="header-section-number">12.5.4</span> Predicting New Data</h3>
<p>Let’s use our final forest model to make some predictions. We’ll be passing in the same observations as with the CART example: a 54-year-old Asian voter, a 41-year-old Black voter, a 56-year-old White voter, and a 70-year-old Native American voter. However, our forest also includes a gender variable. We’ll make the first and third observations female.</p>
<div class="sourceCode" id="cb1072"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1072-1"><a href="discrete-response.html#cb1072-1"></a><span class="co"># DK: If the model has been fit with race as a factor, then race in newdata must</span></span>
<span id="cb1072-2"><a href="discrete-response.html#cb1072-2"></a><span class="co"># also be a factor and, subtle point!, must have the same levels as the variable</span></span>
<span id="cb1072-3"><a href="discrete-response.html#cb1072-3"></a><span class="co"># which was used to fit the model, even if there are no observations with those</span></span>
<span id="cb1072-4"><a href="discrete-response.html#cb1072-4"></a><span class="co"># values in the newdata.</span></span>
<span id="cb1072-5"><a href="discrete-response.html#cb1072-5"></a></span>
<span id="cb1072-6"><a href="discrete-response.html#cb1072-6"></a></span>
<span id="cb1072-7"><a href="discrete-response.html#cb1072-7"></a>newdata &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">race =</span> <span class="kw">c</span>(<span class="st">"Asian"</span>, <span class="st">"Black"</span>, <span class="st">"White"</span>, <span class="st">"Native American"</span>),</span>
<span id="cb1072-8"><a href="discrete-response.html#cb1072-8"></a>                  <span class="dt">age =</span> <span class="kw">c</span>(<span class="dv">54</span>, <span class="dv">41</span>, <span class="dv">56</span>, <span class="dv">70</span>),</span>
<span id="cb1072-9"><a href="discrete-response.html#cb1072-9"></a>                 <span class="dt">gender =</span> <span class="kw">c</span>(<span class="st">"Female"</span>, <span class="st">"Male"</span>, <span class="st">"Female"</span>, <span class="st">"Male"</span>))</span>
<span id="cb1072-10"><a href="discrete-response.html#cb1072-10"></a></span>
<span id="cb1072-11"><a href="discrete-response.html#cb1072-11"></a><span class="co">#predict(forest_fit, new_data = newdata) </span></span></code></pre></div>
<p>Our forest predicted that the first three voters would disaprove of the President and that our final voter would approve. These predictions differ from our CART predictions most likely because of the added variable: gender. Because females disapproved of the President at higher rates, the third voter was predicted to disapprove instead.</p>
<div class="sourceCode" id="cb1073"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1073-1"><a href="discrete-response.html#cb1073-1"></a><span class="co">#posterior_linpred(forest_fit, newdata)</span></span></code></pre></div>
</div>
</div>
<div id="comparing-models" class="section level2">
<h2>
<span class="header-section-number">12.6</span> Comparing Models</h2>
<p>Now that we’ve modelled the same formula using three different types of models (logistic regression, CART, and random forest), let’s compare these models to see which was the most accurate.</p>
<p>Here is the output:</p>
<div class="sourceCode" id="cb1074"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1074-1"><a href="discrete-response.html#cb1074-1"></a><span class="kw">fit_resamples</span>(logistic_mod, </span>
<span id="cb1074-2"><a href="discrete-response.html#cb1074-2"></a>              log_recipe, </span>
<span id="cb1074-3"><a href="discrete-response.html#cb1074-3"></a>              log_folds) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.608     5 0.00394
## 2 roc_auc  binary     0.655     5 0.00335</code></pre>
<div class="sourceCode" id="cb1076"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1076-1"><a href="discrete-response.html#cb1076-1"></a><span class="kw">fit_resamples</span>(tree_mod, </span>
<span id="cb1076-2"><a href="discrete-response.html#cb1076-2"></a>              cart_recipe, </span>
<span id="cb1076-3"><a href="discrete-response.html#cb1076-3"></a>              cart_folds) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.610     5 0.00234
## 2 roc_auc  binary     0.620     5 0.00644</code></pre>
<div class="sourceCode" id="cb1078"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1078-1"><a href="discrete-response.html#cb1078-1"></a><span class="kw">fit_resamples</span>(forest_mod, </span>
<span id="cb1078-2"><a href="discrete-response.html#cb1078-2"></a>              forest_recipe, </span>
<span id="cb1078-3"><a href="discrete-response.html#cb1078-3"></a>              forest_folds) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.611     5 0.00306
## 2 roc_auc  binary     0.651     5 0.00270</code></pre>
<p>Based on <code>accuracy</code>, the random forest performed best by a small margin, followed by logistic regression and then CART. <code>accuracy</code> is the percentage of observations in the testing set that the model (which was constructed using the training set) was able to successfully predict. All of the models performed within a margin of 0.01 of eachother. However, it makes sense that random forest was the most successful. By generating many prediction trees and averaging them rather than relying on one, random forest is a more precise form of modelling than CART.</p>
<p><code>roc_auc</code> is similar to <code>accuracy</code> in that it is a measure of how good a model is at predicting 0s to be 0 and 1s to be 1 in the dataset. However, it is calculated by taking the area under the ROC curve. <code>roc_auc</code> itself is a shortened form of “area under the ROC curve.”</p>
<div class="sourceCode" id="cb1080"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1080-1"><a href="discrete-response.html#cb1080-1"></a>knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">"12-discrete-response/images/ROC_AUC.png"</span>)</span></code></pre></div>
<p><img src="12-discrete-response/images/ROC_AUC.png" width="487"></p>
<p>The ROC curve is created by graphing the rate of true positives as the y-axis (those who approved of the President who were labelled as such) against the rate of false positives (those who approved of the President but were labelled as disapproving). The area under this curve comes out to a very similar measure of accuracy. A <code>roc_auc</code> of 1 means that the testing set was perfectly predicted, with no false positives, while a 0 means that every observation of the testing set was falsely predicted.</p>
<p>In the case of the <code>roc_auc</code>, our logistic regression performed best, followed by the random forest, and then CART. They were all within 0.05 of eachother, a wider range than <code>accuracy</code>. Being a cruder version of random forest, it again makes sense that it placed last.</p>
<!-- MB: Say something about log being better than RF -->
<!-- MB: bring back discussion of humility -->
</div>
<div id="the-question-pt.-2" class="section level2">
<h2>
<span class="header-section-number">12.7</span> The Question Pt. 2</h2>
<p>There are an infinite numbers of comparisons to be made within this dataset. When creating a model, it’s best to have a specific question in mind. At the end, you can answer that question, using your model to learn more about the world.</p>
<p>Let’s return to our original question of how Asians and Whites specifically vary in their support of the president. We’ll use our logistic regression first, as that model performed with the highest roc_auc.</p>
<!-- imprtnt: When comparing blacks and hispanics, the approval was predicted to be 0 for every age. So i switched to white and asian here. Need to go back and fix in earlier work (in The Question section) -->
<p>We’ve created a dataset of 24 new Asian and White voters so that we can use our model to predict if they will vote for the President. Each voter’s race, age, and gender have been included as these are the 3 demographic factors our model uses to predict approval.</p>
<div class="sourceCode" id="cb1081"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1081-1"><a href="discrete-response.html#cb1081-1"></a>newdata</span></code></pre></div>
<pre><code>## # A tibble: 24 x 3
##    race    age gender
##    &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 
##  1 Asian    24 Female
##  2 Asian    24 Male  
##  3 Asian    36 Female
##  4 Asian    36 Male  
##  5 Asian    48 Female
##  6 Asian    48 Male  
##  7 Asian    65 Female
##  8 Asian    65 Male  
##  9 Asian    73 Female
## 10 Asian    73 Male  
## # … with 14 more rows</code></pre>
<p>Next, we’ll use the <code>predict()</code> function to see how our model thinks these voters will regard the President.</p>
<div class="sourceCode" id="cb1083"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1083-1"><a href="discrete-response.html#cb1083-1"></a>x &lt;-<span class="st"> </span><span class="kw">predict</span>(logistic_fit, <span class="dt">new_data =</span> newdata) </span>
<span id="cb1083-2"><a href="discrete-response.html#cb1083-2"></a>x</span></code></pre></div>
<pre><code>## # A tibble: 24 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 0          
##  2 0          
##  3 0          
##  4 0          
##  5 0          
##  6 0          
##  7 0          
##  8 0          
##  9 0          
## 10 1          
## # … with 14 more rows</code></pre>
<p>Well, now what? We have our predictions for each of the new voters, but it isn’t helpful to answering our question yet. To see greater trends between White and Asian voters, let’s visualize our predictions! The new data had an intentionally even spread of age, race, and gender to make our comparisons easier.</p>
<p>Let’s use the same strategy from our exploratory data analysis earlier. We can graph age against approval, facet by gender, and color the observations by race. This way, we can view trends across all three variables in one graph.</p>
<div class="sourceCode" id="cb1085"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1085-1"><a href="discrete-response.html#cb1085-1"></a><span class="co"># Cleaning the data </span></span>
<span id="cb1085-2"><a href="discrete-response.html#cb1085-2"></a></span>
<span id="cb1085-3"><a href="discrete-response.html#cb1085-3"></a>y &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1085-4"><a href="discrete-response.html#cb1085-4"></a><span class="st">  </span><span class="kw">bind_cols</span>(newdata) <span class="op">%&gt;%</span></span>
<span id="cb1085-5"><a href="discrete-response.html#cb1085-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> .pred_class) <span class="op">%&gt;%</span></span>
<span id="cb1085-6"><a href="discrete-response.html#cb1085-6"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>.pred_class)</span>
<span id="cb1085-7"><a href="discrete-response.html#cb1085-7"></a></span>
<span id="cb1085-8"><a href="discrete-response.html#cb1085-8"></a><span class="co"># Graphing the data</span></span>
<span id="cb1085-9"><a href="discrete-response.html#cb1085-9"></a></span>
<span id="cb1085-10"><a href="discrete-response.html#cb1085-10"></a>y <span class="op">%&gt;%</span></span>
<span id="cb1085-11"><a href="discrete-response.html#cb1085-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> approval, <span class="dt">color =</span> race)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1085-12"><a href="discrete-response.html#cb1085-12"></a><span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">width =</span> <span class="dv">8</span>, <span class="dt">height =</span> <span class="dv">0</span>) <span class="op">+</span><span class="st"> </span><span class="co"># Adding width so that the dots don't block eachother</span></span>
<span id="cb1085-13"><a href="discrete-response.html#cb1085-13"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>gender) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1085-14"><a href="discrete-response.html#cb1085-14"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Age"</span>,</span>
<span id="cb1085-15"><a href="discrete-response.html#cb1085-15"></a>       <span class="dt">y =</span> <span class="st">"Predicted Approval"</span>,</span>
<span id="cb1085-16"><a href="discrete-response.html#cb1085-16"></a>       <span class="dt">title =</span> <span class="st">"Predicted Approval of President for White and Asian Voters"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-852-1.png" width="672"></p>
<p>So, how do White and Asian voters vary in their support of the President in 2018? As we can see, this is a more complex question than one may think. For example, if one were only considering race, they would guess an White voter to be more likely to approve of the President than an Asian voter. However, a 40-year-old White male, according to our model, is not as likely to approve of the President as a 70-year-old Asian man. This is useful information to consider: when using our model to its full extent, we can make predictions based on more than one variable at a time.</p>
<p>What else can we learn from this graph? Both races follow the trend of older voters being more likely to approve of the President. However, we see that Asian woman were predicted to <em>disapprove</em> of the President at any age, while the older Asian men were predicted to approve of him. The margin was much closer for White voters, with both older men and women being predicted to approve.</p>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="continuous-response.html"><button class="btn btn-default">Previous</button></a>
<a href="appendices.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-08-21
</p>
</div>
</div>



</body>
</html>
