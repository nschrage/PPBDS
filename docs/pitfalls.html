<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 10 Pitfalls | Gov 50: Data" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="davidkane9/PPBDS" />



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 10 Pitfalls | Gov 50: Data">

<title>Chapter 10 Pitfalls | Gov 50: Data</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Gov 50: Data<p><p class="author"></p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html"></a>
<a href="preamble.html">Preamble</a>
<a href="shopping-week.html">Shopping Week</a>
<a href="visualization.html"><span class="toc-section-number">1</span> Visualization</a>
<a href="wrangling.html"><span class="toc-section-number">2</span> Wrangling</a>
<a href="rubin-causal-model.html"><span class="toc-section-number">3</span> Rubin Causal Model</a>
<a href="functions.html"><span class="toc-section-number">4</span> Functions</a>
<a href="probability.html"><span class="toc-section-number">5</span> Probability</a>
<a href="one-parameter.html"><span class="toc-section-number">6</span> One Parameter</a>
<a href="two-parameters.html"><span class="toc-section-number">7</span> Two Parameters</a>
<a href="three-parameters.html"><span class="toc-section-number">8</span> Three Parameters</a>
<a href="n-parameters.html"><span class="toc-section-number">9</span> N Parameters</a>
<a id="active-page" href="pitfalls.html"><span class="toc-section-number">10</span> Pitfalls</a><ul class="toc-sections">
<li class="toc"><a href="#model1"> Teaching evaluations: one numerical explanatory variable</a></li>
<li class="toc"><a href="#machine-learning"> Machine Learning</a></li>
<li class="toc"><a href="#the-process-of-machine-learning"> The process of machine learning</a></li>
<li class="toc"><a href="#what-does-it-mean-for-a-model-to-be-good"> What does it mean for a model to be “good?”</a></li>
<li class="toc"><a href="#data-q-scores-for-harvard-academic-year-2018-2019"> Data: Q Scores for Harvard Academic Year 2018-2019</a></li>
<li class="toc"><a href="#the-modeling-process-using-tidymodels"> The modeling process using <strong>tidymodels</strong></a></li>
<li class="toc"><a href="#cross-validation"> Cross validation</a></li>
<li class="toc"><a href="#conclusion"> Conclusion</a></li>
</ul>
<a href="continuous-response.html"><span class="toc-section-number">11</span> Continuous Response</a>
<a href="discrete-response.html"><span class="toc-section-number">12</span> Discrete Response</a>
<a href="appendices.html">Appendices</a>
<a href="tools.html">Tools</a>
<a href="shiny.html">Shiny</a>
<a href="maps.html">Maps</a>
<a href="animation.html">Animation</a>
<a href="references.html">References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="pitfalls" class="section level1">
<h1>
<span class="header-section-number">Chapter 10</span> Pitfalls</h1>
<p><em>This chapter is still very much a draft.</em> Come back in a few weeks for a better version.</p>
<p>The fundamental goal of data modeling is to make explicit the relationship between:</p>
<ul>
<li>an <em>outcome variable</em> <span class="math inline">\(y\)</span>, also called a <em>dependent variable</em> or response variable, and<br>
</li>
<li>an <em>explanatory/predictor variable</em> <span class="math inline">\(x\)</span>, also called an <em>independent variable</em> or covariate.</li>
</ul>
<p>Another way to state this is using mathematical terminology: we will model the outcome variable <span class="math inline">\(y\)</span> “as a function” of the explanatory/predictor variable <span class="math inline">\(x\)</span>. When we say “function” here, we aren’t referring to functions in R like the <code>ggplot()</code> function, but rather to a mathematical function. But, why do we have two different labels, explanatory and predictor, for the variable <span class="math inline">\(x\)</span>? That’s because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Modeling for explanation</strong>: When you want to explicitly describe and quantify the relationship between the outcome variable <span class="math inline">\(y\)</span> and an explanatory variable <span class="math inline">\(x\)</span>, determine the importance of any relationships, have measures summarizing these relationships, and possibly identify any <em>causal</em> relationships between the variables. (What’s a causal relationship? Remember the <a href="rubin-causal-model.html#rubin-causal-model">Rubin Causal Model</a>! The <em>causal effect</em> of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> is the difference in <em>potential outcomes</em> of <span class="math inline">\(y\)</span> given different values of <span class="math inline">\(x\)</span>.)</li>
<li>
<strong>Modeling for prediction</strong>: When you want to predict an outcome variable <span class="math inline">\(y\)</span> based on the information contained in a set of predictor variables <span class="math inline">\(x\)</span>. Unlike modeling for explanation, however, you don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about <span class="math inline">\(y\)</span> using the information in <span class="math inline">\(x\)</span>.</li>
</ol>
<p>For example, say you are interested in an outcome variable <span class="math inline">\(y\)</span> of whether patients develop lung cancer and information <span class="math inline">\(x\)</span> on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as increasing family income. In that case, you would want to know the causal effect of income on the incidence of lung cancer.</p>
<p>If we are modeling for prediction, however, we wouldn’t care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer.</p>
<!-- DK: Find a way to use this reference: [*An Introduction to Statistical Learning with Applications in R (ISLR)*](http://www-bcf.usc.edu/~gareth/ISL/)  Use @islr2017. Maybe side margin with book cover. -->
<p>Linear regression involves a <em>numerical</em> outcome variable <span class="math inline">\(y\)</span> and explanatory variables <span class="math inline">\(x\)</span> that are either <em>numerical</em> or <em>categorical</em>. Furthermore, the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> is assumed to be linear, or in other words, a line. However, we’ll see that what constitutes a “line” will vary depending on the nature of your explanatory variables <span class="math inline">\(x\)</span>.</p>
<!-- DK: Could give a better plan overview, including discussion of chapters 11 and 12. Indeed, perhaps also looking backward to sampling and uncertainty. Need to rewrite this if we re-organize the book. Indeed, the introductions (and conclusions) to each chapter should be similar, providing a framework in which that chapter fits. -->
<p>In Section <a href="pitfalls.html#model1">10.1</a>, the explanatory variable will be numerical. This scenario is known as <em>simple linear regression</em>. In Section <a href="#model2"><strong>??</strong></a>, the explanatory variable will be categorical.</p>
<p>In Chapter <a href="continuous-response.html#continuous-response">11</a> on multiple regression, we’ll extend the ideas behind basic regression and consider models with two explanatory variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. In Section <a href="#model4"><strong>??</strong></a>, we’ll have two numerical explanatory variables. In Section <a href="#model3"><strong>??</strong></a>, we’ll have one numerical and one categorical explanatory variable. In particular, we’ll consider two such models: <em>interaction</em> and <em>parallel slopes</em> models.</p>
<p>Let’s now begin with basic regression,  which refers to linear regression models with a single explanatory variable <span class="math inline">\(x\)</span>. We’ll also discuss important statistical concepts like the <em>correlation coefficient</em>, that “correlation isn’t necessarily causation,” and what it means for a line to be “best-fitting.”</p>
<p>Let’s now load all the packages needed for this chapter (this assumes you’ve already installed them). The main packages are ones we have used before. The Advanced Section of the chapter makes use of</p>
<ol style="list-style-type: decimal">
<li>The <strong>rstanarm</strong> package, which provides an interface to the statistical inference engine, Stan, for Bayesian Regression Modeling.</li>
<li>The <strong>tidybayes</strong> package, which aids in formating Bayesian modeling outputs in a tidy manner and provides ggplot geoms for plotting.</li>
<li>The <strong>broom.mixed</strong> package, which provides broom-type functions for the output objects generated by <strong>rstanarm</strong>.</li>
</ol>
<div class="sourceCode" id="cb941"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb941-1"><a href="pitfalls.html#cb941-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb941-2"><a href="pitfalls.html#cb941-2"></a><span class="kw">library</span>(PPBDS.data)</span>
<span id="cb941-3"><a href="pitfalls.html#cb941-3"></a><span class="kw">library</span>(broom)</span>
<span id="cb941-4"><a href="pitfalls.html#cb941-4"></a><span class="kw">library</span>(broom.mixed)</span>
<span id="cb941-5"><a href="pitfalls.html#cb941-5"></a><span class="kw">library</span>(parsnip)</span>
<span id="cb941-6"><a href="pitfalls.html#cb941-6"></a><span class="kw">library</span>(skimr)</span>
<span id="cb941-7"><a href="pitfalls.html#cb941-7"></a><span class="kw">library</span>(gapminder)</span>
<span id="cb941-8"><a href="pitfalls.html#cb941-8"></a><span class="kw">library</span>(rstanarm)</span>
<span id="cb941-9"><a href="pitfalls.html#cb941-9"></a><span class="kw">library</span>(rsample)</span>
<span id="cb941-10"><a href="pitfalls.html#cb941-10"></a><span class="kw">library</span>(tidybayes)</span>
<span id="cb941-11"><a href="pitfalls.html#cb941-11"></a><span class="kw">library</span>(tune)</span>
<span id="cb941-12"><a href="pitfalls.html#cb941-12"></a><span class="kw">library</span>(yardstick)</span></code></pre></div>
<div id="model1" class="section level2">
<h2>
<span class="header-section-number">10.1</span> Teaching evaluations: one numerical explanatory variable</h2>
<!-- EG: I really like this section- I think that the in-depth explanations of not only how to find correlation coefficients but also interpret them accurately and effectively is great. -->
<!-- EG: I'll change this to qscores, along with an adjusted EDA for that dataset and more explanation of how correlation != causation. I'll also provide more investigation into the many ways confounding variables could impact why students provide certain qscores rather than simply hours of work, along with more language of comparison. -->
<p>Why do some professors and instructors at universities and colleges receive high teaching evaluations scores from students while others receive lower ones? Are there differences in teaching evaluations between instructors of different demographic groups? Could there be an impact due to student biases? These are all questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which instructors and professors get promoted.</p>
<p>In this section, we’ll keep things simple for now and try to explain differences in instructor ratings within the Harvard music department based on average hourly workload for that class. Could it be that instructors with lower hourly workloads also have higher ratings? Could it be instead that instructors with lower hourly workloads tend to have lower ratings? Or could it be that there is no relationship between workload and teaching evaluations? We’ll answer these questions by modeling the relationship between rating and workload using <em>simple linear regression</em>  where we have:</p>
<ol style="list-style-type: decimal">
<li>A numerical outcome variable <span class="math inline">\(y\)</span> (the instructor’s teaching rating) and</li>
<li>A single numerical explanatory variable <span class="math inline">\(x\)</span> (the average hourly workload for the class).</li>
</ol>
<div id="model1EDA" class="section level3">
<h3>
<span class="header-section-number">10.1.1</span> Exploratory data analysis</h3>
<p>The data on the Q Guide Music Department ratings can be found in the <code>qscores</code> data frame included in the <strong>PPBDS.data</strong> package. However, to keep things simple, let’s <code>select()</code> only the subset of the variables we’ll consider in this chapter, and save this data in a new data frame called <code>qscores_ch10</code>:</p>
<div class="sourceCode" id="cb942"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb942-1"><a href="pitfalls.html#cb942-1"></a><span class="kw">library</span>(PPBDS.data)</span>
<span id="cb942-2"><a href="pitfalls.html#cb942-2"></a></span>
<span id="cb942-3"><a href="pitfalls.html#cb942-3"></a>qscores_ch10 &lt;-<span class="st"> </span>qscores <span class="op">%&gt;%</span></span>
<span id="cb942-4"><a href="pitfalls.html#cb942-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">rating =</span> rating<span class="op">*</span><span class="dv">100</span>) <span class="op">%&gt;%</span></span>
<span id="cb942-5"><a href="pitfalls.html#cb942-5"></a><span class="st">  </span><span class="kw">filter</span>(department <span class="op">==</span><span class="st"> "MUSIC"</span>) <span class="op">%&gt;%</span></span>
<span id="cb942-6"><a href="pitfalls.html#cb942-6"></a><span class="st">  </span><span class="kw">select</span>(number, rating, hours, enrollment)</span></code></pre></div>
<p>A crucial step before doing any kind of analysis or modeling is performing an <em>exploratory data analysis</em>,  or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA:</p>
<!-- DK: Good stuff. We should keep this and follow it, each chapter. -->
<ol style="list-style-type: decimal">
<li>Most crucially, looking at the raw data values.</li>
<li>Computing summary statistics, such as means, medians, and interquartile ranges.</li>
<li>Creating data visualizations.</li>
</ol>
<p>Let’s perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road.</p>
<p>You can do this by using RStudio’s spreadsheet viewer or by using the <code>glimpse()</code> function as introduced in Subsection <a href="#exploredataframes"><strong>??</strong></a> on exploring data frames:</p>
<!-- DK: Add summary() -->
<div class="sourceCode" id="cb943"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb943-1"><a href="pitfalls.html#cb943-1"></a><span class="kw">glimpse</span>(qscores_ch10)</span></code></pre></div>
<pre><code>## Rows: 14
## Columns: 4
## $ number     &lt;chr&gt; "10B", "14B", "161R", "16B", "175R", "189R", "20", "10A", …
## $ rating     &lt;dbl&gt; 400, 490, 440, 480, 420, 480, 400, 420, 470, 480, 480, 490…
## $ hours      &lt;dbl&gt; 3.5, 2.8, 5.2, 2.7, 3.4, 3.7, 4.3, 3.1, 2.6, 3.5, 2.5, 3.7…
## $ enrollment &lt;int&gt; 43, 22, 19, 24, 16, 55, 56, 46, 29, 16, 24, 40, 85, 25</code></pre>
<div class="sourceCode" id="cb945"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb945-1"><a href="pitfalls.html#cb945-1"></a><span class="kw">glimpse</span>(qscores_ch10) <span class="op">%&gt;%</span></span>
<span id="cb945-2"><a href="pitfalls.html#cb945-2"></a><span class="st">  </span><span class="kw">summary</span>()</span></code></pre></div>
<pre><code>## Rows: 14
## Columns: 4
## $ number     &lt;chr&gt; "10B", "14B", "161R", "16B", "175R", "189R", "20", "10A", …
## $ rating     &lt;dbl&gt; 400, 490, 440, 480, 420, 480, 400, 420, 470, 480, 480, 490…
## $ hours      &lt;dbl&gt; 3.5, 2.8, 5.2, 2.7, 3.4, 3.7, 4.3, 3.1, 2.6, 3.5, 2.5, 3.7…
## $ enrollment &lt;int&gt; 43, 22, 19, 24, 16, 55, 56, 46, 29, 16, 24, 40, 85, 25</code></pre>
<pre><code>##     number              rating        hours       enrollment
##  Length:14          Min.   :290   Min.   :2.5   Min.   :16  
##  Class :character   1st Qu.:420   1st Qu.:2.9   1st Qu.:22  
##  Mode  :character   Median :455   Median :3.5   Median :27  
##                     Mean   :441   Mean   :3.5   Mean   :36  
##                     3rd Qu.:480   3rd Qu.:3.7   3rd Qu.:45  
##                     Max.   :490   Max.   :5.2   Max.   :85</code></pre>
<!-- EG: Should I just keep glimpse() with summary(), or should I include an analysis of glimpse() alone before doing both? -->
<p>Observe that <code>Rows: 14</code> indicates that there are 14 rows/observations in <code>qscores_ch10</code>, where each row corresponds to one observed music course at Harvard. It is important to note that the <em>observational unit</em>  is an individual course and not an individual instructor. Recall from Subsection <a href="#exploredataframes"><strong>??</strong></a> that the observational unit is the “type of thing” that is being measured by our variables. Since instructors teach more than one course in an academic year, the same instructor will appear more than once in the data. Hence there are fewer than 748 unique instructors being represented in <code>qscores_ch10</code>.</p>
<p>To further explore the data, we can add a second function to our initial call of <code>glimpse()</code>: the <code>summary()</code> function. <code>summary()</code> can provide us with useful result summaries of all the observations in a dataset for each variable. Calling <code>glimpse()</code> on <code>qscores_ch10</code> and then <code>summary()</code> allows us to see the minimum and maximum values along with several other quantiles for numeric variables and tells us the number of observations, class, and mode of categorical variables. For instance, examining the <code>hours</code> column in the <code>summary()</code> results shows that the minimum number of workload hours reported for a Harvard music class in the Q Guide was 2.5 hours, while the median was 3.5 hours and the maximum was 5.2 hours.</p>
<!-- EG: Commenting this out for the moment "A full description of all the variables included in `evals` can be found at [openintro.org](https://www.openintro.org/data/index.php?data=evals) or by reading the associated help file (run `?evals` in the console)." Is there a place where we have descriptions of the PPBDS library data like this? -->
<p>However, let’s fully describe only the 4 variables we selected in <code>qscores_ch10</code>:</p>
<ol style="list-style-type: decimal">
<li>
<code>number</code>: An identification variable used to distinguish among courses within the same department. Courses in different departments may have the same number.</li>
<li>
<code>rating</code>: A numerical variable of the overall quality of a course, where the average is computed from the evaluation scores from all the students who choose to provide feedback for that course. Ratings of 1 are lowest and 5 are highest. This is the outcome variable <span class="math inline">\(y\)</span> of interest.</li>
<li>
<code>hours</code>: A numerical variable of the amount of work students put into a course per week in hours, where the average is computed from the evaluation scores from all students who choose to provide feedback for that course. This is the explanatory variable <span class="math inline">\(x\)</span> of interest.</li>
<li>
<code>enrollment</code>: A numerical variable of the amount of students enrolled in a course.</li>
</ol>
<p>An alternative way to look at the raw data values is by choosing a random sample of the rows in <code>qscores_ch10</code> by piping it into the <code>sample_n()</code>  function from the <strong>dplyr</strong> package. Here we set the <code>size</code> argument to be <code>5</code>, indicating that we want a random sample of 5 rows. We display the results below. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.</p>
<div class="sourceCode" id="cb948"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb948-1"><a href="pitfalls.html#cb948-1"></a>qscores_ch10 <span class="op">%&gt;%</span></span>
<span id="cb948-2"><a href="pitfalls.html#cb948-2"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dt">size =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 4
##   number rating hours enrollment
##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;int&gt;
## 1 15A       480   3.5         16
## 2 10B       400   3.5         43
## 3 23        430   3.1         85
## 4 175R      420   3.4         16
## 5 189R      480   3.7         55</code></pre>
<!-- EG: Should we still include summarize() if we're including summary() with glimpse() above? Summary() seems to include more information and is generally more useful. Or should we simply compare the two and advise when one could be more useful than the other? -->
<p>Now that we’ve looked at the raw values in our <code>qscores_ch10</code> data frame and got a preliminary sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s start by computing the mean and median of our numerical outcome variable <code>rating</code> and our numerical explanatory variable <code>hours</code>. We’ll do this by using the <code>summarize()</code> function from <code>dplyr</code> along with the <code>mean()</code> and <code>median()</code> summary functions we saw in Section <a href="visualization.html#summarize">1.4.7</a>.</p>
<div class="sourceCode" id="cb950"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb950-1"><a href="pitfalls.html#cb950-1"></a>qscores_ch10 <span class="op">%&gt;%</span></span>
<span id="cb950-2"><a href="pitfalls.html#cb950-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mean_hours =</span> <span class="kw">mean</span>(hours),</span>
<span id="cb950-3"><a href="pitfalls.html#cb950-3"></a>            <span class="dt">mean_rating =</span> <span class="kw">mean</span>(rating),</span>
<span id="cb950-4"><a href="pitfalls.html#cb950-4"></a>            <span class="dt">median_hours =</span> <span class="kw">median</span>(hours),</span>
<span id="cb950-5"><a href="pitfalls.html#cb950-5"></a>            <span class="dt">median_rating =</span> <span class="kw">median</span>(rating))</span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##   mean_hours mean_rating median_hours median_rating
##        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;
## 1       3.46        441.         3.45           455</code></pre>
<!-- DK: This is nice. Having motivated the use of skim() once, we can just go straight to using it in other chapters. And/or show other tricks each chapter, like across(). -->
<p>However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles?</p>
<p>Typing out all these summary statistic functions in <code>summarize()</code> would be long and tedious. Instead, let’s use the convenient <code>skim()</code> function from the <strong>skimr</strong> package. This function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our <code>qscores_ch10</code> data frame, <code>select()</code> only the outcome and explanatory variables teaching <code>score</code> and <code>hours</code>, and pipe them into the <code>skim()</code> function:</p>
<div class="sourceCode" id="cb952"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb952-1"><a href="pitfalls.html#cb952-1"></a>qscores_ch10 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb952-2"><a href="pitfalls.html#cb952-2"></a><span class="st">  </span><span class="kw">select</span>(rating, hours) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb952-3"><a href="pitfalls.html#cb952-3"></a><span class="st">  </span><span class="kw">skim</span>()</span></code></pre></div>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:unnamed-chunk-745">TABLE 10.1: </span>Data summary</span><!--</caption>--></p>
<table><tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">14</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody></table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">rating</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">440.7</td>
<td align="right">54.70</td>
<td align="right">290.0</td>
<td align="right">420.0</td>
<td align="right">455.0</td>
<td align="right">480.0</td>
<td align="right">490.0</td>
<td align="left">▁▁▂▅▇</td>
</tr>
<tr class="even">
<td align="left">hours</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3.5</td>
<td align="right">0.77</td>
<td align="right">2.5</td>
<td align="right">2.9</td>
<td align="right">3.5</td>
<td align="right">3.7</td>
<td align="right">5.2</td>
<td align="left">▆▇▃▃▂</td>
</tr>
</tbody>
</table>
<p>For the numerical variables teaching <code>rating</code> and <code>hours</code> it returns:</p>
<ul>
<li>
<code>n_missing</code>: the number of missing values</li>
<li>
<code>complete_rate</code>: the percentage of non-missing or complete values</li>
<li>
<code>mean</code>: the average</li>
<li>
<code>sd</code>: the standard deviation</li>
<li>
<code>p0</code>: the 0th percentile: the value at which 0% of observations are smaller than it (the <em>minimum</em> value)</li>
<li>
<code>p25</code>: the 25th percentile: the value at which 25% of observations are smaller than it (the <em>1st quartile</em>)</li>
<li>
<code>p50</code>: the 50th percentile: the value at which 50% of observations are smaller than it (the <em>2nd</em> quartile and more commonly called the <em>median</em>)</li>
<li>
<code>p75</code>: the 75th percentile: the value at which 75% of observations are smaller than it (the <em>3rd quartile</em>)</li>
<li>
<code>p100</code>: the 100th percentile: the value at which 100% of observations are smaller than it (the <em>maximum</em> value)</li>
</ul>
<p>Looking at this output, we can see how the values of both variables are distributed. For example, the mean music course rating was 4.41 out of 5, whereas the mean workload per week was 3.46 hours. Furthermore, the middle 50% of course ratings was between 4.2 and 4.8 (the first and third quartiles), whereas the middle 50% of hours of work falls within 2.87 to 3.7, with a maximum reported workload of 5.2 hours per week.</p>
<!-- DK: Keep this. -->
<p>The <code>skim()</code> function only returns what are known as <em>univariate</em>  summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist <em>bivariate</em>  summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the  <em>correlation coefficient</em>. Generally speaking, <em>coefficients</em> are quantitative expressions of a specific phenomenon. A <em>correlation coefficient</em> is a quantitative expression of the <em>strength of the linear relationship between two numerical variables</em>. Its value ranges between -1 and 1 where:</p>
<ul>
<li>-1 indicates a perfect <em>negative relationship</em>: As one variable increases, the value of the other variable tends to go down, following a straight line.</li>
<li>0 indicates no relationship: The values of both variables go up/down independently of each other.</li>
<li>+1 indicates a perfect <em>positive relationship</em>: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.</li>
</ul>
<p>The following figure gives examples of 9 different correlation coefficient values for hypothetical numerical variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, but it is not as strong as the negative linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> when the correlation coefficient is -0.9 or -1.</p>
<div class="figure">
<span id="fig:unnamed-chunk-746"></span>
<p class="caption marginnote shownote">
FIGURE 10.1: Nine different correlation coefficients.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-746-1.png" alt="Nine different correlation coefficients." width="672">
</div>
<p>The correlation coefficient can be computed using the <code>cor()</code> summary function within a <code>summarize()</code>:</p>
<div class="sourceCode" id="cb953"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb953-1"><a href="pitfalls.html#cb953-1"></a>qscores_ch10 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb953-2"><a href="pitfalls.html#cb953-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">correlation =</span> <span class="kw">cor</span>(rating, hours))</span></code></pre></div>
<p>In our case, the correlation coefficient of -0.49 indicates that the relationship between overall course rating and average weekly workload in hours is negative. There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren’t close to the extreme values of -1, 0, and 1.</p>
<p>Let’s now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the <code>rating</code> and <code>hours</code> variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let’s do this using <code>geom_point()</code> and display the result. Furthermore, let’s highlight the six points in the top right of the visualization in a box.</p>
<div class="sourceCode" id="cb954"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb954-1"><a href="pitfalls.html#cb954-1"></a>qscores_ch10 <span class="op">%&gt;%</span></span>
<span id="cb954-2"><a href="pitfalls.html#cb954-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> hours, <span class="dt">y =</span> rating)) <span class="op">+</span></span>
<span id="cb954-3"><a href="pitfalls.html#cb954-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb954-4"><a href="pitfalls.html#cb954-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Hours of Work Per Week"</span>, </span>
<span id="cb954-5"><a href="pitfalls.html#cb954-5"></a>       <span class="dt">y =</span> <span class="st">"Q Guide Rating"</span>,</span>
<span id="cb954-6"><a href="pitfalls.html#cb954-6"></a>       <span class="dt">title =</span> <span class="st">"Scatterplot of relationship between Q Scores and Weekly Workload"</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:unnamed-chunk-750"></span>
<p class="caption marginnote shownote">
FIGURE 10.2: Q Guide Scores at Harvard
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-750-1.png" alt="Q Guide Scores at Harvard" width="672">
</div>
<!-- EG: Need to be more precise with this in the future. Will come back and edit later. -->
<p>Observe that most courses have reported average workloads between 2 and 10 hours per week, while most teaching scores lie between 3 and 5. Furthermore, while opinions may vary, it is our opinion that the relationship between Q guide rating and weekly workload in hours is “weakly negative.” This is consistent with our earlier computed correlation coefficient of -0.49.</p>
<!-- EG: I'll change this language below as well once I figure out how to work on the box. -->
<p>Furthermore, there appear to be six points in the top-right of this plot highlighted in the box. However, this is not actually the case, as this plot suffers from <em>overplotting</em>. Recall from Subsection <a href="#overplotting"><strong>??</strong></a> that overplotting occurs when several points are stacked directly on top of each other, making it difficult to distinguish them. So while it may appear that there are only six points in the box, there are actually more. This fact is only apparent when using <code>geom_jitter()</code> in place of <code>geom_point()</code>. We display the resulting plot along with the same small box as before.</p>
<div class="sourceCode" id="cb955"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb955-1"><a href="pitfalls.html#cb955-1"></a>qscores_ch10 <span class="op">%&gt;%</span></span>
<span id="cb955-2"><a href="pitfalls.html#cb955-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> hours, <span class="dt">y =</span> rating)) <span class="op">+</span></span>
<span id="cb955-3"><a href="pitfalls.html#cb955-3"></a><span class="st">  </span><span class="kw">geom_jitter</span>() <span class="op">+</span></span>
<span id="cb955-4"><a href="pitfalls.html#cb955-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Hours of Work Per Week"</span>, <span class="dt">y =</span> <span class="st">"Q Guide Score"</span>,</span>
<span id="cb955-5"><a href="pitfalls.html#cb955-5"></a>       <span class="dt">title =</span> <span class="st">"Scatterplot of relationship between Q Scores and Weekly Workload"</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:unnamed-chunk-752"></span>
<p class="caption marginnote shownote">
FIGURE 10.3: Q Guide Scores at Harvard.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-752-1.png" alt="Q Guide Scores at Harvard." width="672">
</div>
<!-- EG: I'll change this section as well after changing the box size. -->
<p>It is now apparent that there are ?? points in the area highlighted in the box and not six as originally suggested. Recall from Subsection <a href="#overplotting"><strong>??</strong></a> on overplotting that jittering adds a little random “nudge” to each of the points to break up these ties. Furthermore, recall that jittering is strictly a visualization tool; it does not alter the original values in the data frame <code>qscores_ch10</code>. To keep things simple going forward, however, we’ll only present regular scatterplots rather than their jittered counterparts.</p>
<p>Let’s build on the unjittered scatterplot by adding a “best-fitting” line: of all possible lines we can draw on this scatterplot, it is the line that “best” fits through the cloud of points. We do this by adding a new <code>geom_smooth(method = "lm", se = FALSE)</code> layer to the <code>ggplot()</code> code that created the scatterplot. The <code>method = "lm"</code> argument sets the line to be a “<code>l</code>inear <code>m</code>odel.” The <code>se = FALSE</code>  argument suppresses <em>standard error</em> uncertainty bars. (We defined the concept of <em>standard error</em> in Subsection <a href="one-parameter.html#sampling-definitions">6.4.2</a>.)</p>
<div class="sourceCode" id="cb956"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb956-1"><a href="pitfalls.html#cb956-1"></a>qscores_ch10 <span class="op">%&gt;%</span></span>
<span id="cb956-2"><a href="pitfalls.html#cb956-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> hours, <span class="dt">y =</span> rating)) <span class="op">+</span></span>
<span id="cb956-3"><a href="pitfalls.html#cb956-3"></a><span class="st">  </span><span class="kw">geom_jitter</span>() <span class="op">+</span></span>
<span id="cb956-4"><a href="pitfalls.html#cb956-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Hours of Work Per Week"</span>, <span class="dt">y =</span> <span class="st">"Q Guide Score"</span>,</span>
<span id="cb956-5"><a href="pitfalls.html#cb956-5"></a>       <span class="dt">title =</span> <span class="st">"Scatterplot of relationship between Q Scores and Weekly Workload for Music Classes"</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb956-6"><a href="pitfalls.html#cb956-6"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">"lm"</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula 'y ~ x'</code></pre>
<div class="figure">
<span id="fig:unnamed-chunk-753"></span>
<p class="caption marginnote shownote">
FIGURE 10.4: Regression line.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-753-1.png" alt="Regression line." width="672">
</div>
<p>The line in the resulting figure is called a “regression line.” The regression line  is a visual summary of the relationship between two numerical variables, in our case the outcome variable <code>rating</code> and the explanatory variable <code>hours</code>. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of -0.49 suggesting that there is a negative relationship between these two variables: as students report higher average weekly workloads for music classes, courses receive lower teaching evaluations. We’ll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value.</p>
<p>Furthermore, a regression line is “best-fitting” in that it minimizes some mathematical criteria. We present these mathematical criteria in Section <a href="#leastsquares"><strong>??</strong></a>, but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable.</p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
# Machine Learning {#machine-learning}
=======
### Prudence</p>
</div>
</div>
<div id="machine-learning" class="section level2">
<h2>
<span class="header-section-number">10.2</span> Machine Learning</h2>
<!-- AR: I used material from this blog post: https://alison.rbind.io/post/2020-02-27-better-tidymodels/#tidymodels-101
and from this github:
https://github.com/rstudio-conf-2020/applied-ml
Should be added to the Acknowledgements section -->
<!-- Model Review: Not for this week, but maybe for next week. Add a section which reviews how we can re-estime all these models using tidymodels. (I know that you do some of that in chapter 13, but now we are more formal and thorough.) This also provides a nice occasion to review what we have learned, to see the larger framework. We don't start with a model. We start with the data. The type of Y variable we have guides our model choice. Maybe we should add another model which can be used on continuos data. Maybe random forests? Would be nice to compare two models for continuous data and three for categorical data. Not this week! -->
<!-- Other tutorials: -->
<!-- https://github.com/rstudio-conf-2020/applied-ml -->
<!-- https://dnield.com/posts/tidymodels-intro/ -->
<!-- https://alison.rbind.io/post/2020-02-27-better-tidymodels/#tidymodels-101 -->
<!-- A much better book is: https://bradleyboehmke.github.io/HOML/. But that won't be open for another year. And, it is too advanced. And it does not quite use all the latest machine learning approaches in R. But the style is nice, and perhaps worth emulating. A related issue is tying this work back to the last three weeks. After all, in each of the last three weeks we fitted hundreds of models. Isn't that what machine learning is? Sort of! Big difference is that, previously, the separate models have shared no data with each other. With machine learning, they will! How can we teach this in a way which is closest to the approaches they have learned? Hmmm. -->
<p>Now that we have spent some time exploring these Q scores, it is time to dive in deeper and investigate interesting relationships within the data. Thus far, we have learned four models: linear regression, logistic regression, CART, and random forest. But there are hundreds more! We need a consistent way to try lots of models and to compare them. Hence, we introduced in the last chapter the <strong>tidymodels</strong> collection of packages.</p>
<p>But with so many models to choose from, how do we know which models are best? Recall that we came up with several different models for each of our example data sets. Which one should we use? The framework of <strong>machine learning</strong> helps.</p>
<p>Perhaps the most popular data science methodologies come from the field of machine learning. Machine learning is the study and application of algorithms that learn from and make predictions on data. From search results to self-driving cars, it has manifested itself in all areas of our lives and is one of the most exciting and fast-growing fields of research in the world of data science. Machine learning success stories include the handwritten zip code readers implemented by the postal service, speech recognition technology such as Apple’s Siri, movie recommendation systems, spam and malware detectors, housing price predictors, and driverless cars.</p>
<p>There are a wide variety of machine learning algorithms, including the models we have learned so far. We won’t introduce new models this chapter. Rather, we’ll use linear regression as an example for how to apply machine learning techniques to improve your predictive modeling.</p>
</div>
<div id="the-process-of-machine-learning" class="section level2">
<h2>
<span class="header-section-number">10.3</span> The process of machine learning</h2>
<p>In machine learning, modeling is a <em>process</em>, not a single step. Common steps during model building are:</p>
<ul>
<li>Estimating model parameters (i.e. training models)</li>
<li>Determining the values of tuning parameters that cannot be directly calculated from the data</li>
<li>Model selection (within a model type) and model comparison (between types)</li>
<li>Calculating the performance of the final model that will generalize to new data</li>
</ul>
<p>Many books and courses portray predictive modeling as a short sprint. A better analogy would be a marathon or campaign (depending on how hard the problem is).</p>
<p>We often think of the model as the only real data analysis step in this process. However, there are other procedures that are often applied before or after the model fit that are data-driven and have an impact. If we only think of the model as being important, we might end up accidentally <em>overfitting</em> to the data in-hand. This is very similar to the problems of “the garden of forking paths” and “<a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">p-hacking</a>.”</p>
<p>Let’s conceptualize a process or <em>workflow</em> that involves all of the steps where the data are analyzed in a significant way. This includes the model but might also include other <em>estimation</em> steps. Admittedly, there is some grey area here. This includes data preparation steps (e.g., imputation, encoding, transformations) and selection of which terms go into the model.</p>
<p>This concept of a “modeling workflow” will become important when we talk about measuring performance of the modeling process. Ultimately, when we evaluate models, we are evaluating the whole process. All the steps involved in the process can affect the performance of the final model.</p>
</div>
<div id="what-does-it-mean-for-a-model-to-be-good" class="section level2">
<h2>
<span class="header-section-number">10.4</span> What does it mean for a model to be “good?”</h2>
<p>Before we start describing machine learning approaches to optimize the way we build models, we first need to define what we mean when we say one approach is better than another. In this section, we focus on describing ways in which machine learning algorithms are evaluated. Specifically, we need to quantify what we mean by “better.”</p>
<div id="training-and-test-sets" class="section level3">
<h3>
<span class="header-section-number">10.4.1</span> Training and test sets</h3>
<p>Ultimately, a machine learning algorithm should be evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only <em>after</em> we are done constructing it. We refer to the group for which we know the outcome, and use to develop the algorithm, as the <em>training</em> set. We refer to the group for which we pretend we don’t know the outcome as the <em>test</em> set. A standard way of generating the training and test sets is by randomly splitting the data.</p>
<ul>
<li>Training Set: these data are used to estimate model parameters and to pick the values of the complexity parameter(s) for the model.</li>
<li>Test Set: these data can be used to get an independent assessment of model efficacy. They should not be used during model training.</li>
</ul>
<p>We then develop an algorithm using <strong>only</strong> the training set. Once we are done developing the algorithm, we will <em>freeze</em> it and evaluate it using the test set. But remember, <strong>it is important that we optimize the model using only the training set</strong>: the test set is only for evaluation. Evaluating an algorithm on the training set can lead to <em>overfitting</em>, which often results in dangerously over-optimistic assessments.</p>
</div>
<div id="loss-function" class="section level3">
<h3>
<span class="header-section-number">10.4.2</span> The loss function</h3>
<p>The general approach to defining “best” in machine learning is to define a <em>loss function</em>. This concept can be applied to both categorical and continuous data.</p>
<p>The most commonly used loss function is the <em>squared loss function</em>.<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> Note that there are loss functions other than the squared loss. For example, the <em>Mean Absolute Error</em> uses absolute values, <span class="math inline">\(|\hat{Y}_i - Y_i|\)</span> instead of squaring the errors <span class="math inline">\((\hat{Y}_i - Y_i)^2\)</span>. However, in this chapter we focus on minimizing square loss since it is the most widely used.</span> If <span class="math inline">\(\hat{y}\)</span> is our predictor and <span class="math inline">\(y\)</span> is the observed outcome, the squared loss function is simply:</p>
<p><span class="math display">\[
(\hat{y} - y)^2
\]</span></p>
<p>Because we often have a test set with many observations, say <span class="math inline">\(N\)</span>, we use the mean squared error (MSE):</p>
<p><span class="math display">\[
MSE = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span></p>
<p>In practice, we often report the root mean squared error (RMSE), which is <span class="math inline">\(\sqrt{\mbox{MSE}}\)</span>, because it is in the same units as the outcomes.<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> Doing the math is often easier with the MSE and it is therefore more commonly used in textbooks, since these usually describe theoretical properties of algorithms.</span></p>
<p>If the outcomes are binary, both RMSE and MSE are equivalent to accuracy, since <span class="math inline">\((\hat{y} - y)^2\)</span> is 0 if the prediction was correct and 1 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.</p>
</div>
</div>
<div id="data-q-scores-for-harvard-academic-year-2018-2019" class="section level2">
<h2>
<span class="header-section-number">10.5</span> Data: Q Scores for Harvard Academic Year 2018-2019</h2>
<p>Now that we have a way to evaluate models, we’re almost ready to start going through the modeling process using <strong>tidymodels</strong>. However, we’ll first need some data! We will continue to use the data from our earlier EDA, but this time extend our focus to the entire dataset rather than simply the scores for the Music department.</p>
<p>Imagine that you are a professional data scientist. A Harvard professor comes to you and says that she wants to increase the average Q Guide scores of the courses she teaches. As a result, she asks you to build a model that will predict the most important factors in influencing higher Q Guide scores. But with so many potential models at your disposal, which ought you to choose? How can you decide which variables you should include in your analysis? How can you be certain that this model will accurately predict the most important factors correlated with higher Q Guide scores? All of these questions can be addressed by using tidymodels, as we will do now.</p>
<p>Let’s start by taking a look at the <code>qscores</code> object:</p>
<div class="sourceCode" id="cb958"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb958-1"><a href="pitfalls.html#cb958-1"></a>qscores</span></code></pre></div>
<pre><code>## # A tibble: 748 x 8
##    name             department number term   enrollment hours rating instructor 
##    &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;       &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      
##  1 Introduction to… AFRAMER    100Y   2019-…         49   2.6    4.2 Jesse McCa…
##  2 American Democr… AFRAMER    123Z   2019-…         49   3.6    4.4 Cornel West
##  3 Urban Inequalit… AFRAMER    125X   2019-…         40   5.2    4.5 Elizabeth …
##  4 Richard Wright   AFRAMER    130X   2019-…         23   7.2    4.4 Glenda Car…
##  5 19th century Bl… AFRAMER    131Y   2019-…         20   3.5    4.9 Linda Chav…
##  6 Social Revoluti… AFRAMER    199X   2019-…         19   7.2    4.8 Alejandro …
##  7 Martin Luther K… AFRAMER    199Y   2019-…         40   4.2    4.7 Brandon Mi…
##  8 Elementary Afri… AFRIKAAN   AB     2019-…         22   2.9    4.9 John M Mug…
##  9 Elementary Jama… JAMAICAN   AB     2019-…         18   1.5    4.9 John M Mug…
## 10 Elementary West… WSTAFRCN   AB     2019-…         29   2.6    4   John M Mug…
## # … with 738 more rows</code></pre>
<p>We have a lot of observations (more than 700), each of which corresponds to a single course at Harvard.</p>
<p>The outcome we’ll focus on, as we did before, is <em>rating</em>.</p>
<p>Let’s <code>select()</code> the variables we’ll be using and <code>glimpse()</code> our tibble:</p>
<div class="sourceCode" id="cb960"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb960-1"><a href="pitfalls.html#cb960-1"></a>qscores &lt;-<span class="st"> </span>qscores <span class="op">%&gt;%</span></span>
<span id="cb960-2"><a href="pitfalls.html#cb960-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">division =</span> <span class="kw">ifelse</span>(department <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">"AFRAMER"</span>, <span class="st">"AFRIKAAN"</span>, <span class="st">"AMSTDIES"</span>, <span class="st">"ANTHRO"</span>, <span class="st">"ECON"</span>, <span class="st">"ESPP"</span>, <span class="st">"GHHP"</span>, <span class="st">"GOV"</span>, <span class="st">"HIST"</span>, <span class="st">"HISTSCI"</span>, <span class="st">"JAMAICAN"</span>, <span class="st">"PSY"</span>, <span class="st">"SOCIOL"</span>, <span class="st">"WSTAFRCN"</span>, <span class="st">"ZULU"</span>), <span class="st">"Social Sciences"</span>, department),</span>
<span id="cb960-3"><a href="pitfalls.html#cb960-3"></a>         <span class="dt">division =</span> <span class="kw">ifelse</span>(department <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">"APCOMP"</span>, <span class="st">"APMTH"</span>, <span class="st">"APPHY"</span>, <span class="st">"ASTRON"</span>, <span class="st">"BE"</span>, <span class="st">"BIOPHYS"</span>, <span class="st">"BIOSTAT"</span>, <span class="st">"CELLBIO"</span>, <span class="st">"CHEM"</span>, <span class="st">"COMPSCI"</span>,  <span class="st">"E-PSCI"</span>, <span class="st">"ENG-SCI"</span>, <span class="st">"ESE"</span>, <span class="st">"HBTM"</span>, <span class="st">"HEB"</span>, <span class="st">"IMMUN"</span>, <span class="st">"LIFESCI"</span>, <span class="st">"LPS"</span>, <span class="st">"MATH"</span>, <span class="st">"MBB"</span>, <span class="st">"MCB"</span>, <span class="st">"PHYSCI"</span>, <span class="st">"PHYSICS"</span>, <span class="st">"NEURO"</span>, <span class="st">"OEB"</span>, <span class="st">"SCRB"</span>, <span class="st">"STAT"</span>), <span class="st">"SEAS"</span>, department),</span>
<span id="cb960-4"><a href="pitfalls.html#cb960-4"></a>         <span class="dt">division =</span> <span class="kw">ifelse</span>(department <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">"ARABIC"</span>, <span class="st">"ANE"</span>, <span class="st">"CELTIC"</span>, <span class="st">"CHNSE"</span>, <span class="st">"CHNSHIS"</span>, <span class="st">"CLS-STDY"</span>, <span class="st">"COMPLIT"</span>, <span class="st">"EAFM"</span>, <span class="st">"EASTD"</span>, <span class="st">"ENGLISH"</span>, <span class="st">"FOLKMYTH"</span>, <span class="st">"FRENCH"</span>, <span class="st">"HAA"</span>, <span class="st">"HIND-URD"</span>, <span class="st">"HIST-LIT"</span>, <span class="st">"HUMAN"</span>, <span class="st">"ISLAMCIV"</span>, <span class="st">"JAPAN"</span>, <span class="st">"JAPANLIT"</span>, <span class="st">"KOREAN"</span>, <span class="st">"LATIN"</span>, <span class="st">"LING"</span>, <span class="st">"MUSIC"</span>, <span class="st">"PHIL"</span>, <span class="st">"ROM-STD"</span>, <span class="st">"RUSS"</span>, <span class="st">"SAS"</span>, <span class="st">"SLAVIC"</span>, <span class="st">"SPANSH"</span>, <span class="st">"SWEDISH"</span>, <span class="st">"TDM"</span>, <span class="st">"VES"</span>, <span class="st">"WOMGEN"</span>), <span class="st">"Arts and Humanities"</span>, department),</span>
<span id="cb960-5"><a href="pitfalls.html#cb960-5"></a>         <span class="dt">division =</span> <span class="kw">ifelse</span>(department <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">"AESTHINT"</span>, <span class="st">"CULTBLF"</span>, <span class="st">"EMREAS"</span>, <span class="st">"ETHRSON"</span>, <span class="st">"SCILIVSY"</span>, <span class="st">"SCIPHUNV"</span>, <span class="st">"SOCWORLD"</span>, <span class="st">"US-WORLD"</span>), <span class="st">"General Education"</span>, department),</span>
<span id="cb960-6"><a href="pitfalls.html#cb960-6"></a>         <span class="dt">division =</span> <span class="kw">ifelse</span>(department <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">"EXPOS"</span>, <span class="st">"FRSEMR"</span>), <span class="st">"First-Year Courses"</span>, department)) <span class="op">%&gt;%</span></span>
<span id="cb960-7"><a href="pitfalls.html#cb960-7"></a><span class="st">  </span><span class="kw">select</span>(rating, hours, department, enrollment, term, division)</span>
<span id="cb960-8"><a href="pitfalls.html#cb960-8"></a><span class="co"># Two of the four Humanities courses, which I included in Arts and Humanities, are first-year exclusive courses. Should I exclude Humanities entirely?</span></span>
<span id="cb960-9"><a href="pitfalls.html#cb960-9"></a><span class="kw">glimpse</span>(qscores)</span></code></pre></div>
<pre><code>## Rows: 748
## Columns: 6
## $ rating     &lt;dbl&gt; 4.2, 4.4, 4.5, 4.4, 4.9, 4.8, 4.7, 4.9, 4.9, 4.0, 4.8, 4.7…
## $ hours      &lt;dbl&gt; 2.6, 3.6, 5.2, 7.2, 3.5, 7.2, 4.2, 2.9, 1.5, 2.6, 2.6, 3.5…
## $ department &lt;chr&gt; "AFRAMER", "AFRAMER", "AFRAMER", "AFRAMER", "AFRAMER", "AF…
## $ enrollment &lt;int&gt; 49, 49, 40, 23, 20, 19, 40, 22, 18, 29, 35, 17, 17, 21, 30…
## $ term       &lt;chr&gt; "2019-Spring", "2019-Spring", "2019-Spring", "2019-Spring"…
## $ division   &lt;chr&gt; "AFRAMER", "AFRAMER", "AFRAMER", "AFRAMER", "AFRAMER", "AF…</code></pre>
<p>Now we have one outcome (<code>rating</code>, which ranges from 1 to 5) and three potential predictors:</p>
<ul>
<li><code>hours</code></li>
<li><code>department</code></li>
<li><code>enrollment</code></li>
<li><code>term</code></li>
<li><code>division</code></li>
</ul>
<p>In past chapters, we’ve shown example models with one or two predictors, perhaps with an interaction. When you have more variables in your dataset, how can you decide which predictors to include? The techniques of machine learning can help answer this question.</p>
<p>For this chapter, we’ll consider x possible models. Let’s consider the following combinations:
<!--EG: Which interactions should be performed? -->
1. <code>hours</code> alone
1. <code>hours</code>, <code>enrollment</code>, <code>department</code>, <code>term</code>, and <code>division</code>
1. Same as above plus <code>enrollment</code>interacted with <code>department</code>
1. ?
1. The kitchen sink: ?</p>
<p>Of course, these are a small subset of the possible models we could consider, either with the variables we have selected or with the larger set of all the variables in the qscores. But we’ll use these as examples for the machine learning techniques in this chapter; if you’d like, you can use the methods we learn here to test additional models.</p>
<p>Let’s save these as <code>formula</code> objects in R, so we can easily access them later. We’ll start with the simplest model we’ll consider, as <code>basic_form</code>:</p>
<div class="sourceCode" id="cb962"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb962-1"><a href="pitfalls.html#cb962-1"></a>basic_form &lt;-<span class="st"> </span><span class="kw">formula</span>(rating <span class="op">~</span><span class="st"> </span>hours)</span></code></pre></div>
<p>Next, we can use <code>update()</code> to create the more complicated formulas. <code>update()</code> takes as its first argument a formula and as its second argument the additions you want to make. To keep all the predictors from the first formula and add more, you will start with <code>~ . +</code> and then add more predictors, like so:</p>
<div class="sourceCode" id="cb963"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb963-1"><a href="pitfalls.html#cb963-1"></a>adding_terms &lt;-<span class="st"> </span><span class="kw">update</span>(basic_form,</span>
<span id="cb963-2"><a href="pitfalls.html#cb963-2"></a>                    <span class="op">~</span><span class="st"> </span>enrollment <span class="op">+</span><span class="st"> </span>department <span class="op">+</span><span class="st"> </span>division <span class="op">+</span><span class="st"> </span>term)</span>
<span id="cb963-3"><a href="pitfalls.html#cb963-3"></a></span>
<span id="cb963-4"><a href="pitfalls.html#cb963-4"></a>adding_interaction &lt;-<span class="st"> </span><span class="kw">update</span>(adding_terms,</span>
<span id="cb963-5"><a href="pitfalls.html#cb963-5"></a>                    <span class="op">~</span><span class="st"> </span>enrollment <span class="op">*</span><span class="st"> </span>department)</span>
<span id="cb963-6"><a href="pitfalls.html#cb963-6"></a></span>
<span id="cb963-7"><a href="pitfalls.html#cb963-7"></a><span class="co"># more_interactions &lt;- </span></span></code></pre></div>
<!-- EG: Commenting out for the moment until I figure out exactly which models to use: "Since the last model is the same as `more_interactions` but with `ideo5 * pres_gop` added, we'll use `update(demo_interact_form)` rather than `update(basic_form)` here:" -->
<div class="sourceCode" id="cb964"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb964-1"><a href="pitfalls.html#cb964-1"></a><span class="co"># full_form &lt;- </span></span></code></pre></div>
<p>Now we have five <code>formula</code> objects we can use to fit models.</p>
<p>So we can access them easily, we’ll save them in a tibble and give them easy-to-remember names:</p>
<div class="sourceCode" id="cb965"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb965-1"><a href="pitfalls.html#cb965-1"></a>qscores_formulas &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">formula =</span> <span class="kw">c</span>(basic_form,</span>
<span id="cb965-2"><a href="pitfalls.html#cb965-2"></a>                                    adding_terms,</span>
<span id="cb965-3"><a href="pitfalls.html#cb965-3"></a>                                    adding_interaction),</span>
<span id="cb965-4"><a href="pitfalls.html#cb965-4"></a>                                    <span class="co">#more_interactions,</span></span>
<span id="cb965-5"><a href="pitfalls.html#cb965-5"></a>                                    <span class="co">#full_form),</span></span>
<span id="cb965-6"><a href="pitfalls.html#cb965-6"></a>                       <span class="dt">group =</span> <span class="kw">c</span>(<span class="st">"Basic model"</span>,</span>
<span id="cb965-7"><a href="pitfalls.html#cb965-7"></a>                                 <span class="st">"Multiple terms model"</span>,</span>
<span id="cb965-8"><a href="pitfalls.html#cb965-8"></a>                                 <span class="st">"Single interaction model"</span>))</span>
<span id="cb965-9"><a href="pitfalls.html#cb965-9"></a>                                 <span class="co">#"Multiple interactions model",</span></span>
<span id="cb965-10"><a href="pitfalls.html#cb965-10"></a>                                 <span class="co">#"Full model"))</span></span></code></pre></div>
<div id="justice" class="section level3">
<h3>
<span class="header-section-number">10.5.1</span> Justice</h3>
<!-- Preceptor Table. This data is for 2018/2019.  -->
<!-- math formula -->
</div>
</div>
<div id="the-modeling-process-using-tidymodels" class="section level2">
<h2>
<span class="header-section-number">10.6</span> The modeling process using <strong>tidymodels</strong>
</h2>
<p>If you are using <strong>tidymodels</strong>, many machine learning tasks are simplified, since you can use the same kind of code as the building blocks for any predictive modeling pipeline.</p>
<div id="parsnip-build-the-model" class="section level3">
<h3>
<span class="header-section-number">10.6.1</span> <strong>parsnip</strong>: build the model</h3>
<p>This step is really three, using only the <a href="https://tidymodels.github.io/parsnip/"><strong>parsnip</strong> package</a>. In this setp, we can choose the <em>model</em>, the <em>engine</em> to run the model in R, and, for some models, the <em>mode</em>. Here, our model will be linear regression, the engine <code>lm</code>, and the mode “regression” (the only possible mode for a linear regression).</p>
<div class="sourceCode" id="cb966"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb966-1"><a href="pitfalls.html#cb966-1"></a>lm_spec &lt;-<span class="st"> </span></span>
<span id="cb966-2"><a href="pitfalls.html#cb966-2"></a><span class="st">  </span></span>
<span id="cb966-3"><a href="pitfalls.html#cb966-3"></a><span class="st">  </span><span class="co"># Pick model</span></span>
<span id="cb966-4"><a href="pitfalls.html#cb966-4"></a><span class="st">  </span></span>
<span id="cb966-5"><a href="pitfalls.html#cb966-5"></a><span class="st">  </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb966-6"><a href="pitfalls.html#cb966-6"></a><span class="st">  </span></span>
<span id="cb966-7"><a href="pitfalls.html#cb966-7"></a><span class="st">  </span><span class="co"># Set engine</span></span>
<span id="cb966-8"><a href="pitfalls.html#cb966-8"></a><span class="st">  </span></span>
<span id="cb966-9"><a href="pitfalls.html#cb966-9"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"lm"</span>) <span class="op">%&gt;%</span></span>
<span id="cb966-10"><a href="pitfalls.html#cb966-10"></a><span class="st">  </span></span>
<span id="cb966-11"><a href="pitfalls.html#cb966-11"></a><span class="st">  </span><span class="co"># Set mode</span></span>
<span id="cb966-12"><a href="pitfalls.html#cb966-12"></a><span class="st">  </span></span>
<span id="cb966-13"><a href="pitfalls.html#cb966-13"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">"regression"</span>) </span>
<span id="cb966-14"><a href="pitfalls.html#cb966-14"></a></span>
<span id="cb966-15"><a href="pitfalls.html#cb966-15"></a>lm_spec</span></code></pre></div>
<pre><code>## Linear Regression Model Specification (regression)
## 
## Computational engine: lm</code></pre>
<p>To keep things simple, we’ll only be evaluating linear regressions in this chapter, although there are many other modeling choices one could make for predicting Q Guide ratings, some of which may be superior.<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> For instance, linear regression allows for predicted values which are below 1 and above 5, which are theoretically forbidden. Furthermore, linear regression assumes that the distance between each response category is the same, since the distance between 1 and 2 is the same as 2 and 3, and so on, but there may be real world “breakpoints,” for instance if it is more important to go from neutral to somewhat approval than from somewhat approval to strong approval. However, for the purposes of this chapter, we will proceed with linear regression.</span> Note that you could evaluate the performances of those other models using the same building blocks of code that we show you here.</p>
<p>Things that are missing: data (we haven’t touched it yet) and a formula (no data, no variables, no twiddle ~). This is an abstract model specification. See other possible parsnip models <a href="https://tidymodels.github.io/parsnip/articles/articles/Models.html">here</a>.</p>
</div>
<div id="recipes-not-happening-here-folks" class="section level3">
<h3>
<span class="header-section-number">10.6.2</span> <strong>recipes</strong>: not happening here, folks</h3>
<p>This is where one would normally insert some code for <em>feature engineering</em> using the <strong>recipes</strong> package. Feature engineering involves transforming your data to create different predictors, such as by taking log transformations, turning numerical variables into factors or vise versa, and so on. We engaged in some rudimentary feature engineering when we <code>mutate</code>d the CCES at the beginning of this chapter. But for the purposes of this chapter, we will treat our data as-is.</p>
</div>
<div id="rsample-initial-split" class="section level3">
<h3>
<span class="header-section-number">10.6.3</span> <strong>rsample</strong>: initial split</h3>
<p>We’ll use the <a href="https://tidymodels.github.io/rsample/"><strong>rsample</strong> package</a> to split the qscores data up into two datasets: training and testing. The <code>initial_split()</code> function takes a dataset and splits it into a training and test set. By default, 75% of the data is kept in the training set and the rest are allocated to the test set. This can be changed with the <code>prop</code> argument – we’ll set it at 0.8. Because the split is done at random, we need to use <code>set.seed()</code> to ensure our results are replicable.</p>
<div class="sourceCode" id="cb968"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb968-1"><a href="pitfalls.html#cb968-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb968-2"><a href="pitfalls.html#cb968-2"></a></span>
<span id="cb968-3"><a href="pitfalls.html#cb968-3"></a>qscores_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(qscores, <span class="dt">prop =</span> <span class="fl">0.8</span>)</span>
<span id="cb968-4"><a href="pitfalls.html#cb968-4"></a>qscores_train &lt;-<span class="st"> </span><span class="kw">training</span>(qscores_split)</span>
<span id="cb968-5"><a href="pitfalls.html#cb968-5"></a>qscores_test &lt;-<span class="st"> </span><span class="kw">testing</span>(qscores_split)</span></code></pre></div>
</div>
<div id="fitting-the-model-once" class="section level3">
<h3>
<span class="header-section-number">10.6.4</span> Fitting the model once</h3>
<p>Fitting a single model once is… not <em>exactly</em> the hardest part.</p>
<p>First, we can get the fitted model using the <code>fit()</code> function:</p>
<div class="sourceCode" id="cb969"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb969-1"><a href="pitfalls.html#cb969-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb969-2"><a href="pitfalls.html#cb969-2"></a><span class="st">  </span><span class="kw">fit</span>(basic_form, <span class="dt">data =</span> qscores_train)</span></code></pre></div>
<pre><code>## parsnip model object
## 
## Fit time:  5ms 
## 
## Call:
## stats::lm(formula = rating ~ hours, data = data)
## 
## Coefficients:
## (Intercept)        hours  
##      4.2802      -0.0165</code></pre>
<p>Note that we can use <code>tidy()</code>, just like we did in previous chapters, to take a look at the results:</p>
<div class="sourceCode" id="cb971"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb971-1"><a href="pitfalls.html#cb971-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb971-2"><a href="pitfalls.html#cb971-2"></a><span class="st">  </span><span class="kw">fit</span>(basic_form, <span class="dt">data =</span> qscores_train) <span class="op">%&gt;%</span></span>
<span id="cb971-3"><a href="pitfalls.html#cb971-3"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb971-4"><a href="pitfalls.html#cb971-4"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 4
##   term        estimate conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   4.28     4.20     4.36   
## 2 hours        -0.0165  -0.0283  -0.00472</code></pre>
<p>Based on this training data, there is a weakly negative correlation between average hourly workload per week and Q scores.</p>
<p>Now that we have fit a model on the <em>training</em> set, is it time to make predictions on the <em>test</em> set? In general, we would <strong>not</strong> want to predict the test set at this point, although we will do so to illustrate how the code works. In a real scenario, we would use <em>resampling</em> methods (e.g., cross-validation, bootstrapping) to evaluate how well the model is doing. <strong>tidymodels</strong> has a great infrastructure to do this with <strong>rsample</strong>, and we will talk about this soon to demonstrate how we should really evaluate models.</p>
<p>To make predictions, we’ll use the <code>predict()</code> function. We will use the argument <code>new_data = qscores_test</code> to make predictions on the test set.</p>
<div class="sourceCode" id="cb973"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb973-1"><a href="pitfalls.html#cb973-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb973-2"><a href="pitfalls.html#cb973-2"></a><span class="st">  </span></span>
<span id="cb973-3"><a href="pitfalls.html#cb973-3"></a><span class="st">  </span><span class="co"># Train: get fitted model</span></span>
<span id="cb973-4"><a href="pitfalls.html#cb973-4"></a><span class="st">  </span></span>
<span id="cb973-5"><a href="pitfalls.html#cb973-5"></a><span class="st">  </span><span class="kw">fit</span>(basic_form, <span class="dt">data =</span> qscores_train) <span class="op">%&gt;%</span></span>
<span id="cb973-6"><a href="pitfalls.html#cb973-6"></a><span class="st">  </span></span>
<span id="cb973-7"><a href="pitfalls.html#cb973-7"></a><span class="st">  </span><span class="co"># Test: get predictions</span></span>
<span id="cb973-8"><a href="pitfalls.html#cb973-8"></a><span class="st">  </span></span>
<span id="cb973-9"><a href="pitfalls.html#cb973-9"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> qscores_test)</span></code></pre></div>
<pre><code>## # A tibble: 149 x 1
##    .pred
##    &lt;dbl&gt;
##  1  4.16
##  2  4.24
##  3  4.10
##  4  4.17
##  5  4.13
##  6  4.13
##  7  4.20
##  8  4.15
##  9  4.21
## 10  4.18
## # … with 139 more rows</code></pre>
<p>Now we have a tibble of predictions. How can we evaluate it? The <strong>yardstick</strong> package is a tidy interface for computing measures of performance, with individual functions for specific metrics (e.g., <code>accuracy()</code>, <code>rmse()</code>). The <code>rmse()</code> function in the <strong>yardstick</strong> package will compute the RMSE for us, as long as we have the actual values. So we’ll <code>bind_cols()</code> to the test data and use <code>rmse()</code> to evaluate our model. <code>rmse()</code> requires that we give it the <code>truth</code> (here, <code>pres_approval</code>) and the our <code>estimate</code> (here, <code>.pred</code>):</p>
<div class="sourceCode" id="cb975"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb975-1"><a href="pitfalls.html#cb975-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb975-2"><a href="pitfalls.html#cb975-2"></a><span class="st">  </span></span>
<span id="cb975-3"><a href="pitfalls.html#cb975-3"></a><span class="st">  </span><span class="co"># Train: get fitted model</span></span>
<span id="cb975-4"><a href="pitfalls.html#cb975-4"></a><span class="st">  </span></span>
<span id="cb975-5"><a href="pitfalls.html#cb975-5"></a><span class="st">  </span><span class="kw">fit</span>(basic_form, <span class="dt">data =</span> qscores_train) <span class="op">%&gt;%</span></span>
<span id="cb975-6"><a href="pitfalls.html#cb975-6"></a><span class="st">  </span></span>
<span id="cb975-7"><a href="pitfalls.html#cb975-7"></a><span class="st">  </span><span class="co"># Test: get predictions</span></span>
<span id="cb975-8"><a href="pitfalls.html#cb975-8"></a><span class="st">  </span></span>
<span id="cb975-9"><a href="pitfalls.html#cb975-9"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> qscores_test) <span class="op">%&gt;%</span></span>
<span id="cb975-10"><a href="pitfalls.html#cb975-10"></a><span class="st">  </span></span>
<span id="cb975-11"><a href="pitfalls.html#cb975-11"></a><span class="st">  </span><span class="co"># Compare: get metrics</span></span>
<span id="cb975-12"><a href="pitfalls.html#cb975-12"></a><span class="st">  </span></span>
<span id="cb975-13"><a href="pitfalls.html#cb975-13"></a><span class="st">  </span><span class="kw">bind_cols</span>(qscores_test) <span class="op">%&gt;%</span></span>
<span id="cb975-14"><a href="pitfalls.html#cb975-14"></a><span class="st">  </span><span class="kw">rmse</span>(<span class="dt">truth =</span> rating, <span class="dt">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       0.501</code></pre>
</div>
<div id="fitting-many-models-using-map" class="section level3">
<h3>
<span class="header-section-number">10.6.5</span> Fitting many models using <code>map()</code>
</h3>
<p>If you squint, you might see that we could make this process into a function like the one below below:</p>
<div class="sourceCode" id="cb977"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb977-1"><a href="pitfalls.html#cb977-1"></a>fit_lm_split &lt;-<span class="st"> </span><span class="cf">function</span>(formula, train, test) {</span>
<span id="cb977-2"><a href="pitfalls.html#cb977-2"></a>  lm_spec <span class="op">%&gt;%</span></span>
<span id="cb977-3"><a href="pitfalls.html#cb977-3"></a><span class="st">    </span><span class="kw">fit</span>(formula, <span class="dt">data =</span> train) <span class="op">%&gt;%</span></span>
<span id="cb977-4"><a href="pitfalls.html#cb977-4"></a><span class="st">    </span><span class="kw">predict</span>(<span class="dt">new_data =</span> test) <span class="op">%&gt;%</span></span>
<span id="cb977-5"><a href="pitfalls.html#cb977-5"></a><span class="st">    </span><span class="kw">bind_cols</span>(test)</span>
<span id="cb977-6"><a href="pitfalls.html#cb977-6"></a>}</span></code></pre></div>
<p>This function takes a <code>formula</code> object and fits it a linear regression on the training data. It returns the test data with a new column (<code>.pred</code>) that contains predictions from the model that we fit on the training data.</p>
<p>It’s not a great leap to can then create a tibble that has all the predictions for every specification in <code>qscores_formualas</code>, using our old friend <code>map()</code>:</p>
<div class="sourceCode" id="cb978"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb978-1"><a href="pitfalls.html#cb978-1"></a><span class="co">#qscores_test_preds &lt;- qscores_formulas %&gt;%</span></span>
<span id="cb978-2"><a href="pitfalls.html#cb978-2"></a> <span class="co"># mutate(preds = map(formula, ~ fit_lm_split(., qscores_train, qscores_test))) %&gt;%</span></span>
<span id="cb978-3"><a href="pitfalls.html#cb978-3"></a> <span class="co"># unnest(preds)</span></span>
<span id="cb978-4"><a href="pitfalls.html#cb978-4"></a><span class="co">#not working at the moment</span></span></code></pre></div>
<p>Finally, we can use the <code>rmse()</code> function to compare our five specifications.</p>
<div class="sourceCode" id="cb979"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb979-1"><a href="pitfalls.html#cb979-1"></a><span class="co">#qscores_test_preds %&gt;%</span></span>
<span id="cb979-2"><a href="pitfalls.html#cb979-2"></a>  <span class="co">#group_by(group) %&gt;%</span></span>
<span id="cb979-3"><a href="pitfalls.html#cb979-3"></a>  <span class="co">#rmse(truth = rating, estimate = .pred)</span></span></code></pre></div>
<!-- EG: Will change predictions once the code above is working again. "Here we see that adding ideology to the model above and beyond partisanship makes the predictions worse.  The two demographic models perform similarly.  Interestingly, adding back in ideology to the demographic model with interactions performs the best on the test set." -->
<p>But, unfortunately, we shouldn’t be predicting with the test set over and over again like this. It isn’t good practice to predict with the test set more than one time. What is a good predictive modeler to do? We should be saving (holding out) the test set and use it to generate predictions exactly once, at the very end — after we’ve compared different models, selected features, and tuned hyperparameters. How do you do this? You do <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html">cross-validation</a> with the training set, and you leave the testing set for <a href="https://tidymodels.github.io/tune/reference/last_fit.html"><em>the very last fit you do</em></a>.</p>
</div>
<div id="fortitude" class="section level3">
<h3>
<span class="header-section-number">10.6.6</span> Fortitude</h3>
</div>
</div>
<div id="cross-validation" class="section level2">
<h2>
<span class="header-section-number">10.7</span> Cross validation</h2>
<p>In this section we introduce cross validation, one of the most important ideas in machine learning.</p>
<p>In Section <a href="pitfalls.html#loss-function">10.4.2</a>, we described that a common goal of machine learning is to find an algorithm that produces predictors <span class="math inline">\(\hat{Y}\)</span> for an outcome <span class="math inline">\(Y\)</span> that minimizes the MSE:</p>
<p><span class="math display">\[
MSE = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span>
There are two important characteristics of the MSE we should always keep in mind:</p>
<ol style="list-style-type: decimal">
<li><p>We can think our estimate of the MSE is a random variable. For example, the dataset we have may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck.</p></li>
<li><p>If we train an algorithm on the same dataset that we use to compute the MSE, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error.</p></li>
</ol>
<p>Cross validation is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the <em>true error</em>, a theoretical quantity, as the average of many <em>apparent errors</em> obtained by applying the algorithm to new random samples of the data, none of them used to train the algorithm.</p>
<p>However, we only have available one set of outcomes: the ones we actually observed. Cross validation is based on the idea of generating a series of different random samples on which to apply our algorithm. There are several approaches we can use, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error.</p>
<div id="k-fold-cross-validation" class="section level3">
<h3>
<span class="header-section-number">10.7.1</span> K-fold cross validation</h3>
<p>The first approach we describe is <em>K-fold cross validation</em>.</p>
<p>Generally speaking, a machine learning challenge starts with a dataset (blue in the image below). We need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow).</p>
<p>But we don’t get to see these independent datasets.</p>
<p>So to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a <em>training set</em> (blue) and a <em>test set</em> (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.</p>
<p>We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models. The <code>initial_split()</code> function reserves 25% of the data for testing by default.</p>
<p>Let’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting predictors, nothing!</p>
<p>Now this presents a new problem because for most machine learning algorithms we need to select parameters. We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain. This is where cross validation is most useful.</p>
<p>For each set of algorithm parameters being considered, we want an estimate of the MSE and then we will choose the parameters with the smallest MSE. Cross validation provides this estimate.</p>
<p>Let’s start by describing how to construct the first “fold”: we simply pick <span class="math inline">\(M=N/K\)</span> observations at random (we round if <span class="math inline">\(M\)</span> is not a round number) and think of these as a random sample. We call this the <em>validation set</em>:</p>
<p>Now we can fit the model in the training set, then compute the MSE on the validation set. Note that this is just one sample and will therefore return a noisy estimate of the true error. This is why we take <span class="math inline">\(K\)</span> samples, not just one. In K-cross validation, we randomly split the observations into <span class="math inline">\(K\)</span> non-overlapping sets:</p>
<p>Then, for our final estimate, we compute the average MSE across our <span class="math inline">\(K\)</span> samples.</p>
<p>We have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and therefore we need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on:</p>
<p>Once we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters.</p>
<p>Now how do we pick the cross validation <span class="math inline">\(K\)</span>? Large values of <span class="math inline">\(K\)</span> are preferable because the training data better imitates the original dataset. However, larger values of <span class="math inline">\(K\)</span> will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of <span class="math inline">\(K=5\)</span> and <span class="math inline">\(K=10\)</span> are popular.</p>
</div>
<div id="implementing-cross-validation-using-rsample" class="section level3">
<h3>
<span class="header-section-number">10.7.2</span> Implementing cross-validation using <strong>rsample</strong>
</h3>
<p>Now let’s add cross-validation to our <strong>tidymodels</strong> workflow! To do this, we’ll use a function called <code>vfold_cv()</code> in the <strong>rsample</strong> package. The argument <code>v</code> sets the number of folds, which is 10 by default. We’ll do 5-fold cross-validation in this example.</p>
<div class="sourceCode" id="cb980"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb980-1"><a href="pitfalls.html#cb980-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb980-2"><a href="pitfalls.html#cb980-2"></a></span>
<span id="cb980-3"><a href="pitfalls.html#cb980-3"></a>qscores_folds &lt;-<span class="st"> </span>qscores_train <span class="op">%&gt;%</span></span>
<span id="cb980-4"><a href="pitfalls.html#cb980-4"></a><span class="st">  </span><span class="kw">vfold_cv</span>(<span class="dt">v =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>How can we work with the <code>qscores_folds</code> object? <strong>tidymodels</strong> makes it easy by using the <code>fit_resamples()</code> function in the <strong>tune</strong> package. (<strong>Note</strong>: make sure you have at least version 0.1.0 of the <strong>tune</strong> package installed, as the following code uses new syntax.) The <code>fit_resamples()</code> function takes as its first argument a model specification (such as <code>lm_spec</code>), then a formula as its second argument, called <code>preprocessor</code>.<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> Or a recipe, if you delve more deeply into <strong>tidymodels</strong>.</span> Finally, the <code>resamples</code> argument is where you input the cross-validation dataset.</p>
<div class="sourceCode" id="cb981"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb981-1"><a href="pitfalls.html#cb981-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb981-2"><a href="pitfalls.html#cb981-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">preprocessor =</span> basic_form,</span>
<span id="cb981-3"><a href="pitfalls.html#cb981-3"></a>                <span class="dt">resamples =</span> qscores_folds)</span></code></pre></div>
<pre><code>## # Resampling results
## # 5-fold cross-validation 
## # A tibble: 5 x 4
##   splits            id    .metrics         .notes          
##   &lt;list&gt;            &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          
## 1 &lt;split [479/120]&gt; Fold1 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
## 2 &lt;split [479/120]&gt; Fold2 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
## 3 &lt;split [479/120]&gt; Fold3 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
## 4 &lt;split [479/120]&gt; Fold4 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
## 5 &lt;split [480/119]&gt; Fold5 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;</code></pre>
<p>To inspect the average metrics across all the folds, we can use the <code>collect_metrics()</code> function:</p>
<div class="sourceCode" id="cb983"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb983-1"><a href="pitfalls.html#cb983-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb983-2"><a href="pitfalls.html#cb983-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">preprocessor =</span> basic_form,</span>
<span id="cb983-3"><a href="pitfalls.html#cb983-3"></a>                <span class="dt">resamples =</span> qscores_folds) <span class="op">%&gt;%</span></span>
<span id="cb983-4"><a href="pitfalls.html#cb983-4"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric .estimator   mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   0.519      5 0.0205 
## 2 rsq     standard   0.0150     5 0.00662</code></pre>
<p>It’s not that hard to extend this to all of our formulas using <code>map()</code>.</p>
<div class="sourceCode" id="cb985"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb985-1"><a href="pitfalls.html#cb985-1"></a><span class="co">#folds_metrics &lt;- qscores_formulas %&gt;%</span></span>
<span id="cb985-2"><a href="pitfalls.html#cb985-2"></a> <span class="co"># mutate(metrics = map(formula, ~ fit_resamples(lm_spec,</span></span>
<span id="cb985-3"><a href="pitfalls.html#cb985-3"></a>                                               <span class="co"># preprocessor = .,</span></span>
<span id="cb985-4"><a href="pitfalls.html#cb985-4"></a>                                              <span class="co">#  resamples = qscores_folds) %&gt;%</span></span>
<span id="cb985-5"><a href="pitfalls.html#cb985-5"></a>                        <span class="co"># collect_metrics()))</span></span></code></pre></div>
<p>(Note that <code>fit_resamples()</code> currently gives the warning message “prediction from a rank-deficient fit may be misleading” whenever a factor or character variable is used as a predictor; this does not necessarily mean that the fit was actually rank-deficient.)</p>
<!-- AR: this annoying behavior appears to be because they want you to use
recipes and then step_dummy() the factor variables -->
<p>Let’s present the results stored in our <code>folds_metrics</code> object:</p>
<div class="sourceCode" id="cb986"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb986-1"><a href="pitfalls.html#cb986-1"></a><span class="co">#folds_metrics %&gt;%</span></span>
<span id="cb986-2"><a href="pitfalls.html#cb986-2"></a>  <span class="co">#mutate(mean_rmse = map_dbl(metrics, ~ filter(., .metric == "rmse") %&gt;% pull(mean)),</span></span>
<span id="cb986-3"><a href="pitfalls.html#cb986-3"></a>        <span class="co"># se_rmse = map_dbl(metrics, ~ filter(., .metric == "rmse") %&gt;% pull(std_err))) %&gt;%</span></span>
<span id="cb986-4"><a href="pitfalls.html#cb986-4"></a>  <span class="co">#select(group, mean_rmse, se_rmse)</span></span></code></pre></div>
</div>
<div id="bootstrap" class="section level3">
<h3>
<span class="header-section-number">10.7.3</span> Bootstrap</h3>
<p>One way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick <span class="math inline">\(K\)</span> sets of some size at random.</p>
<p>One popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice) – our old friend the <em>bootstrap</em>.</p>
<p>In <strong>rsample</strong>, we can do that using the <code>boostraps()</code> function. The <code>times</code> argument states how many bootstrap samples you want to take:</p>
<div class="sourceCode" id="cb987"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb987-1"><a href="pitfalls.html#cb987-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb987-2"><a href="pitfalls.html#cb987-2"></a></span>
<span id="cb987-3"><a href="pitfalls.html#cb987-3"></a>qscores_boots &lt;-<span class="st"> </span>qscores_train <span class="op">%&gt;%</span></span>
<span id="cb987-4"><a href="pitfalls.html#cb987-4"></a><span class="st">  </span><span class="kw">bootstraps</span>(<span class="dt">times =</span> <span class="dv">25</span>)</span></code></pre></div>
<p>Now we can use <code>fit_resample()</code> just like we did with <code>qscores_folds</code>. Here’s how we do it with one formula:</p>
<div class="sourceCode" id="cb988"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb988-1"><a href="pitfalls.html#cb988-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb988-2"><a href="pitfalls.html#cb988-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">preprocessor =</span> basic_form,</span>
<span id="cb988-3"><a href="pitfalls.html#cb988-3"></a>                <span class="dt">resamples =</span> qscores_boots) <span class="op">%&gt;%</span></span>
<span id="cb988-4"><a href="pitfalls.html#cb988-4"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric .estimator   mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   0.528     25 0.00715
## 2 rsq     standard   0.0144    25 0.00202</code></pre>
<p>And we can also adapt our code to run this for every formula in <code>qscores_formulas</code>:</p>
<div class="sourceCode" id="cb990"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb990-1"><a href="pitfalls.html#cb990-1"></a><span class="co">#boots_metrics &lt;- qscores_formulas %&gt;%</span></span>
<span id="cb990-2"><a href="pitfalls.html#cb990-2"></a>  <span class="co">#mutate(metrics = map(formula, ~ fit_resamples(lm_spec,</span></span>
<span id="cb990-3"><a href="pitfalls.html#cb990-3"></a>                                                <span class="co">#preprocessor = .,</span></span>
<span id="cb990-4"><a href="pitfalls.html#cb990-4"></a>                                                <span class="co">#resamples = qscores_boots) %&gt;%</span></span>
<span id="cb990-5"><a href="pitfalls.html#cb990-5"></a>                         <span class="co">#collect_metrics()))</span></span>
<span id="cb990-6"><a href="pitfalls.html#cb990-6"></a></span>
<span id="cb990-7"><a href="pitfalls.html#cb990-7"></a><span class="co">#boots_metrics %&gt;%</span></span>
<span id="cb990-8"><a href="pitfalls.html#cb990-8"></a>  <span class="co">#mutate(mean_rmse = map_dbl(metrics, ~ filter(., .metric == "rmse") %&gt;% pull(mean)),</span></span>
<span id="cb990-9"><a href="pitfalls.html#cb990-9"></a>         <span class="co">#se_rmse = map_dbl(metrics, ~ filter(., .metric == "rmse") %&gt;% pull(std_err))) %&gt;%</span></span>
<span id="cb990-10"><a href="pitfalls.html#cb990-10"></a>  <span class="co">#select(group, mean_rmse, se_rmse)</span></span></code></pre></div>
<!-- AR: this takes a while, so I saved the result, but be sure to uncomment
this if you need to run this code again -->
<p>Cross-validation and the bootstrap lead to similar estimates of the RMSE, but note that the standard error of the RMSE goes down using the bootstrap method, since more samples were taken. However, the trade-off is that the more samples you take, the more computing time you’ll use.</p>
</div>
<div id="temperance" class="section level3">
<h3>
<span class="header-section-number">10.7.4</span> Temperance</h3>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>
<span class="header-section-number">10.8</span> Conclusion</h2>
<p>In this chapter, as in the Primer as a whole, we have been making models to better understand the world. But we need to act, as well as to understand.</p>
<p>We’ve given you tools to construct models and to evaluate features of these models, such as the causal effects of variables of interest and predicted values of the outcome. We’ve also shown you how to construct measures of uncertainty around these measures.</p>
<p>Once you have this information, what should you do? Remember that all models are meant to simplify some real-world phenomenon. Ultimately, you have to use the informations from the models to make a decision. For example, let’s say that you construct a model to evaluate the benefits of a hypertension drug. Should you prescribe the drug? You can take the estimates of the likely drug from the model to help with that decision. The model won’t make the decision for you, however – you’ll also want to incorporate other information, such as the costs of the drug.</p>
<p>From the other perspective, you should always make models while keeping in mind what unknown features of the real world you need to estimate. Maybe you are trying to estimate a single value, such as a mean (how many adults are in the United States right now?) Maybe you want to predict the future value of some variable (how many adults will be in the United States ten years from now?). Or maybe you want to estimate the relationship between two variables (how do changes in immigration policy affect the U.S. adult population?).</p>
<p>We’ve given you tools to help answer all these questions. Now it’s up to you to use those tools to help make decisions in real life!</p>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="n-parameters.html"><button class="btn btn-default">Previous</button></a>
<a href="continuous-response.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-08-28
</p>
</div>
</div>



</body>
</html>
