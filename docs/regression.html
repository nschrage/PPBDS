<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 10 Continuous Response I | Preceptor’s Primer for Bayesian Data Science" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="davidkane9/PPBDS" />

<meta name="author" content="David Kane" />

<meta name="date" content="2020-07-08" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 10 Continuous Response I | Preceptor’s Primer for Bayesian Data Science">

<title>Chapter 10 Continuous Response I | Preceptor’s Primer for Bayesian Data Science</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Preceptor's Primer for Bayesian Data Science<p><p class="author">David Kane</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Cover</a>
<a href="preamble.html">Preamble</a>
<a href="visualization.html"><span class="toc-section-number">1</span> Visualization</a>
<a href="wrangling.html"><span class="toc-section-number">2</span> Tidyverse</a>
<a href="rubin-causal-model.html"><span class="toc-section-number">3</span> Rubin Causal Model</a>
<a href="functions.html"><span class="toc-section-number">4</span> Functions</a>
<a href="probability.html"><span class="toc-section-number">5</span> Probability</a>
<a href="sampling.html"><span class="toc-section-number">6</span> Sampling</a>
<a href="one-parameter.html"><span class="toc-section-number">7</span> One Parameter</a>
<a href="two-parameters.html"><span class="toc-section-number">8</span> Two Parameters</a>
<a href="n-parameters.html"><span class="toc-section-number">9</span> N Parameters</a>
<a id="active-page" href="regression.html"><span class="toc-section-number">10</span> Continuous Response I</a><ul class="toc-sections">
<li class="toc"><a href="#model1"> Teaching evaluations: one numerical explanatory variable</a></li>
<li class="toc"><a href="#uncertainty-in-simple-linear-regressions"> Uncertainty in simple linear regressions</a></li>
<li class="toc"><a href="#case-study-2018-gubernatorial-forecasts"> Case study: 2018 gubernatorial forecasts</a></li>
<li class="toc"><a href="#leastsquares"> Best-fitting line</a></li>
<li class="toc"><a href="#advanced-bayesian-regression"> Advanced: Bayesian Regression</a></li>
<li class="toc"><a href="#introduction-to-rstanarm"> Introduction to rstanarm</a></li>
<li class="toc"><a href="#bayesian-regression-with-a-continuous-variable"> Bayesian Regression with a Continuous Variable</a></li>
<li class="toc"><a href="#bayesian-regression-with-categorical-variable"> Bayesian Regression with Categorical Variable</a></li>
<li class="toc"><a href="#conclusion"> Conclusion</a></li>
</ul>
<a href="multiple-regression.html"><span class="toc-section-number">11</span> Continuous Response II</a>
<a href="classification.html"><span class="toc-section-number">12</span> Discrete Response</a>
<a href="appendices.html">Appendices</a>
<a href="productivity.html">Productivity</a>
<a href="shiny.html">Shiny</a>
<a href="maps.html">Maps</a>
<a href="animation.html">Animation</a>
<a href="rubin-causal-model.html">References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="regression" class="section level1">
<h1>
<span class="header-section-number">Chapter 10</span> Continuous Response I</h1>
<!-- 1. Replace Texas data with qscores: rating ~ hours. Keep all that material and verbiage. Add key items from style.Rmd, e.g., parameter uncertainty, unmodelled variation and so on. -->
<!-- 2. Use a different model, like loess, to solve the exact same problem. Go through the same overview, but more quickly. Not everything will work. For example, there are no parameter estimates for loess, or at least none that are easily visible. -->
<!-- 3. How do we decide? lm() or loess() or something? Incorporate (i.e., copy and paste) chapter 14 material. Don't do anything with tidymodel syntax. All that falls to chapter 11. But you are explaing every concept. -->
<!-- 4. We need a Rubin Causal Model section.  Set up a Zoom with Cass to discuss. She is a good source for information about how to make nice looking tables and about how to think about the issue. We might use qscores and just pretend/assume random assignment or we might use a different data set. Key is to set up a Rubin Table in which the potential outcomes are clear. For example, what would my rating be in a assigned 5 hours of work? What about 10 hours? 15 hours? Each of these is a differnt treatment and, therefore, generates a different potential outcome. -->
<!-- 5. Show the technology for creating multiple models. Instead of a single linear model connecting rating to hours, maybe the relationship is different in different divisions or in different departments or in different class enrollment buckets. Show how to explore this. The governor example in the book is not bad. But nor is it good. -->
<!-- 6. We explicitly avoid talking about Bayesian models here because we are only estimating a one or two parameters in each of the models above. So, there is no occasion for pooling, or any of the other Bayesian magic. But you can see how estimating a 100 rating ~ hours models, one for each department, lends itself to a Bayesian approach. On to chapter 11. -->
<!-- In loess section, grab a copy of this xkcd and use it: https://xkcd.com/2048/ -->
<!-- Packages: tidyverse, broom -->
<!-- Commands:  -->
<!-- How do we solve the problems which were identified in chapter 9? -->
<!-- no bootstrap;  introduce tidymodels here for first time; -->
<!-- holdout sample, cross validation, machine learning, test  -->
<!-- DK: Maybe build this from tidymodels, while also mentioning the traditional way of just using lm()? The problem with raw lm() is that it does not work natively in a pipe since the first argument is a formula instead of the data. (Or maybe a lm(y ~ x, data = .) hack is OK?) -->
<!-- DK: Change the smoking example to political campaigns. If you work on the NYT, all you care about is forecasting election results conditional on campaign spending. If you are a (rich!) candidate, you care about the causal effect of spending on votes. Same model might be estimated by both! But the latter needs to be much more careful in deciding whether or not the results are real. -->
<!-- Albert points out a difficulty in combining the RCM with regression. You can't easily put in a distribution for the unknown potential outcome, even if you have a good regression model. You can't just add to the observed outcome because . . . actually I am confused about this! -->
<p>The fundamental goal of data modeling is to make explicit the relationship between:</p>
<ul>
<li>an <em>outcome variable</em> <span class="math inline">\(y\)</span>, also called a <em>dependent variable</em> or response variable, and<br>
</li>
<li>an <em>explanatory/predictor variable</em> <span class="math inline">\(x\)</span>, also called an <em>independent variable</em> or covariate.</li>
</ul>
<p>Another way to state this is using mathematical terminology: we will model the outcome variable <span class="math inline">\(y\)</span> “as a function” of the explanatory/predictor variable <span class="math inline">\(x\)</span>. When we say “function” here, we aren’t referring to functions in R like the <code>ggplot()</code> function, but rather to a mathematical function. But, why do we have two different labels, explanatory and predictor, for the variable <span class="math inline">\(x\)</span>? That’s because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Modeling for explanation</strong>: When you want to explicitly describe and quantify the relationship between the outcome variable <span class="math inline">\(y\)</span> and an explanatory variable <span class="math inline">\(x\)</span>, determine the importance of any relationships, have measures summarizing these relationships, and possibly identify any <em>causal</em> relationships between the variables. (What’s a causal relationship? Remember the <a href="rubin-causal-model.html#rubin-causal-model">Rubin Causal Model</a>! The <em>causal effect</em> of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> is the difference in <em>potential outcomes</em> of <span class="math inline">\(y\)</span> given different values of <span class="math inline">\(x\)</span>.)</li>
<li>
<strong>Modeling for prediction</strong>: When you want to predict an outcome variable <span class="math inline">\(y\)</span> based on the information contained in a set of predictor variables <span class="math inline">\(x\)</span>. Unlike modeling for explanation, however, you don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about <span class="math inline">\(y\)</span> using the information in <span class="math inline">\(x\)</span>.</li>
</ol>
<p>For example, say you are interested in an outcome variable <span class="math inline">\(y\)</span> of whether patients develop lung cancer and information <span class="math inline">\(x\)</span> on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as increasing family income. In that case, you would want to know the causal effect of income on the incidence of lung cancer.</p>
<p>If we are modeling for prediction, however, we wouldn’t care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer.</p>
<!-- DK: Find a way to use this reference: [*An Introduction to Statistical Learning with Applications in R (ISLR)*](http://www-bcf.usc.edu/~gareth/ISL/)  -->
<p>Linear regression involves a <em>numerical</em> outcome variable <span class="math inline">\(y\)</span> and explanatory variables <span class="math inline">\(x\)</span> that are either <em>numerical</em> or <em>categorical</em>. Furthermore, the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> is assumed to be linear, or in other words, a line. However, we’ll see that what constitutes a “line” will vary depending on the nature of your explanatory variables <span class="math inline">\(x\)</span>.</p>
<!-- DK: Could give a better plan overview, including discussion of chapters 11 and 12. Indeed, perhaps also looking backward to sampling and uncertainty. Need to rewrite this if we re-organize the book. Indeed, the introductions (and conclusions) to each chapter should be similar, providing a framework in which that chapter fits. -->
<p>In Chapter <a href="regression.html#regression">10</a> on basic regression, we’ll only consider models with a single explanatory variable <span class="math inline">\(x\)</span>. In Section <a href="regression.html#model1">10.1</a>, the explanatory variable will be numerical. This scenario is known as <em>simple linear regression</em>. In Section <a href="#model2"><strong>??</strong></a>, the explanatory variable will be categorical.</p>
<p>In Chapter <a href="multiple-regression.html#multiple-regression">11</a> on multiple regression, we’ll extend the ideas behind basic regression and consider models with two explanatory variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. In Section <a href="multiple-regression.html#model4">11.1</a>, we’ll have two numerical explanatory variables. In Section <a href="multiple-regression.html#model3">11.2</a>, we’ll have one numerical and one categorical explanatory variable. In particular, we’ll consider two such models: <em>interaction</em> and <em>parallel slopes</em> models.</p>
<p>Let’s now begin with basic regression,  which refers to linear regression models with a single explanatory variable <span class="math inline">\(x\)</span>. We’ll also discuss important statistical concepts like the <em>correlation coefficient</em>, that “correlation isn’t necessarily causation,” and what it means for a line to be “best-fitting.”</p>
<p>Let’s now load all the packages needed for this chapter (this assumes you’ve already installed them). The main packages are ones we have used before. The Advanced Section of the chapter makes use of</p>
<ol style="list-style-type: decimal">
<li>The <strong>rstanarm</strong> package, which provides an interface to the statistical inference engine, Stan, for Bayesian Regression Modeling.</li>
<li>The <strong>tidybayes</strong> package, which aids in formating Bayesian modeling outputs in a tidy manner and provides ggplot geoms for plotting.</li>
</ol>
<div class="sourceCode" id="cb803"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb803-1"><a href="regression.html#cb803-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb803-2"><a href="regression.html#cb803-2"></a><span class="kw">library</span>(PPBDS.data)</span>
<span id="cb803-3"><a href="regression.html#cb803-3"></a><span class="kw">library</span>(broom)</span>
<span id="cb803-4"><a href="regression.html#cb803-4"></a><span class="kw">library</span>(skimr)</span>
<span id="cb803-5"><a href="regression.html#cb803-5"></a><span class="kw">library</span>(gapminder)</span>
<span id="cb803-6"><a href="regression.html#cb803-6"></a><span class="kw">library</span>(rstanarm)</span>
<span id="cb803-7"><a href="regression.html#cb803-7"></a><span class="kw">library</span>(tidybayes)</span></code></pre></div>
<div id="model1" class="section level2">
<h2>
<span class="header-section-number">10.1</span> Teaching evaluations: one numerical explanatory variable</h2>
<!-- DK: I think this is good. I like walking through EDA. Should do that each chapter, maybe showing more tricks each time. But in what order? skim() here then calculate your own simply in 12 and then calculate using new dplyr 1.0.0 tricks like across() in 13. Also, stop replying on moderndive. Just include this data in our package or on line. Indeed, do we need our own package? Or just our own dedicated set of Google sheets? We need our own package, since we will be using a bunch of datasets, some created/cleaned by us. -->
<!-- EG: I really like this section- I think that the in-depth explanations of not only how to find correlation coefficients but also interpret them accurately and effectively is great. -->
<!-- DK: Next version, can't use these examples. Need political ones. Let's use an updated version of US congress campaigns. This connects to the new introduction. Can use latest data. Can include variables like last election results, allowing us to follow Gelman's version. Can also include spending data, so that we can discuss a causal model. Perhaps in this chapter, we do one model with each, the second going faster. Then, next chapter, put both variables in the regression. This example would also be nice because we could use list columns for the secret trick, estimating models by year and by country region. -->
<!-- EG: I'll change this to qscores, along with an adjusted EDA for that dataset and more explanation of how correlation != causation. I'll also provide more investigation into the many ways confounding variables could impact why students provide certain qscores rather than simply hours of work, along with more language of comparison. -->
<!-- DK: I have skimmed the next section, since I plan on cutting it all. -->
<p>Why do some professors and instructors at universities and colleges receive high teaching evaluations scores from students while others receive lower ones? Are there differences in teaching evaluations between instructors of different demographic groups? Could there be an impact due to student biases? These are all questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which instructors and professors get promoted.</p>
<p>Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer the following research question: what factors explain differences in instructor teaching evaluation scores? To this end, they collected instructor and course information on 463 courses. A full description of the study can be found at <a href="https://www.openintro.org/data/index.php?data=evals">openintro.org</a>.</p>
<p>In this section, we’ll keep things simple for now and try to explain differences in instructor teaching scores as a function of one numerical variable: the instructor’s “beauty” score (we’ll describe how this score was determined shortly). Could it be that instructors with higher “beauty” scores also have higher teaching evaluations? Could it be instead that instructors with higher “beauty” scores tend to have lower teaching evaluations? Or could it be that there is no relationship between “beauty” score and teaching evaluations? We’ll answer these questions by modeling the relationship between teaching scores and “beauty” scores using <em>simple linear regression</em>  where we have:</p>
<ol style="list-style-type: decimal">
<li>A numerical outcome variable <span class="math inline">\(y\)</span> (the instructor’s teaching score) and</li>
<li>A single numerical explanatory variable <span class="math inline">\(x\)</span> (the instructor’s “beauty” score).</li>
</ol>
<div id="model1EDA" class="section level3">
<h3>
<span class="header-section-number">10.1.1</span> Exploratory data analysis</h3>
<p>The data on the 463 courses at UT Austin can be found in the <code>evals</code> data frame included in the <strong>moderndive</strong> package. However, to keep things simple, let’s <code>select()</code> only the subset of the variables we’ll consider in this chapter, and save this data in a new data frame called <code>evals_ch11</code>:</p>
<div class="sourceCode" id="cb804"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb804-1"><a href="regression.html#cb804-1"></a><span class="kw">library</span>(moderndive)</span>
<span id="cb804-2"><a href="regression.html#cb804-2"></a></span>
<span id="cb804-3"><a href="regression.html#cb804-3"></a>evals_ch11 &lt;-<span class="st"> </span>evals <span class="op">%&gt;%</span></span>
<span id="cb804-4"><a href="regression.html#cb804-4"></a><span class="st">  </span><span class="kw">select</span>(ID, score, bty_avg, age)</span></code></pre></div>
<p>A crucial step before doing any kind of analysis or modeling is performing an <em>exploratory data analysis</em>,  or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA:</p>
<!-- DK: Good stuff. We should keep this and follow it, each chapter. -->
<ol style="list-style-type: decimal">
<li>Most crucially, looking at the raw data values.</li>
<li>Computing summary statistics, such as means, medians, and interquartile ranges.</li>
<li>Creating data visualizations.</li>
</ol>
<p>Let’s perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road.</p>
<p>You can do this by using RStudio’s spreadsheet viewer or by using the <code>glimpse()</code> function as introduced in Subsection <a href="#exploredataframes"><strong>??</strong></a> on exploring data frames:</p>
<!-- DK: Add summary() -->
<div class="sourceCode" id="cb805"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb805-1"><a href="regression.html#cb805-1"></a><span class="kw">glimpse</span>(evals_ch11) <span class="op">%&gt;%</span></span>
<span id="cb805-2"><a href="regression.html#cb805-2"></a><span class="st">  </span><span class="kw">summary</span>()</span></code></pre></div>
<pre><code>## Rows: 463
## Columns: 4
## $ ID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…
## $ score   &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4…
## $ bty_avg &lt;dbl&gt; 5.000, 5.000, 5.000, 5.000, 3.000, 3.000, 3.000, 3.333, 3.333…
## $ age     &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 4…</code></pre>
<pre><code>##        ID            score          bty_avg           age       
##  Min.   :  1.0   Min.   :2.300   Min.   :1.667   Min.   :29.00  
##  1st Qu.:116.5   1st Qu.:3.800   1st Qu.:3.167   1st Qu.:42.00  
##  Median :232.0   Median :4.300   Median :4.333   Median :48.00  
##  Mean   :232.0   Mean   :4.175   Mean   :4.418   Mean   :48.37  
##  3rd Qu.:347.5   3rd Qu.:4.600   3rd Qu.:5.500   3rd Qu.:57.00  
##  Max.   :463.0   Max.   :5.000   Max.   :8.167   Max.   :73.00</code></pre>
<!-- EG: I can change some of this language later to reflect the vocab of glimpse with summary as opposed to just glimpse, if we like this option. -->
<p>Observe that <code>Observations: 463</code> indicates that there are 463 rows/observations in <code>evals_ch11</code>, where each row corresponds to one observed course at UT Austin. It is important to note that the <em>observational unit</em>  is an individual course and not an individual instructor. Recall from Subsection <a href="#exploredataframes"><strong>??</strong></a> that the observational unit is the “type of thing” that is being measured by our variables. Since instructors teach more than one course in an academic year, the same instructor will appear more than once in the data. Hence there are fewer than 463 unique instructors being represented in <code>evals_ch11</code>.</p>
<p>A full description of all the variables included in <code>evals</code> can be found at <a href="https://www.openintro.org/data/index.php?data=evals">openintro.org</a> or by reading the associated help file (run <code>?evals</code> in the console). However, let’s fully describe only the 4 variables we selected in <code>evals_ch11</code>:</p>
<ol style="list-style-type: decimal">
<li>
<code>ID</code>: An identification variable used to distinguish between the 1 through 463 courses in the dataset.</li>
<li>
<code>score</code>: A numerical variable of the course instructor’s average teaching score, where the average is computed from the evaluation scores from all students in that course. Teaching scores of 1 are lowest and 5 are highest. This is the outcome variable <span class="math inline">\(y\)</span> of interest.</li>
<li>
<code>bty_avg</code>: A numerical variable of the course instructor’s average “beauty” score, where the average is computed from a separate panel of six students. “Beauty” scores of 1 are lowest and 10 are highest. This is the explanatory variable <span class="math inline">\(x\)</span> of interest.</li>
<li>
<code>age</code>: A numerical variable of the course instructor’s age.</li>
</ol>
<p>An alternative way to look at the raw data values is by choosing a random sample of the rows in <code>evals_ch11</code> by piping it into the <code>sample_n()</code>  function from the <strong>dplyr</strong> package. Here we set the <code>size</code> argument to be <code>5</code>, indicating that we want a random sample of 5 rows. We display the results below. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.</p>
<div class="sourceCode" id="cb808"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb808-1"><a href="regression.html#cb808-1"></a>evals_ch11 <span class="op">%&gt;%</span></span>
<span id="cb808-2"><a href="regression.html#cb808-2"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dt">size =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 4
##      ID score bty_avg   age
##   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;
## 1    44   4.8    4.67    33
## 2   146   4.3    3.83    58
## 3    92   3.6    2.5     56
## 4   187   4.3    4.33    47
## 5   360   3.9    5.83    52</code></pre>
<p>Now that we’ve looked at the raw values in our <code>evals_ch11</code> data frame and got a preliminary sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s start by computing the mean and median of our numerical outcome variable <code>score</code> and our numerical explanatory variable “beauty” score denoted as <code>bty_avg</code>. We’ll do this by using the <code>summarize()</code> function from <code>dplyr</code> along with the <code>mean()</code> and <code>median()</code> summary functions we saw in Section <a href="wrangling.html#summarize">2.1</a>.</p>
<div class="sourceCode" id="cb810"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb810-1"><a href="regression.html#cb810-1"></a>evals_ch11 <span class="op">%&gt;%</span></span>
<span id="cb810-2"><a href="regression.html#cb810-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mean_bty_avg =</span> <span class="kw">mean</span>(bty_avg),</span>
<span id="cb810-3"><a href="regression.html#cb810-3"></a>            <span class="dt">mean_score =</span> <span class="kw">mean</span>(score),</span>
<span id="cb810-4"><a href="regression.html#cb810-4"></a>            <span class="dt">median_bty_avg =</span> <span class="kw">median</span>(bty_avg),</span>
<span id="cb810-5"><a href="regression.html#cb810-5"></a>            <span class="dt">median_score =</span> <span class="kw">median</span>(score))</span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##   mean_bty_avg mean_score median_bty_avg median_score
##          &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;
## 1         4.42       4.17           4.33          4.3</code></pre>
<!-- DK: This is nice. Having motivated the use of skim() once, we can just go straight to using it in other chapters. And/or show other tricks each chapter, like across(). -->
<p>However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles?</p>
<p>Typing out all these summary statistic functions in <code>summarize()</code> would be long and tedious. Instead, let’s use the convenient <code>skim()</code> function from the <strong>skimr</strong> package. This function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our <code>evals_ch11</code> data frame, <code>select()</code> only the outcome and explanatory variables teaching <code>score</code> and <code>bty_avg</code>, and pipe them into the <code>skim()</code> function:</p>
<div class="sourceCode" id="cb812"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb812-1"><a href="regression.html#cb812-1"></a>evals_ch11 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb812-2"><a href="regression.html#cb812-2"></a><span class="st">  </span><span class="kw">select</span>(score, bty_avg) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb812-3"><a href="regression.html#cb812-3"></a><span class="st">  </span><span class="kw">skim</span>()</span></code></pre></div>
<table style="width: auto;" class="table table-condensed">
<caption>
<span id="tab:unnamed-chunk-521">TABLE 10.1: </span>Data summary
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Name
</td>
<td style="text-align:left;">
Piped data
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of rows
</td>
<td style="text-align:left;">
463
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of columns
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
_______________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Column type frequency:
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
________________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Group variables
</td>
<td style="text-align:left;">
None
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead><tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
p0
</th>
<th style="text-align:right;">
p25
</th>
<th style="text-align:right;">
p50
</th>
<th style="text-align:right;">
p75
</th>
<th style="text-align:right;">
p100
</th>
<th style="text-align:left;">
hist
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
score
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4.17
</td>
<td style="text-align:right;">
0.54
</td>
<td style="text-align:right;">
2.30
</td>
<td style="text-align:right;">
3.80
</td>
<td style="text-align:right;">
4.30
</td>
<td style="text-align:right;">
4.6
</td>
<td style="text-align:right;">
5.00
</td>
<td style="text-align:left;">
▁▁▅▇▇
</td>
</tr>
<tr>
<td style="text-align:left;">
bty_avg
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4.42
</td>
<td style="text-align:right;">
1.53
</td>
<td style="text-align:right;">
1.67
</td>
<td style="text-align:right;">
3.17
</td>
<td style="text-align:right;">
4.33
</td>
<td style="text-align:right;">
5.5
</td>
<td style="text-align:right;">
8.17
</td>
<td style="text-align:left;">
▃▇▇▃▂
</td>
</tr>
</tbody>
</table>
<p>For the numerical variables teaching <code>score</code> and <code>bty_avg</code> it returns:</p>
<ul>
<li>
<code>n_missing</code>: the number of missing values</li>
<li>
<code>complete_rate</code>: the percentage of non-missing or complete values</li>
<li>
<code>mean</code>: the average</li>
<li>
<code>sd</code>: the standard deviation</li>
<li>
<code>p0</code>: the 0th percentile: the value at which 0% of observations are smaller than it (the <em>minimum</em> value)</li>
<li>
<code>p25</code>: the 25th percentile: the value at which 25% of observations are smaller than it (the <em>1st quartile</em>)</li>
<li>
<code>p50</code>: the 50th percentile: the value at which 50% of observations are smaller than it (the <em>2nd</em> quartile and more commonly called the <em>median</em>)</li>
<li>
<code>p75</code>: the 75th percentile: the value at which 75% of observations are smaller than it (the <em>3rd quartile</em>)</li>
<li>
<code>p100</code>: the 100th percentile: the value at which 100% of observations are smaller than it (the <em>maximum</em> value)</li>
</ul>
<p>Looking at this output, we can see how the values of both variables are distributed. For example, the mean teaching score was 4.17 out of 5, whereas the mean “beauty” score was 4.42 out of 10. Furthermore, the middle 50% of teaching scores was between 3.80 and 4.6 (the first and third quartiles), whereas the middle 50% of “beauty” scores falls within 3.17 to 5.5 out of 10.</p>
<!-- DK: Keep this. -->
<p>The <code>skim()</code> function only returns what are known as <em>univariate</em>  summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist <em>bivariate</em>  summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the  <em>correlation coefficient</em>. Generally speaking, <em>coefficients</em> are quantitative expressions of a specific phenomenon. A <em>correlation coefficient</em> is a quantitative expression of the <em>strength of the linear relationship between two numerical variables</em>. Its value ranges between -1 and 1 where:</p>
<ul>
<li>-1 indicates a perfect <em>negative relationship</em>: As one variable increases, the value of the other variable tends to go down, following a straight line.</li>
<li>0 indicates no relationship: The values of both variables go up/down independently of each other.</li>
<li>+1 indicates a perfect <em>positive relationship</em>: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.</li>
</ul>
<p>The following figure gives examples of 9 different correlation coefficient values for hypothetical numerical variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, but it is not as strong as the negative linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> when the correlation coefficient is -0.9 or -1.</p>
<div class="figure">
<span id="fig:unnamed-chunk-522"></span>
<p class="caption marginnote shownote">
FIGURE 10.1: Nine different correlation coefficients.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-522-1.png" alt="Nine different correlation coefficients." width="672">
</div>
<p>The correlation coefficient can be computed using the <code>cor()</code> summary function within a <code>summarize()</code>:</p>
<div class="sourceCode" id="cb813"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb813-1"><a href="regression.html#cb813-1"></a>evals_ch11 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb813-2"><a href="regression.html#cb813-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">correlation =</span> <span class="kw">cor</span>(score, bty_avg))</span></code></pre></div>
<p>In our case, the correlation coefficient of 0.187 indicates that the relationship between teaching evaluation score and “beauty” average is “weakly positive.” There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren’t close to the extreme values of -1, 0, and 1.</p>
<p>Let’s now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the <code>score</code> and <code>bty_avg</code> variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let’s do this using <code>geom_point()</code> and display the result. Furthermore, let’s highlight the six points in the top right of the visualization in a box.</p>
<div class="sourceCode" id="cb814"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb814-1"><a href="regression.html#cb814-1"></a>evals_ch11 <span class="op">%&gt;%</span></span>
<span id="cb814-2"><a href="regression.html#cb814-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span></span>
<span id="cb814-3"><a href="regression.html#cb814-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb814-4"><a href="regression.html#cb814-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Beauty Score"</span>, </span>
<span id="cb814-5"><a href="regression.html#cb814-5"></a>       <span class="dt">y =</span> <span class="st">"Teaching Score"</span>,</span>
<span id="cb814-6"><a href="regression.html#cb814-6"></a>       <span class="dt">title =</span> <span class="st">"Scatterplot of relationship of teaching and beauty scores"</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:unnamed-chunk-526"></span>
<p class="caption marginnote shownote">
FIGURE 10.2: Instructor evaluation scores at UT Austin.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-526-1.png" alt="Instructor evaluation scores at UT Austin." width="672">
</div>
<p>Observe that most “beauty” scores lie between 2 and 8, while most teaching scores lie between 3 and 5. Furthermore, while opinions may vary, it is our opinion that the relationship between teaching score and “beauty” score is “weakly positive.” This is consistent with our earlier computed correlation coefficient of 0.187.</p>
<p>Furthermore, there appear to be six points in the top-right of this plot highlighted in the box. However, this is not actually the case, as this plot suffers from <em>overplotting</em>. Recall from Subsection <a href="visualization.html#overplotting">1.7.2</a> that overplotting occurs when several points are stacked directly on top of each other, making it difficult to distinguish them. So while it may appear that there are only six points in the box, there are actually more. This fact is only apparent when using <code>geom_jitter()</code> in place of <code>geom_point()</code>. We display the resulting plot along with the same small box as before.</p>
<div class="sourceCode" id="cb815"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb815-1"><a href="regression.html#cb815-1"></a>evals_ch11 <span class="op">%&gt;%</span></span>
<span id="cb815-2"><a href="regression.html#cb815-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span></span>
<span id="cb815-3"><a href="regression.html#cb815-3"></a><span class="st">  </span><span class="kw">geom_jitter</span>() <span class="op">+</span></span>
<span id="cb815-4"><a href="regression.html#cb815-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Beauty Score"</span>, <span class="dt">y =</span> <span class="st">"Teaching Score"</span>,</span>
<span id="cb815-5"><a href="regression.html#cb815-5"></a>       <span class="dt">title =</span> <span class="st">"Scatterplot of relationship of teaching and beauty scores"</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:unnamed-chunk-528"></span>
<p class="caption marginnote shownote">
FIGURE 10.3: Instructor evaluation scores at UT Austin.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-528-1.png" alt="Instructor evaluation scores at UT Austin." width="672">
</div>
<p>It is now apparent that there are 12 points in the area highlighted in the box and not six as originally suggested. Recall from Subsection <a href="visualization.html#overplotting">1.7.2</a> on overplotting that jittering adds a little random “nudge” to each of the points to break up these ties. Furthermore, recall that jittering is strictly a visualization tool; it does not alter the original values in the data frame <code>evals_ch11</code>. To keep things simple going forward, however, we’ll only present regular scatterplots rather than their jittered counterparts.</p>
<p>Let’s build on the unjittered scatterplot by adding a “best-fitting” line: of all possible lines we can draw on this scatterplot, it is the line that “best” fits through the cloud of points. We do this by adding a new <code>geom_smooth(method = "lm", se = FALSE)</code> layer to the <code>ggplot()</code> code that created the scatterplot. The <code>method = "lm"</code> argument sets the line to be a “<code>l</code>inear <code>m</code>odel.” The <code>se = FALSE</code>  argument suppresses <em>standard error</em> uncertainty bars. (We defined the concept of <em>standard error</em> in Subsection <a href="sampling.html#sampling-definitions">6.3.2</a>.)</p>
<div class="sourceCode" id="cb816"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb816-1"><a href="regression.html#cb816-1"></a>evals_ch11 <span class="op">%&gt;%</span></span>
<span id="cb816-2"><a href="regression.html#cb816-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span></span>
<span id="cb816-3"><a href="regression.html#cb816-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb816-4"><a href="regression.html#cb816-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Beauty Score"</span>, <span class="dt">y =</span> <span class="st">"Teaching Score"</span>,</span>
<span id="cb816-5"><a href="regression.html#cb816-5"></a>       <span class="dt">title =</span> <span class="st">"Relationship between teaching and beauty scores"</span>) <span class="op">+</span><span class="st">  </span></span>
<span id="cb816-6"><a href="regression.html#cb816-6"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">"lm"</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula 'y ~ x'</code></pre>
<div class="figure">
<span id="fig:unnamed-chunk-529"></span>
<p class="caption marginnote shownote">
FIGURE 10.4: Regression line.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-529-1.png" alt="Regression line." width="672">
</div>
<p>The line in the resulting figure is called a “regression line.” The regression line  is a visual summary of the relationship between two numerical variables, in our case the outcome variable <code>score</code> and the explanatory variable <code>bty_avg</code>. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of 0.187 suggesting that there is a positive relationship between these two variables: as instructors have higher “beauty” scores, so also do they receive higher teaching evaluations. We’ll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value.</p>
<p>Furthermore, a regression line is “best-fitting” in that it minimizes some mathematical criteria. We present these mathematical criteria in Section <a href="regression.html#leastsquares">10.4</a>, but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable.</p>
</div>
<div id="model1table" class="section level3">
<h3>
<span class="header-section-number">10.1.2</span> Simple linear regression</h3>
<!-- DK: Make this with just one decimal. Indeed, can we make that consistent everywhere? Also, bty_avg should be renamed beauty. We don't really care if it is an average. 

The train example? With att_start and att_end? Or some other data set? Which one? Or qscores?
-->
<!-- EG: I like the train example. It's simple, politically-focused, and works well with the tricks we're trying to show here. -->
<p>You may recall from secondary/high school algebra that the equation of a line is <span class="math inline">\(y = a + b\cdot x\)</span>. (Note that the <span class="math inline">\(\cdot\)</span> symbol is equivalent to the <span class="math inline">\(\times\)</span> “multiply by” mathematical symbol. We’ll use the <span class="math inline">\(\cdot\)</span> symbol in the rest of this book as it is more succinct.) It is defined by two coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The intercept coefficient <span class="math inline">\(a\)</span> is the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>. The slope coefficient <span class="math inline">\(b\)</span> for <span class="math inline">\(x\)</span> is the increase in <span class="math inline">\(y\)</span> for every increase of one in <span class="math inline">\(x\)</span>. This is also called the “rise over run.”</p>
<p>However, when defining a regression line, we use slightly different notation: the equation of the regression line is <span class="math inline">\(\widehat{y} = b_0 + b_1 \cdot x\)</span> . The intercept coefficient is <span class="math inline">\(b_0\)</span>, so <span class="math inline">\(b_0\)</span> is the value of <span class="math inline">\(\widehat{y}\)</span> when <span class="math inline">\(x = 0\)</span>. The slope coefficient for <span class="math inline">\(x\)</span> is <span class="math inline">\(b_1\)</span>, i.e., the increase in <span class="math inline">\(\widehat{y}\)</span> for every increase of one in <span class="math inline">\(x\)</span>. Why do we put a “hat” on top of the <span class="math inline">\(y\)</span>? It’s a form of notation commonly used in regression to indicate that we have a  “fitted value,” or the value of <span class="math inline">\(y\)</span> on the regression line for a given <span class="math inline">\(x\)</span> value. We’ll discuss this more in the upcoming Subsection <a href="regression.html#model1points">10.2.2</a>.</p>
<!-- DK: Is this a good introduction to hat notation? This stikes me as a subtle point that should be wesved throughout the book.  -->
<!-- EG: Honestly don't think that this introduction is bad, but certainly think that it could happen much earlier in the book. Many students may already be familiar with hat notation and it is key to regression language. -->
<p>We know that the regression line we plotted has a positive slope <span class="math inline">\(b_1\)</span> corresponding to our explanatory <span class="math inline">\(x\)</span> variable <code>bty_avg</code>. Why? Because instructors with higher <code>bty_avg</code> scores tend to have higher teaching evaluation <code>scores</code>. However, what is the numerical value of the slope <span class="math inline">\(b_1\)</span>? What about the intercept <span class="math inline">\(b_0\)</span>? Let’s not compute these two values by hand, but rather let’s use a computer!</p>
<p>We can obtain the values of the intercept <span class="math inline">\(b_0\)</span> and the slope for <code>btg_avg</code> <span class="math inline">\(b_1\)</span> in two steps:</p>
<ol style="list-style-type: decimal">
<li>“Fit” the linear regression model using the <code>lm()</code> function and save it in <code>score_model</code>. We put the name of the outcome variable on the left-hand side of the <code>~</code> “tilde” sign, while putting the name of the explanatory variable on the right-hand side. This is known as R’s  <em>formula notation</em>.</li>
<li>Apply the <code>tidy()</code>  function from the <strong>broom</strong> package to <code>score_model</code>.</li>
</ol>
<!-- EG: Should we take a sentence to explain why tidy is being used? Something as simple as "This will create the regression table." --><div class="sourceCode" id="cb818"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb818-1"><a href="regression.html#cb818-1"></a><span class="co"># Fit regression model:</span></span>
<span id="cb818-2"><a href="regression.html#cb818-2"></a></span>
<span id="cb818-3"><a href="regression.html#cb818-3"></a>score_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch11)</span>
<span id="cb818-4"><a href="regression.html#cb818-4"></a></span>
<span id="cb818-5"><a href="regression.html#cb818-5"></a><span class="co"># Get regression table:</span></span>
<span id="cb818-6"><a href="regression.html#cb818-6"></a></span>
<span id="cb818-7"><a href="regression.html#cb818-7"></a>score_model <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb818-8"><a href="regression.html#cb818-8"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 7
##   term        estimate std.error statistic   p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   3.88      0.0761     51.0  1.56e-191   3.73      4.03  
## 2 bty_avg       0.0666    0.0163      4.09 5.08e-  5   0.0346    0.0987</code></pre>
<p>Note that we used the argument <code>conf.int = TRUE</code> in the <code>tidy()</code> function; that will be important for later.</p>
<p>How did this code work? First, we “fit” the linear regression model to the <code>data</code> using the <code>lm()</code>  function and saved this as <code>score_model</code>. When we say “fit”, we mean “find the best fitting line to this data.” <code>lm()</code> stands for “linear model” and is used as follows: <code>lm(y ~ x, data = data_frame_name)</code> where:</p>
<ul>
<li>
<code>y</code> is the outcome variable, followed by a tilde <code>~</code>. In our case, <code>y</code> is set to <code>score</code>.</li>
<li>
<code>x</code> is the explanatory variable. In our case, <code>x</code> is set to <code>bty_avg</code>.</li>
<li>The combination of <code>y ~ x</code> is called a <em>model formula</em>. (Note the order of <code>y</code> and <code>x</code>.) In our case, the model formula is <code>score ~ bty_avg</code>.</li>
<li>
<code>data_frame_name</code> is the name of the data frame that contains the variables <code>y</code> and <code>x</code>. In our case, <code>data_frame_name</code> is the <code>evals_ch11</code> data frame.</li>
</ul>
<p>Second, we take the saved model in <code>score_model</code> and apply the <code>tidy()</code> function from the <strong>broom</strong> package to it to obtain the regression table.</p>
<p>We can also do this with pipes:</p>
<div class="sourceCode" id="cb820"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb820-1"><a href="regression.html#cb820-1"></a>evals_ch11 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb820-2"><a href="regression.html#cb820-2"></a><span class="st">  </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> .) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb820-3"><a href="regression.html#cb820-3"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb820-4"><a href="regression.html#cb820-4"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 4
##   term        estimate conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   3.88     3.73      4.03  
## 2 bty_avg       0.0666   0.0346    0.0987</code></pre>
<p>Recall from our discussion in Chapter @ref{wrangling} that we can use “.” to refer to the tibble that has been passed in by the proceeding pipe. In this case, that is the <code>eval_ch11</code> tibble. Using “.” to refer to the passed-down tibble will be a trick which we use again and again in the coming chapters.</p>
<p>You may wonder why this code does not work:</p>
<div class="sourceCode" id="cb822"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb822-1"><a href="regression.html#cb822-1"></a>evals_ch11 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb822-2"><a href="regression.html#cb822-2"></a><span class="st">  </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg) </span></code></pre></div>
<!-- DK: How can we show the error which happens when this code runs? -->
<!-- EG: Not totally sure what is meant by "show the error," but I think the explanation below the code is sufficient for people to understand why lm does not work with piped in data. -->
<p>With most of the functions we have seen, we don’t need the “hack” of referring to the passed-down tibble with a “.”. Things “just work.” Functions like <code>filter()</code> and <code>arrange()</code> just know to work on the tibble which is coming there way. Why doesn’t <code>lm()</code> do that?</p>
<p>The answer is that <code>lm()</code> is, like many parts of R, an old function, created before the Tidyverse came into style. Look at the help page with <code>?lm</code>. As you can see, the first argument is <code>formula</code>. The automatic pass-down trick only works with functions in which the first argument is the data to be used. This is (almost) always the case in Tidyverse functions.</p>
</div>
<div id="interpreting-regression-coefficients" class="section level3">
<h3>
<span class="header-section-number">10.1.3</span> Interpreting regression coefficients</h3>
<p>In the <code>estimate</code> column are the intercept <span class="math inline">\(b_0\)</span> = 3.880338 and the slope <span class="math inline">\(b_1\)</span> = 0.066637 for <code>bty_avg</code>. Thus the equation of the regression line follows:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{y} &amp;= b_0 + b_1 \cdot x\\
\widehat{\text{score}} &amp;= b_0 + b_{\text{bty}\_\text{avg}} \cdot\text{bty}\_\text{avg}\\
&amp;= 3.880 + 0.067\cdot\text{bty}\_\text{avg}
\end{aligned}
\]</span></p>
<!-- DK: "beauty" rather than "bty_avg" would be a better variable name. Don't need to recall that the beauty score is an average. After all, the score is an average also! Also: Need a discussion of potential outcomes and RCM here. Can we manipulate beauty? On one hand, no. On the other hand, yes. Makeup, lighting and so on. Could provide an interesting complex case to discuss. -->
<!-- EG: I agree that RCM and potential outcomes would fit well within this discussion.Took a look at chapter 3 and noticed that the trains example is used there to explain potential outcomes. Should we stick with using the UT data here and simply apply the language of potential outcomes to this example as well? -->
<p>The intercept <span class="math inline">\(b_0\)</span> = 3.880338 is the average teaching score <span class="math inline">\(\widehat{y}\)</span> = <span class="math inline">\(\widehat{\text{score}}\)</span> for those courses where the instructor had a “beauty” score <code>bty_avg</code> of 0. Or in graphical terms, it’s where the line intersects the <span class="math inline">\(y\)</span> axis when <span class="math inline">\(x\)</span> = 0. Note, however, that while the  intercept of the regression line has a mathematical interpretation, it has no <em>practical</em> interpretation here, since observing a <code>bty_avg</code> of 0 is impossible; it is the average of six panelists’ “beauty” scores ranging from 1 to 10. Furthermore, looking at the scatterplot with the regression line, no instructors had a “beauty” score anywhere near 0.</p>
<!-- DK: Add discussion of "hat". Recall the p hat which we estimated last time. y hat is like that! It is not a variable that we can observe. It is an estimate. (Indeed, it is an estimate of a potential outcome!) This is different form x, which has no hat, because it is real data, something we can see. Side note: not sure if the hat versus no hat distinction works well with b_0/b_1 being things we can't see and need to estimate, but for which we do not use hat notation. -->
<!-- EG: I'll add a discussion of y hat as I start writing. -->
<p>Of greater interest is the  slope <span class="math inline">\(b_1\)</span> = <span class="math inline">\(b_{\text{bty_avg}}\)</span> of 0.066637. The “bty_avg” subscript indicates that this number summarizes the relationship between the teaching and average beauty score variables. Note that the sign is positive, suggesting a positive relationship between these two variables, meaning teachers with higher <code>bty_avg</code> scores also tend to have higher teaching scores. Recall from earlier that the correlation coefficient is 0.187. They both have the same positive sign, but have a different value. Recall further that the correlation’s interpretation is the “strength of linear association”. The  slope’s interpretation is a little different:</p>
<!-- DK: Let's make sure that these interpretations are highly consistent with Gelman and across the book. -->
<blockquote>
<p>For every increase of 1 unit in <code>bty_avg</code>, there is an <em>associated</em> increase of, <em>on average</em>, 0.066637 units of <code>score</code>.</p>
</blockquote>
<p>We say that this associated increase is <em>on average</em> 0.066637 units of teaching <code>score</code>, because you might have two instructors whose <code>bty_avg</code> scores differ by 1 unit, but their difference in teaching scores won’t necessarily be exactly 0.066637. What the slope of 0.066637 is saying is that across all possible courses, the <em>average</em> difference in teaching score between two instructors whose “beauty” scores differ by one is 0.066637.</p>
<p>Furthermore, we only state that there is an <em>associated</em> increase and not necessarily a <em>causal</em> increase. For example, perhaps it’s not that higher “beauty” scores directly cause higher teaching scores per se. Instead, the following could hold true: individuals from wealthier backgrounds tend to have stronger educational backgrounds and hence have higher teaching scores, while at the same time these wealthy individuals also tend to have higher “beauty” scores. In other words, just because two variables are strongly associated, it doesn’t necessarily mean that one causes the other. This is summed up in the often quoted phrase, “correlation is not necessarily causation.”</p>
<p>Consider an example: a not-so-great medical doctor goes through medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares, “Sleeping with shoes on causes headaches!”</p>
<div class="figure">
<span id="fig:unnamed-chunk-535"></span>
<p class="caption marginnote shownote">
FIGURE 10.5: Does sleeping with shoes on cause headaches?
</p>
<img src="10-continuous-response-i/images/shoes_headache.png" alt="Does sleeping with shoes on cause headaches?">
</div>
<p>However, there is a good chance that if someone is sleeping with their shoes on, it’s potentially because they are intoxicated from alcohol. Furthermore, higher levels of drinking leads to more hangovers, and hence more headaches. The amount of alcohol consumption here is what’s known as a <em>confounding</em> variable. It lurks behind the scenes, confounding the causal relationship (if any) of “sleeping with shoes on” with “waking up with a headache.” We can summarize this with a <em>causal graph</em> where:</p>
<ul>
<li>Y is a <em>response</em> variable; here it is “waking up with a headache.” </li>
<li>X is a <em>treatment</em> variable whose causal effect we are interested in; here it is “sleeping with shoes on.”</li>
</ul>
<div class="figure">
<span id="fig:unnamed-chunk-536"></span>
<p class="caption marginnote shownote">
FIGURE 10.6: Causal graph.
</p>
<img src="10-continuous-response-i/images/flowchart.009-cropped.png" alt="Causal graph.">
</div>
<p>To study the relationship between Y and X, we could use a regression model where the outcome variable is set to Y and the explanatory variable is set to be X, as you’ve been doing throughout this chapter. However, the causal graph also includes a third variable with arrows pointing at both X and Y:</p>
<ul>
<li>Z is a <em>confounding</em> variable  that affects both X and Y, thereby “confounding” their relationship. Here the confounding variable is alcohol.</li>
</ul>
<p>Alcohol will cause people to be both more likely to sleep with their shoes on as well as be more likely to wake up with a headache. We can see why this is a problem under the potential outcomes framework: both whether you receive the treatment “sleeping with shoes on” and your potential outcome depend on whether you consumed alcohol, which means that you can’t estimate the treatment effect of “sleeping with shoes on” simply by comparing the observed outcome under treatment and the observed outcome under control. Thus any regression model of the relationship between X and Y should also use Z as an explanatory variable. In other words, our doctor needs to take into account who had been drinking the night before. In the next chapter, we’ll start covering multiple regression models that allow us to incorporate more than one variable in our regression models.</p>
<!-- DK: In the next chapter? Not if we re-organize. -->
<p>Establishing causation is a tricky problem and frequently takes either randomized controlled trials or methods to adjust for the effects of confounding variables. Both these approaches attempt, as best they can, either to take all possible confounding variables into account or negate their impact. This allows researchers to focus only on the relationship of interest: the relationship between the outcome variable Y and the treatment variable X.</p>
<p>As you read news stories, be careful not to fall into the trap of thinking that correlation necessarily implies causation. Check out the <a href="http://www.tylervigen.com/spurious-correlations">Spurious Correlations</a> website for some rather comical examples of variables that are correlated, but are definitely not causally related.</p>
<!--AR: trying to be precise that it is correlation with the treatment and *potential* outcomes that matters, not correlation with the treatment and observed outcomes.  What's a better way to explain this?
DK: Agreed! Indeed, we need to hit the notion of potential outcomes each chapter. Potential outcomes != observed outcomes! Not mentioning RCM for 4 weeks is a mistake. -->
<p>Let’s say, however, that we were confident that there is no confounding: that is, that there are no variables such as socioeconomic background that correlate both with a teacher’s beauty score and the teacher’s potential outcomes. (We shouldn’t be confident in this, but let’s play along for the moment.) Then, we can interpret the slope in terms of the <a href="rubin-causal-model.html#rubin-causal-model">Rubin Causal Model</a>. The slope coefficient on <code>bty_avg</code> of 0.066637 then means that the <em>average treatment effect</em> of increasing a teacher’s “beauty” score by 1 is 0.066637. In the absence of randomization, however, this is likely not a good interpretation of this regression! Adding additional variables, as we’ll do in Chapter @ref{multiple-regression}, may make it more plausible to interpret the regression causally.</p>
<p>Keeping with the causal interpretation, let’s say that a teacher named Joe had a “beauty” score of 7 and a teaching score of 4. We are curious about how much moving from a “beauty” score of 7 versus “beauty” score of 8 would have affected Joe’s teaching score. Thus, we are comparing the <em>potential outcome</em> for Joe when <code>bty_avg</code> is 7 versus the <em>potential outcome</em> when <code>bty_avg</code> is 8. Only one of these potential outcomes was observed.</p>
<!-- DK: This is good stuff. Belongs in every chapter. -->
<!-- EG: Should we be using kable here? Or are we trying to make the switch to gt? -->
<!-- EG: I'm going to switch this to gt next pass around. -->
<table class="table table-striped" style="width: auto !important; ">
<thead><tr>
<th style="text-align:left;">
Subject
</th>
<th style="text-align:left;">
<span class="math inline">\(Y_{bty\_avg = 7}\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(Y_{bty\_avg = 8}\)</span>
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid;">
Joe
</td>
<td style="text-align:left;">
<span class="math inline">\(4\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(?\)</span>
</td>
</tr></tbody>
</table>
<p>We can use our slope coefficient from the linear regression, which is our best estimate of the average treatment effect (ATE), to fill in the missing potential outcome:</p>
<table class="table table-striped" style="width: auto !important; ">
<thead><tr>
<th style="text-align:left;">
Subject
</th>
<th style="text-align:left;">
<span class="math inline">\(Y_{bty\_avg = 7}\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(Y_{bty\_avg = 8}\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(\widehat{ATE}\)</span>
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid;">
Joe
</td>
<td style="text-align:left;">
<span class="math inline">\(4\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(4.067\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.067\)</span>
</td>
</tr></tbody>
</table>
<p>Thus, our best guess of Joe’s potential outcome under the “treatment” of <span class="math inline">\(score = 8\)</span> is <span class="math inline">\(4.067\)</span>.</p>
</div>
</div>
<div id="uncertainty-in-simple-linear-regressions" class="section level2">
<h2>
<span class="header-section-number">10.2</span> Uncertainty in simple linear regressions</h2>
<!-- EG: Will provide more explanation of parameter undertainty using tidy(). -->
<p>You might be wondering what the remaining five columns created by <code>tidy()</code> are: <code>std.error</code>, <code>statistic</code>, <code>p.value</code>, <code>conf.low</code> and <code>conf.high</code>. They are the <em>standard error</em>, <em>test statistic</em>, <em>p-value</em>, <em>lower 95% confidence interval bound</em>, and <em>upper 95% confidence interval bound</em>. We will focus on the confidence interval bounds. We have already explored the concept of confidence intervals in Chapter <a href="#confidence-intervals"><strong>??</strong></a>. These bounds are an application of the same concept, but applied to the slope of our simple linear regression.</p>
<p>Let’s load the <strong>infer</strong> package. We’re going to use the function <code>rep_sample_n()</code> to resample from our data 1,000 times:</p>
<div class="sourceCode" id="cb823"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb823-1"><a href="regression.html#cb823-1"></a><span class="kw">library</span>(infer)</span>
<span id="cb823-2"><a href="regression.html#cb823-2"></a></span>
<span id="cb823-3"><a href="regression.html#cb823-3"></a>x &lt;-<span class="st"> </span>evals_ch11 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb823-4"><a href="regression.html#cb823-4"></a><span class="st">  </span><span class="kw">select</span>(score, bty_avg) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb823-5"><a href="regression.html#cb823-5"></a><span class="st">  </span><span class="kw">rep_sample_n</span>(<span class="dt">size =</span> <span class="kw">nrow</span>(evals_ch11), <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">reps =</span> <span class="dv">1000</span>)</span>
<span id="cb823-6"><a href="regression.html#cb823-6"></a></span>
<span id="cb823-7"><a href="regression.html#cb823-7"></a>x</span></code></pre></div>
<pre><code>## # A tibble: 463,000 x 3
## # Groups:   replicate [1,000]
##    replicate score bty_avg
##        &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1         1   4.3    4.17
##  2         1   4.9    4.5 
##  3         1   3.6    2.83
##  4         1   4.9    4   
##  5         1   4.4    4.17
##  6         1   3.3    5.5 
##  7         1   3.9    5.83
##  8         1   4.3    4.33
##  9         1   4.6    7.33
## 10         1   4.4    4.17
## # … with 462,990 more rows</code></pre>
<p>For each <code>replicate</code>, we have 463 resamples of <code>score</code> and <code>bty_avg</code>.</p>
<p>Next, we are going to introduce a new function: <code>nest()</code>:</p>
<div class="sourceCode" id="cb825"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb825-1"><a href="regression.html#cb825-1"></a>x &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb825-2"><a href="regression.html#cb825-2"></a><span class="st">  </span><span class="kw">group_by</span>(replicate) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb825-3"><a href="regression.html#cb825-3"></a><span class="st">  </span><span class="kw">nest</span>()</span>
<span id="cb825-4"><a href="regression.html#cb825-4"></a></span>
<span id="cb825-5"><a href="regression.html#cb825-5"></a>x</span></code></pre></div>
<pre><code>## # A tibble: 1,000 x 2
## # Groups:   replicate [1,000]
##    replicate data              
##        &lt;int&gt; &lt;list&gt;            
##  1         1 &lt;tibble [463 × 2]&gt;
##  2         2 &lt;tibble [463 × 2]&gt;
##  3         3 &lt;tibble [463 × 2]&gt;
##  4         4 &lt;tibble [463 × 2]&gt;
##  5         5 &lt;tibble [463 × 2]&gt;
##  6         6 &lt;tibble [463 × 2]&gt;
##  7         7 &lt;tibble [463 × 2]&gt;
##  8         8 &lt;tibble [463 × 2]&gt;
##  9         9 &lt;tibble [463 × 2]&gt;
## 10        10 &lt;tibble [463 × 2]&gt;
## # … with 990 more rows</code></pre>
<p><code>nest()</code> comes from the <strong>tidyr</strong> package. It’s easiest to understand what <code>nest()</code> does by looking at its output. After grouping by <code>replicate</code> and using <code>nest()</code>, we now have a dataset of 1,000 rows and a list column named <code>data</code>. What’s in this list column? Each element of <code>data</code> is a tibble consisting of one of the resampled datasets we created using <code>rep_sample_n()</code>. <code>nest()</code> is thus a useful function when you have a series of bootstrapped samples and want to run the same function on each sample.</p>
<!-- DK: This transition goes a bit fast. We need to take a tour of the tibbles in data. Look at one. Look at another. See that they are different. Explain how this is a more industrial strength way of using rep_sample(). But maybe that means that rep_sample_n(), used alone, is a bad hack. Maybe we should have done it this way, with nest(), in chapters 9 and 10. In fact, maybe we could get rid of rep_sample_n and use something in **rsample** to accomplish these two steps. -->
<!-- EG: I agree with the comment above. Nest() is a tricky function that takes a while to understand (and also to figure out how to use properly and when it is necessary!) I think comparison is a great idea here. Slowly going through the differences in using nest() and rep_sample_n(). I'm not sure about cutting the mention of r_sample_n() entirely (it is indeed a useful hack to know), but I think it is important to go over in detail when nest() ought to be used in the place of rep_sample_n(). -->
<p>Now, we can use <code>map()</code> to run our linear regression for each dataset:</p>
<!-- DK: Whoah! Note that this use of "." is different than the "." that we saw before. Or, rather, a "." within a map function refers to the next item in the list which is driving the map. -->
<div class="sourceCode" id="cb827"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb827-1"><a href="regression.html#cb827-1"></a>x &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb827-2"><a href="regression.html#cb827-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mod =</span> <span class="kw">map</span>(data, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> .)))</span>
<span id="cb827-3"><a href="regression.html#cb827-3"></a></span>
<span id="cb827-4"><a href="regression.html#cb827-4"></a>x</span></code></pre></div>
<pre><code>## # A tibble: 1,000 x 3
## # Groups:   replicate [1,000]
##    replicate data               mod   
##        &lt;int&gt; &lt;list&gt;             &lt;list&gt;
##  1         1 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
##  2         2 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
##  3         3 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
##  4         4 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
##  5         5 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
##  6         6 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
##  7         7 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
##  8         8 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
##  9         9 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
## 10        10 &lt;tibble [463 × 2]&gt; &lt;lm&gt;  
## # … with 990 more rows</code></pre>
<p>Note that the use of “.” in this call to <code>map()</code> is somewhat different to the use of “.” that we have seen before. In a normal pipe, a “.” refers to the tibble which was passed down from the previous line. However, <em>within a map function</em>, a “.” refers to an element in the .x object (the first argument) which the <code>map()</code> function is iterating over.</p>
<p>In this case, <code>map()</code> is iterating over <code>data</code>, which is a column in the tibble <code>x</code>. <code>map()</code> goes through each item of <code>data</code>, passing each to the function which is its <code>.f</code> input, in this case an anonymous function built around <code>lm()</code>. So, each time <code>lm()</code> runs it is using a different row from the <code>data</code> variable. <code>map()</code> is iterating over <code>data</code>, passing each element in <code>data</code> as an argument to <code>lm()</code>.</p>
<p>Now we have a new list column, <code>mod</code>, that contains the model objects created by <code>lm()</code>. We will now want to <code>tidy()</code> the object created by <code>lm()</code>:</p>
<!-- DK: Again, too fast! Need to look at mod. Show us the first model. Show us the second. They are different. Why? Is this what you expected? -->
<!-- EG: Agreed. This is too fast. Nest() and map() should both be discussed in more length- it is challenging to explain them effectively in few explanations/examples. -->
<div class="sourceCode" id="cb829"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb829-1"><a href="regression.html#cb829-1"></a>x &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb829-2"><a href="regression.html#cb829-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">reg_results =</span> <span class="kw">map</span>(mod, <span class="op">~</span><span class="st"> </span><span class="kw">tidy</span>(.)))</span>
<span id="cb829-3"><a href="regression.html#cb829-3"></a></span>
<span id="cb829-4"><a href="regression.html#cb829-4"></a>x</span></code></pre></div>
<pre><code>## # A tibble: 1,000 x 4
## # Groups:   replicate [1,000]
##    replicate data               mod    reg_results     
##        &lt;int&gt; &lt;list&gt;             &lt;list&gt; &lt;list&gt;          
##  1         1 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
##  2         2 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
##  3         3 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
##  4         4 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
##  5         5 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
##  6         6 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
##  7         7 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
##  8         8 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
##  9         9 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
## 10        10 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;
## # … with 990 more rows</code></pre>
<p><code>tidy()</code> stores the coefficients in the <code>estimate</code> column, with each coefficient named in the <code>term</code> column. Thus, if we <code>filter()</code> by <code>term</code> and <code>pull(estimate)</code>, we can get the regression coefficient for each bootstrap sample:</p>
<!-- DK: Show us this! And talk about this anonymous function, built from two parts. -->
<div class="sourceCode" id="cb831"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb831-1"><a href="regression.html#cb831-1"></a>x &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span></span>
<span id="cb831-2"><a href="regression.html#cb831-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">disp_coef =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> "bty_avg"</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(estimate)))</span>
<span id="cb831-3"><a href="regression.html#cb831-3"></a></span>
<span id="cb831-4"><a href="regression.html#cb831-4"></a>x</span></code></pre></div>
<pre><code>## # A tibble: 1,000 x 5
## # Groups:   replicate [1,000]
##    replicate data               mod    reg_results      disp_coef
##        &lt;int&gt; &lt;list&gt;             &lt;list&gt; &lt;list&gt;               &lt;dbl&gt;
##  1         1 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0533
##  2         2 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0642
##  3         3 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0473
##  4         4 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0690
##  5         5 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0524
##  6         6 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0576
##  7         7 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0679
##  8         8 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0770
##  9         9 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0815
## 10        10 &lt;tibble [463 × 2]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt;    0.0409
## # … with 990 more rows</code></pre>
<p>Now that we have 1,000 estimates from our bootstrap samples, we can construct a percentile-based confidence interval easily:</p>
<div class="sourceCode" id="cb833"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb833-1"><a href="regression.html#cb833-1"></a>x <span class="op">%&gt;%</span></span>
<span id="cb833-2"><a href="regression.html#cb833-2"></a><span class="st"> </span><span class="kw">pull</span>(disp_coef) <span class="op">%&gt;%</span></span>
<span id="cb833-3"><a href="regression.html#cb833-3"></a><span class="st"> </span><span class="kw">quantile</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##       2.5%        50%      97.5% 
## 0.03161809 0.06427183 0.10027147</code></pre>
<p>Now let’s return to our example for Joe. Since our slope coefficient is measured with some uncertainty, our estimate of his potential outcome under the “treatment” of having a “beauty” score of 8 is also measured with some uncertainty, and that should be reflected in our table:</p>
<table class="table table-striped" style="width: auto !important; ">
<thead><tr>
<th style="text-align:left;">
Subject
</th>
<th style="text-align:left;">
<span class="math inline">\(Y_{bty\_avg = 7}\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(Y_{bty\_avg = 8}\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(\widehat{ATE}\)</span>
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid;">
Joe
</td>
<td style="text-align:left;">
<span class="math inline">\(4\)</span>
</td>
<td style="text-align:left;">
$<span class="math inline">\(4.067\)</span> <span class="math inline">\((4.034,\)</span> <span class="math inline">\(4.099)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.067\)</span> <span class="math inline">\((0.034,\)</span> <span class="math inline">\(0.099)\)</span>
</td>
</tr></tbody>
</table>
<!-- DK: Not bad. But, again, we want to discuss hypthesis tests (and p-values) each chapter, perhaps growing more sophisticated each time, perhaps ending with the ASA statement. --><p>Instead of looking at confidence intervals, a common alternative approach is to conduct <em>hypothesis tests</em>, where one hypothesis is called the “null hypothesis” (in a regression context, generally that the regression coefficient is equal to zero) and the result of the test is either <em>rejecting</em> the null hypothesis (so you’d conclude that the regression coefficient probably is not zero) or <em>failing to reject</em> the null hypothesis. The decision whether to reject the null hypothesis is generally made with reference to a <em>p-value</em>, a measure of how likely one would observe results at least as extreme as the results actually observed if the null hypothesis were true. A <em>p</em>-value cutoff, often 0.05, is employed: if the <em>p</em>-value is lower, the null hypothesis is rejected, otherwise the hypothesis is not rejected.</p>
<p>We think this is a bad way to make decisions. Two very similar datasets could produce <em>p</em>-values of <span class="math inline">\(p = 0.04\)</span> and <span class="math inline">\(p = 0.06\)</span> for a coefficient of interest. If you would make one decision in the former case and a totally different decision in the latter case, then there’s something wrong with your decision-making process! Rather, we think it is more sensible to look at the data, construct models to summarize important features of the data, and make decisions based on those models that take into account the uncertainty in the models’ estimates.</p>
<div id="using-lm-and-tidy-as-a-shortcut" class="section level3">
<h3>
<span class="header-section-number">10.2.1</span> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</h3>
<p>While this process is relatively straightforward, there’s a simple shortcut we can use. Take a look at the results from <code>score_model %&gt;% tidy(conf.int = TRUE)</code>:</p>
<div class="sourceCode" id="cb835"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb835-1"><a href="regression.html#cb835-1"></a>score_model <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb835-2"><a href="regression.html#cb835-2"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb835-3"><a href="regression.html#cb835-3"></a><span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> "bty_avg"</span>) <span class="op">%&gt;%</span></span>
<span id="cb835-4"><a href="regression.html#cb835-4"></a><span class="st">  </span><span class="kw">select</span>(estimate, conf.low, conf.high)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   estimate conf.low conf.high
##      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1   0.0666   0.0346    0.0987</code></pre>
<!-- DK: Don't hard code numbers! -->
<p>The confidence intervals reported by <code>lm()</code> and <code>tidy()</code> are very similar to our bootstrap confidence intervals. Thus, we can interpret these confidence intervals the same way we did in Chapter <a href="#confidence-intervals"><strong>??</strong></a>. There is a 95% chance that the true value of the coefficient of <code>bty_avg</code> is between, roughly, 0.07 and 0.1. From now on, we’ll just use <code>lm()</code> because it is simpler, but when considering how to interpret the confidence intervals, remember that you’d obtain very similar results from the bootstrap method.</p>
</div>
<div id="model1points" class="section level3">
<h3>
<span class="header-section-number">10.2.2</span> Observed/fitted values and residuals</h3>
<p>We just saw how to get the value of the intercept and the slope of a regression line from the <code>estimate</code> column of a regression table generated by the <code>tidy()</code> function. Now instead say we want information on individual observations. For example, let’s focus on the 21st of the 463 courses in the <code>evals_ch11</code> data frame:</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-547">TABLE 10.2: </span>Data for the 21st course out of 463
</caption>
<thead><tr>
<th style="text-align:right;">
ID
</th>
<th style="text-align:right;">
score
</th>
<th style="text-align:right;">
bty_avg
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
4.9
</td>
<td style="text-align:right;">
7.33
</td>
</tr></tbody>
</table>
<p>What is the value <span class="math inline">\(\widehat{y}\)</span> on the regression line corresponding to this instructor’s <code>bty_avg</code> “beauty” score of 7.333? We will mark three values corresponding to the instructor for this 21st course and give their statistical names:</p>
<ul>
<li>Circle: The <em>observed value</em> <span class="math inline">\(y\)</span> = 4.9 is this course’s instructor’s actual teaching score.</li>
<li>Square: The <em>fitted value</em> <span class="math inline">\(\widehat{y}\)</span> is the value on the regression line for <span class="math inline">\(x\)</span> = <code>bty_avg</code> = 7.333. This value is computed using the intercept and slope in the previous regression table:</li>
</ul>
<p><span class="math display">\[\widehat{y} = b_0 + b_1 \cdot x = 3.880338 + 0.066637 \cdot 7.333 = 4.3689873\]</span></p>
<ul>
<li>Arrow: The length of this arrow is the <em>residual</em>  and is computed by subtracting the fitted value <span class="math inline">\(\widehat{y}\)</span> from the observed value <span class="math inline">\(y\)</span>. The residual can be thought of as a model’s error or “lack of fit” for a particular observation. In the case of this course’s instructor, it is <span class="math inline">\(y - \widehat{y}\)</span> = 4.9 - 4.3689873 = 0.5310127.</li>
</ul>
<pre><code>## `geom_smooth()` using formula 'y ~ x'</code></pre>
<div class="figure">
<span id="fig:unnamed-chunk-548"></span>
<p class="caption marginnote shownote">
FIGURE 10.7: Example of observed value, fitted value, and residual.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-548-1.png" alt="Example of observed value, fitted value, and residual." width="672">
</div>
<p>Now say we want to compute both the fitted value <span class="math inline">\(\widehat{y} = b_0 + b_1 \cdot x\)</span> and the residual <span class="math inline">\(y - \widehat{y}\)</span> for <em>all</em> 463 courses in the study. Recall that each course corresponds to one of the 463 rows in the <code>evals_ch11</code> data frame and also one of the 463 points in the regression plot.</p>
<p>We could repeat the previous calculations we performed by hand 463 times, but that would be tedious and time consuming. Instead, let’s do this using a computer with the <code>augment()</code> function in the <strong>broom</strong> package. Let’s apply the <code>augment()</code> function to <code>score_model</code>, which is where we saved our <code>lm()</code> model in the previous section. We present the results of only the 21st through 24th courses for brevity’s sake.</p>
<div class="sourceCode" id="cb838"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb838-1"><a href="regression.html#cb838-1"></a>regression_points &lt;-<span class="st"> </span>score_model <span class="op">%&gt;%</span></span>
<span id="cb838-2"><a href="regression.html#cb838-2"></a><span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span></span>
<span id="cb838-3"><a href="regression.html#cb838-3"></a><span class="st">  </span><span class="kw">select</span>(score, bty_avg, .fitted, .resid)</span>
<span id="cb838-4"><a href="regression.html#cb838-4"></a>regression_points</span></code></pre></div>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:unnamed-chunk-550">TABLE 10.3: </span>Regression points (for only the 21st through 24th courses)</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:right;">
score
</th>
<th style="text-align:right;">
bty_avg
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
.resid
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;">
4.9
</td>
<td style="text-align:right;">
7.333
</td>
<td style="text-align:right;">
4.369
</td>
<td style="text-align:right;">
0.531
</td>
</tr>
<tr>
<td style="text-align:right;">
4.6
</td>
<td style="text-align:right;">
7.333
</td>
<td style="text-align:right;">
4.369
</td>
<td style="text-align:right;">
0.231
</td>
</tr>
<tr>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
7.333
</td>
<td style="text-align:right;">
4.369
</td>
<td style="text-align:right;">
0.131
</td>
</tr>
<tr>
<td style="text-align:right;">
4.4
</td>
<td style="text-align:right;">
5.500
</td>
<td style="text-align:right;">
4.247
</td>
<td style="text-align:right;">
0.153
</td>
</tr>
</tbody>
</table>
<p>Let’s inspect the individual columns and match them with the elements of our regression plot:</p>
<ul>
<li>The <code>score</code> column represents the observed outcome variable <span class="math inline">\(y\)</span>. This is the y-position of the 463 black points.</li>
<li>The <code>bty_avg</code> column represents the values of the explanatory variable <span class="math inline">\(x\)</span>. This is the x-position of the 463 black points.</li>
<li>The <code>.fitted</code> column represents the fitted values <span class="math inline">\(\widehat{y}\)</span>. This is the corresponding value on the regression line for the 463 <span class="math inline">\(x\)</span> values.</li>
<li>The <code>.resid</code> column represents the residuals <span class="math inline">\(y - \widehat{y}\)</span>. This is the 463 vertical distances between the 463 black points and the regression line.</li>
</ul>
<p>Just as we did for the instructor of the 21st course in the <code>evals_ch11</code> dataset (in the first row of the table), let’s repeat the calculations for the instructor of the 24th course (in the fourth row):</p>
<ul>
<li>
<code>score</code> = 4.4 is the observed teaching <code>score</code> <span class="math inline">\(y\)</span> for this course’s instructor.</li>
<li>
<code>bty_avg</code> = 5.50 is the value of the explanatory variable <code>bty_avg</code> <span class="math inline">\(x\)</span> for this course’s instructor.</li>
<li>
<code>.fitted</code> = 4.25 = 3.880338 + 0.066637 <span class="math inline">\(\cdot\)</span> 5.50 is the fitted value <span class="math inline">\(\widehat{y}\)</span> on the regression line for this course’s instructor.</li>
<li>
<code>.resid</code> = 0.153 = 4.4 - 4.25 is the value of the residual for this instructor. In other words, the model’s fitted value was off by 0.153 teaching score units for this course’s instructor.</li>
</ul>
<p>At this point, you can skip ahead if you like to Section <a href="regression.html#leastsquares">10.4</a> to learn about the processes behind what makes “best-fitting” regression lines. As a primer, a “best-fitting” line refers to the line that minimizes the <em>sum of squared residuals</em> out of all possible lines we can draw through the points. In Section <a href="#model2"><strong>??</strong></a>, we’ll discuss another common scenario of having a categorical explanatory variable and a numerical outcome variable.</p>
<p>Constructing a measure of uncertainty around the fitted values can also be done using the <code>augment()</code> function. The simplest way to do this is through the standard error method that we learned in the last chapter. Note that <code>augment()</code> saves the standard errors of the fitted values in a column called <code>.se.fit</code>:</p>
<div class="sourceCode" id="cb839"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb839-1"><a href="regression.html#cb839-1"></a>score_model <span class="op">%&gt;%</span></span>
<span id="cb839-2"><a href="regression.html#cb839-2"></a><span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span></span>
<span id="cb839-3"><a href="regression.html#cb839-3"></a><span class="st">  </span><span class="kw">select</span>(score, bty_avg, .fitted, .se.fit, .resid)</span></code></pre></div>
<pre><code>## # A tibble: 463 x 5
##    score bty_avg .fitted .se.fit   .resid
##    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
##  1   4.7    5       4.21  0.0266  0.486  
##  2   4.1    5       4.21  0.0266 -0.114  
##  3   3.9    5       4.21  0.0266 -0.314  
##  4   4.8    5       4.21  0.0266  0.586  
##  5   4.6    3       4.08  0.0339  0.520  
##  6   4.3    3       4.08  0.0339  0.220  
##  7   2.8    3       4.08  0.0339 -1.28   
##  8   4.1    3.33    4.10  0.0305 -0.00244
##  9   3.4    3.33    4.10  0.0305 -0.702  
## 10   4.5    3.17    4.09  0.0321  0.409  
## # … with 453 more rows</code></pre>
<p>Now that we have the standard error of the fitted values, we can construct a confidence interval easily:</p>
<div class="sourceCode" id="cb841"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb841-1"><a href="regression.html#cb841-1"></a>score_model <span class="op">%&gt;%</span></span>
<span id="cb841-2"><a href="regression.html#cb841-2"></a><span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span></span>
<span id="cb841-3"><a href="regression.html#cb841-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</span>
<span id="cb841-4"><a href="regression.html#cb841-4"></a>         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></span>
<span id="cb841-5"><a href="regression.html#cb841-5"></a><span class="st">  </span><span class="kw">select</span>(score, bty_avg, .fitted, conf.low, conf.high, .resid)</span></code></pre></div>
<pre><code>## # A tibble: 463 x 6
##    score bty_avg .fitted conf.low conf.high   .resid
##    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1   4.7    5       4.21     4.16      4.27  0.486  
##  2   4.1    5       4.21     4.16      4.27 -0.114  
##  3   3.9    5       4.21     4.16      4.27 -0.314  
##  4   4.8    5       4.21     4.16      4.27  0.586  
##  5   4.6    3       4.08     4.01      4.15  0.520  
##  6   4.3    3       4.08     4.01      4.15  0.220  
##  7   2.8    3       4.08     4.01      4.15 -1.28   
##  8   4.1    3.33    4.10     4.04      4.16 -0.00244
##  9   3.4    3.33    4.10     4.04      4.16 -0.702  
## 10   4.5    3.17    4.09     4.03      4.16  0.409  
## # … with 453 more rows</code></pre>
<p>Note that the uncertainty for <em>particular predictions</em> is higher than the uncertainty for our estimate of the coefficient on <code>bty_avg</code>. We are more confident in the <em>average</em> effect of “beauty” scores on teaching evaluations than we are on any predictions we would make for a <em>particular person</em>.</p>
<!-- AR: We can't really bring Joe back in here, because we are filling in the
potential outcome table with estimates of the treatment effect + Joe's observed
outcome, not with the predicted value for a particular bty_avg.-->
</div>
</div>
<div id="case-study-2018-gubernatorial-forecasts" class="section level2">
<h2>
<span class="header-section-number">10.3</span> Case study: 2018 gubernatorial forecasts</h2>
<!-- AR: beginning of working with list columns -->
<!-- EG: Will practice using augment with lm() to make predictions. Additionally, will add a discussion about Gelman's concern about the assumption of beta being perfectly estimated, but how this is likely an acceptable assumption to make. -->
<p>Now that we know how to run simple linear regressions, let’s run many at once!</p>
<p>We’ll use the <code>governor_state_forecast</code> data from the <strong>fivethirtyeight</strong> package. This dataset has the day-by-day forecasts FiveThirtyEight published in the 2018 gubernatorial races from October 11 to November 6. Which Republican candidate saw his or her probability of winning increase the most during that time? We can get a sense of this by running a series of linear models, with the outcome variable being FiveThirtyEight’s predicted probability of the Republican candidate winning and the explanatory variable being the number of days since the forecast began.</p>
<p>Let’s first load the <code>governor_state_forecast</code> data and get it in the form we’ll need:</p>
<div class="sourceCode" id="cb843"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb843-1"><a href="regression.html#cb843-1"></a><span class="kw">library</span>(fivethirtyeight)</span>
<span id="cb843-2"><a href="regression.html#cb843-2"></a></span>
<span id="cb843-3"><a href="regression.html#cb843-3"></a>gov &lt;-<span class="st"> </span>governor_state_forecast <span class="op">%&gt;%</span></span>
<span id="cb843-4"><a href="regression.html#cb843-4"></a><span class="st">  </span><span class="kw">filter</span>(model <span class="op">==</span><span class="st"> "classic"</span>,</span>
<span id="cb843-5"><a href="regression.html#cb843-5"></a>         party <span class="op">==</span><span class="st"> "R"</span>) <span class="op">%&gt;%</span></span>
<span id="cb843-6"><a href="regression.html#cb843-6"></a><span class="st">  </span><span class="kw">select</span>(forecastdate, state, candidate, party, win_probability) <span class="op">%&gt;%</span></span>
<span id="cb843-7"><a href="regression.html#cb843-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">days =</span> <span class="kw">as.numeric</span>(forecastdate) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(<span class="kw">as.numeric</span>(forecastdate)),</span>
<span id="cb843-8"><a href="regression.html#cb843-8"></a>         <span class="dt">statecand =</span> <span class="kw">paste</span>(candidate, <span class="st">" ("</span>, state, <span class="st">")"</span>, <span class="dt">sep =</span> <span class="st">""</span>))</span>
<span id="cb843-9"><a href="regression.html#cb843-9"></a></span>
<span id="cb843-10"><a href="regression.html#cb843-10"></a><span class="kw">glimpse</span>(gov)</span></code></pre></div>
<pre><code>## Rows: 900
## Columns: 7
## $ forecastdate    &lt;date&gt; 2018-10-11, 2018-10-11, 2018-10-11, 2018-10-11, 2018…
## $ state           &lt;fct&gt; AK, AL, AR, AZ, CA, CO, CT, FL, GA, HI, IA, ID, IL, K…
## $ candidate       &lt;fct&gt; Mike Dunleavy, Kay Ivey, Asa Hutchinson, Doug Ducey, …
## $ party           &lt;fct&gt; R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R,…
## $ win_probability &lt;dbl&gt; 0.7413, 0.9866, 0.9990, 0.9549, 0.0206, 0.0927, 0.147…
## $ days            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ statecand       &lt;chr&gt; "Mike Dunleavy (AK)", "Kay Ivey (AL)", "Asa Hutchinso…</code></pre>
<p>We <code>filter</code>ed to Republicans as well as to <code>model == "classic"</code> because FiveThirtyEight presented three versions of their model and we want to limit our analysis to one. We also <code>mutate</code>d to create a variable <code>days</code> that is the number of days since the forecast began (October 11) and <code>statecand</code> which combines the name of the Republican candidate with the state abbreviation, which will be useful for plotting.</p>
<div id="fitting-multiple-models-using-map" class="section level3">
<h3>
<span class="header-section-number">10.3.1</span> Fitting multiple models using <code>map()</code>
</h3>
<p>Next, we need to use <code>nest()</code> like we did when creating bootstrapped confidence intervals, except this time we are <code>nest</code>ing the data by <code>statecand</code>:</p>
<div class="sourceCode" id="cb845"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb845-1"><a href="regression.html#cb845-1"></a>gov &lt;-<span class="st"> </span>gov <span class="op">%&gt;%</span></span>
<span id="cb845-2"><a href="regression.html#cb845-2"></a><span class="st">  </span><span class="kw">group_by</span>(statecand) <span class="op">%&gt;%</span></span>
<span id="cb845-3"><a href="regression.html#cb845-3"></a><span class="st">  </span><span class="kw">nest</span>()</span></code></pre></div>
<p>Now each observation in our dataset is a candidate and we have a list column <code>data</code> that consists of the rest of the data for each candidate. We can use <code>map_*</code> functions just as in the bootstrapping example to get the coefficient on <code>days</code> for each candidate:</p>
<div class="sourceCode" id="cb846"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb846-1"><a href="regression.html#cb846-1"></a>gov &lt;-<span class="st"> </span>gov <span class="op">%&gt;%</span></span>
<span id="cb846-2"><a href="regression.html#cb846-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mod =</span> <span class="kw">map</span>(data, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(win_probability <span class="op">~</span><span class="st"> </span>days, <span class="dt">data =</span> .)),</span>
<span id="cb846-3"><a href="regression.html#cb846-3"></a>         <span class="dt">reg_results =</span> <span class="kw">map</span>(mod, <span class="op">~</span><span class="st"> </span><span class="kw">tidy</span>(.)),</span>
<span id="cb846-4"><a href="regression.html#cb846-4"></a>         <span class="dt">disp_coef =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> "days"</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(estimate)))</span>
<span id="cb846-5"><a href="regression.html#cb846-5"></a></span>
<span id="cb846-6"><a href="regression.html#cb846-6"></a><span class="kw">glimpse</span>(gov)</span></code></pre></div>
<pre><code>## Rows: 36
## Columns: 5
## Groups: statecand [36]
## $ statecand   &lt;chr&gt; "Mike Dunleavy (AK)", "Kay Ivey (AL)", "Asa Hutchinson (A…
## $ data        &lt;list&gt; [&lt;tbl_df[25 x 6]&gt;, &lt;tbl_df[25 x 6]&gt;, &lt;tbl_df[25 x 6]&gt;, &lt;…
## $ mod         &lt;list&gt; [&lt;0.776306167, -0.005353355, -0.035006167, 0.058047188, …
## $ reg_results &lt;list&gt; [&lt;tbl_df[2 x 5]&gt;, &lt;tbl_df[2 x 5]&gt;, &lt;tbl_df[2 x 5]&gt;, &lt;tbl…
## $ disp_coef   &lt;dbl&gt; -5.353355e-03, -5.252884e-05, -4.464286e-07, 1.647172e-03…</code></pre>
<p>We now have the data to answer the question about which candidate saw his or her estimated probability go up the most. Let’s use the <code>slice()</code> function in the <strong>dplyr</strong> package to answer this. Note that because we have grouped data, we’ll have to <code>ungroup()</code> before we can <code>slice()</code>:</p>
<div class="sourceCode" id="cb848"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb848-1"><a href="regression.html#cb848-1"></a>gov_top5 &lt;-<span class="st"> </span>gov <span class="op">%&gt;%</span></span>
<span id="cb848-2"><a href="regression.html#cb848-2"></a><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></span>
<span id="cb848-3"><a href="regression.html#cb848-3"></a><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(disp_coef)) <span class="op">%&gt;%</span></span>
<span id="cb848-4"><a href="regression.html#cb848-4"></a><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)</span>
<span id="cb848-5"><a href="regression.html#cb848-5"></a></span>
<span id="cb848-6"><a href="regression.html#cb848-6"></a>gov_top5 <span class="op">%&gt;%</span></span>
<span id="cb848-7"><a href="regression.html#cb848-7"></a><span class="st">  </span><span class="kw">select</span>(statecand, disp_coef)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 2
##   statecand            disp_coef
##   &lt;chr&gt;                    &lt;dbl&gt;
## 1 Kim Reynolds (IA)      0.0101 
## 2 Brian Kemp (GA)        0.00291
## 3 Kevin Stitt (OK)       0.00277
## 4 Bob Stefanowski (CT)   0.00277
## 5 Knute Buehler (OR)     0.00263</code></pre>
<p>We can show so much more, however, if we presented the results graphically. To do this, we’ll need to learn the companion function to <code>nest()</code>: <code>unnest()</code>. While <code>nest()</code> collapsed the data so that each observation was a <code>statecand</code>, <code>unnest(data)</code> will return the data to its original unit of analysis, with each row one day’s forecast for a particular candidate:</p>
<div class="sourceCode" id="cb850"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb850-1"><a href="regression.html#cb850-1"></a>gov_top5 <span class="op">%&gt;%</span></span>
<span id="cb850-2"><a href="regression.html#cb850-2"></a><span class="st">  </span><span class="kw">unnest</span>(data) <span class="op">%&gt;%</span></span>
<span id="cb850-3"><a href="regression.html#cb850-3"></a><span class="st">  </span><span class="kw">glimpse</span>()</span></code></pre></div>
<pre><code>## Rows: 125
## Columns: 10
## $ statecand       &lt;chr&gt; "Kim Reynolds (IA)", "Kim Reynolds (IA)", "Kim Reynol…
## $ forecastdate    &lt;date&gt; 2018-10-11, 2018-10-12, 2018-10-13, 2018-10-16, 2018…
## $ state           &lt;fct&gt; IA, IA, IA, IA, IA, IA, IA, IA, IA, IA, IA, IA, IA, I…
## $ candidate       &lt;fct&gt; Kim Reynolds, Kim Reynolds, Kim Reynolds, Kim Reynold…
## $ party           &lt;fct&gt; R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R,…
## $ win_probability &lt;dbl&gt; 0.1663, 0.1629, 0.1661, 0.1526, 0.1498, 0.1540, 0.151…
## $ days            &lt;dbl&gt; 0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…
## $ mod             &lt;list&gt; [&lt;0.07395160, 0.01005817, 0.092348403, 0.078890234, …
## $ reg_results     &lt;list&gt; [&lt;tbl_df[2 x 5]&gt;, &lt;tbl_df[2 x 5]&gt;, &lt;tbl_df[2 x 5]&gt;, …
## $ disp_coef       &lt;dbl&gt; 0.010058169, 0.010058169, 0.010058169, 0.010058169, 0…</code></pre>
<p>Note that <code>unnest</code>ing flattens out the <code>data</code> list column but keeps everything else we’ve created. With the data in this form, it’s easy to use <code>ggplot()</code> to create a scatterplot for each of the five candidates with the greatest <code>disp_coef</code>:</p>
<div class="sourceCode" id="cb852"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb852-1"><a href="regression.html#cb852-1"></a>gov_top5 <span class="op">%&gt;%</span></span>
<span id="cb852-2"><a href="regression.html#cb852-2"></a><span class="st">  </span><span class="kw">unnest</span>(data) <span class="op">%&gt;%</span></span>
<span id="cb852-3"><a href="regression.html#cb852-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> days, <span class="dt">y =</span> win_probability)) <span class="op">+</span></span>
<span id="cb852-4"><a href="regression.html#cb852-4"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb852-5"><a href="regression.html#cb852-5"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">"lm"</span>) <span class="op">+</span></span>
<span id="cb852-6"><a href="regression.html#cb852-6"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>statecand) <span class="op">+</span></span>
<span id="cb852-7"><a href="regression.html#cb852-7"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Days since forecast began"</span>,</span>
<span id="cb852-8"><a href="regression.html#cb852-8"></a>       <span class="dt">y =</span> <span class="st">"FiveThirtyEight's win probability"</span>,</span>
<span id="cb852-9"><a href="regression.html#cb852-9"></a>       <span class="dt">caption =</span> <span class="st">"Data source: FiveThirtyEight"</span>) <span class="op">+</span></span>
<span id="cb852-10"><a href="regression.html#cb852-10"></a><span class="st">  </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula 'y ~ x'</code></pre>
<p><img src="book_temp_files/figure-html/unnamed-chunk-558-1.png" width="672"></p>
<p>We could easily replicate this process if we wanted to find the five candidates who saw their fortunes decline the most:</p>
<div class="sourceCode" id="cb854"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb854-1"><a href="regression.html#cb854-1"></a>gov <span class="op">%&gt;%</span></span>
<span id="cb854-2"><a href="regression.html#cb854-2"></a><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></span>
<span id="cb854-3"><a href="regression.html#cb854-3"></a><span class="st">  </span><span class="kw">arrange</span>(disp_coef) <span class="op">%&gt;%</span></span>
<span id="cb854-4"><a href="regression.html#cb854-4"></a><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) <span class="op">%&gt;%</span></span>
<span id="cb854-5"><a href="regression.html#cb854-5"></a><span class="st">  </span><span class="kw">unnest</span>(data) <span class="op">%&gt;%</span></span>
<span id="cb854-6"><a href="regression.html#cb854-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> days, <span class="dt">y =</span> win_probability)) <span class="op">+</span></span>
<span id="cb854-7"><a href="regression.html#cb854-7"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb854-8"><a href="regression.html#cb854-8"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">"lm"</span>) <span class="op">+</span></span>
<span id="cb854-9"><a href="regression.html#cb854-9"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>statecand) <span class="op">+</span></span>
<span id="cb854-10"><a href="regression.html#cb854-10"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Days since forecast began"</span>,</span>
<span id="cb854-11"><a href="regression.html#cb854-11"></a>       <span class="dt">y =</span> <span class="st">"FiveThirtyEight's win probability"</span>,</span>
<span id="cb854-12"><a href="regression.html#cb854-12"></a>       <span class="dt">caption =</span> <span class="st">"Data source: FiveThirtyEight"</span>) <span class="op">+</span></span>
<span id="cb854-13"><a href="regression.html#cb854-13"></a><span class="st">  </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula 'y ~ x'</code></pre>
<p><img src="book_temp_files/figure-html/unnamed-chunk-559-1.png" width="672"></p>
</div>
</div>
<div id="leastsquares" class="section level2">
<h2>
<span class="header-section-number">10.4</span> Best-fitting line</h2>
<p>Regression lines are also known as “best-fitting” lines. But what do we mean by “best”? Let’s unpack the criteria that is used in regression to determine “best.” Recall the plot where for an instructor with a beauty score of <span class="math inline">\(x = 7.333\)</span> we mark the <em>observed value</em> <span class="math inline">\(y\)</span> with a circle, the <em>fitted value</em> <span class="math inline">\(\widehat{y}\)</span> with a square, and the <em>residual</em> <span class="math inline">\(y - \widehat{y}\)</span> with an arrow. We re-display that plot in the top-left plot of the next figure in addition to three more arbitrarily chosen course instructors:</p>
<pre><code>## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'</code></pre>
<div class="figure">
<span id="fig:unnamed-chunk-560"></span>
<p class="caption marginnote shownote">
FIGURE 10.8: Example of observed value, fitted value, and residual.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-560-1.png" alt="Example of observed value, fitted value, and residual." width="672">
</div>
<p>The three other plots refer to:</p>
<ol style="list-style-type: decimal">
<li>A course whose instructor had a “beauty” score <span class="math inline">\(x\)</span> = 2.333 and teaching score <span class="math inline">\(y\)</span> = 2.7. The residual in this case is <span class="math inline">\(2.7 - 4.036 = -1.336\)</span>, which we mark with a new blue arrow in the top-right plot.</li>
<li>A course whose instructor had a “beauty” score <span class="math inline">\(x = 3.667\)</span> and teaching score <span class="math inline">\(y = 4.4\)</span>. The residual in this case is <span class="math inline">\(4.4 - 4.125 = 0.2753\)</span>, which we mark with a new blue arrow in the bottom-left plot.</li>
<li>A course whose instructor had a “beauty” score <span class="math inline">\(x = 6\)</span> and teaching score <span class="math inline">\(y = 3.8\)</span>. The residual in this case is <span class="math inline">\(3.8 - 4.28 = -0.4802\)</span>, which we mark with a new blue arrow in the bottom-right plot.</li>
</ol>
<p>Now say we repeated this process of computing residuals for all 463 courses’ instructors, then we squared all the residuals, and then we summed them. We call this quantity the <em>sum of squared residuals</em>; it is a measure of the <em>lack of fit</em> of a model. Larger values of the sum of squared residuals indicate a bigger lack of fit. This corresponds to a worse fitting model.</p>
<p>If the regression line fits all the points perfectly, then the sum of squared residuals is 0. This is because if the regression line fits all the points perfectly, then the fitted value <span class="math inline">\(\widehat{y}\)</span> equals the observed value <span class="math inline">\(y\)</span> in all cases, and hence the residual <span class="math inline">\(y-\widehat{y}\)</span> = 0 in all cases, and the sum of even a large number of 0’s is still 0.</p>
<p>Furthermore, of all possible lines we can draw through the cloud of 463 points, the regression line minimizes this value. In other words, the regression and its corresponding fitted values <span class="math inline">\(\widehat{y}\)</span> minimizes the sum of the squared residuals:</p>
<p><span class="math display">\[
\sum_{i=1}^{n}(y_i - \widehat{y}_i)^2
\]</span></p>
<p>Let’s use our data wrangling tools from Chapter <a href="wrangling.html#wrangling">2</a> to compute the sum of squared residuals exactly:</p>
<div class="sourceCode" id="cb857"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb857-1"><a href="regression.html#cb857-1"></a><span class="co"># Fit regression model:</span></span>
<span id="cb857-2"><a href="regression.html#cb857-2"></a></span>
<span id="cb857-3"><a href="regression.html#cb857-3"></a>score_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, </span>
<span id="cb857-4"><a href="regression.html#cb857-4"></a>                  <span class="dt">data =</span> evals_ch11)</span>
<span id="cb857-5"><a href="regression.html#cb857-5"></a></span>
<span id="cb857-6"><a href="regression.html#cb857-6"></a><span class="co"># Get regression points:</span></span>
<span id="cb857-7"><a href="regression.html#cb857-7"></a></span>
<span id="cb857-8"><a href="regression.html#cb857-8"></a>regression_points &lt;-<span class="st"> </span>score_model <span class="op">%&gt;%</span></span>
<span id="cb857-9"><a href="regression.html#cb857-9"></a><span class="st">  </span><span class="kw">augment</span>()</span>
<span id="cb857-10"><a href="regression.html#cb857-10"></a>regression_points</span></code></pre></div>
<pre><code>## # A tibble: 463 x 9
##    score bty_avg .fitted .se.fit   .resid    .hat .sigma      .cooksd .std.resid
##    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;
##  1   4.7    5       4.21  0.0266  0.486   0.00247  0.535 0.00103         0.911  
##  2   4.1    5       4.21  0.0266 -0.114   0.00247  0.535 0.0000560      -0.213  
##  3   3.9    5       4.21  0.0266 -0.314   0.00247  0.535 0.000427       -0.587  
##  4   4.8    5       4.21  0.0266  0.586   0.00247  0.535 0.00149         1.10   
##  5   4.6    3       4.08  0.0339  0.520   0.00403  0.535 0.00192         0.974  
##  6   4.3    3       4.08  0.0339  0.220   0.00403  0.535 0.000343        0.412  
##  7   2.8    3       4.08  0.0339 -1.28    0.00403  0.532 0.0116         -2.40   
##  8   4.1    3.33    4.10  0.0305 -0.00244 0.00325  0.535 0.0000000340   -0.00457
##  9   3.4    3.33    4.10  0.0305 -0.702   0.00325  0.534 0.00282        -1.32   
## 10   4.5    3.17    4.09  0.0321  0.409   0.00361  0.535 0.00106         0.765  
## # … with 453 more rows</code></pre>
<div class="sourceCode" id="cb859"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb859-1"><a href="regression.html#cb859-1"></a><span class="co"># Compute sum of squared residuals</span></span>
<span id="cb859-2"><a href="regression.html#cb859-2"></a></span>
<span id="cb859-3"><a href="regression.html#cb859-3"></a>regression_points <span class="op">%&gt;%</span></span>
<span id="cb859-4"><a href="regression.html#cb859-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">squared_residuals =</span> .resid<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span></span>
<span id="cb859-5"><a href="regression.html#cb859-5"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">sum_of_squared_residuals =</span> <span class="kw">sum</span>(squared_residuals))</span></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   sum_of_squared_residuals
##                      &lt;dbl&gt;
## 1                     132.</code></pre>
<p>Any other straight line drawn in the figure would yield a sum of squared residuals greater than 132. This is a mathematically guaranteed fact that you can prove using calculus and linear algebra. That’s why alternative names for the linear regression line are the <em>best-fitting line</em> and the <em>least-squares line</em>. Why do we square the residuals (i.e., the arrow lengths)? So that both positive and negative deviations of the same amount are treated equally.</p>
<!-- EG: Honestly, I'm not sure that this section on Bayesian regression belongs in this chapter. Jumping so quickly from introducting simple lm() modeling to Bayesian interpretations could be confusing, especially when so many new concepts and functions are being introduced. I personally think that this ought to be its own chapter, with in-depth exploration of examples and plenty of emphasis on how to properly interpret the meaning of EVERY part of a Bayesian model. If we add all that into this chapter, it would get very long. -->
<!-- not sure yet how to best order these sections  -->
</div>
<div id="advanced-bayesian-regression" class="section level2">
<h2>
<span class="header-section-number">10.5</span> Advanced: Bayesian Regression</h2>
</div>
<div id="introduction-to-rstanarm" class="section level2">
<h2>
<span class="header-section-number">10.6</span> Introduction to rstanarm</h2>
<p>In this chapter, you have learned how to use <code>lm()</code> to model the relationship between outcome and explanatory variables using <em>simple linear regression</em>. Furthermore, you learned how to interpret regresssion in a bayesian manner. Recall that instead of bootstrapping, we used <code>lm()</code> and <code>tidy()</code> to produce similar confidence intervals.</p>
<!-- fit model in lm() intepret in bayesian way as refresher  -->
<div class="sourceCode" id="cb861"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb861-1"><a href="regression.html#cb861-1"></a>lm_score_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch11)</span>
<span id="cb861-2"><a href="regression.html#cb861-2"></a></span>
<span id="cb861-3"><a href="regression.html#cb861-3"></a>lm_score_model <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb861-4"><a href="regression.html#cb861-4"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb861-5"><a href="regression.html#cb861-5"></a><span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> "bty_avg"</span>) <span class="op">%&gt;%</span></span>
<span id="cb861-6"><a href="regression.html#cb861-6"></a><span class="st">  </span><span class="kw">select</span>(estimate, conf.low, conf.high)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   estimate conf.low conf.high
##      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1   0.0666   0.0346    0.0987</code></pre>
<p>From what we learned in Chapter 7, we intepreted this confidence intervals as a 95% chance that the true value of the coefficient of <code>bty_avg</code> is between, roughly, 0.03 and 0.1. Thus, we have taken our simple <code>lm()</code> model and interpreted it in a Bayesian way. However, is this a full fledge bayesian analysis? No! For example, <code>lm()</code> does not allow the user to incorporate priors. As mentioned before, <code>lm()</code> is an old function and to perform bayesian analysis we must use a Bayesian analysis tool.</p>
<!-- EG: Do students know what Bayesian regression is? How it differs fundamentally from frequentist modeling? Why we might consider using a Bayesian model instead of a frequentist model? If that hasn't already been introduced somewhere else in the book, I think it should happen here. -->
<p>The tool we need to perform Bayesian Regression is the <code>rstanarm</code> package. <code>rstanarm</code> is an interface to connect with the Stan probabilistic programming language. <!-- EG: should we have some explanation of what the Stan probabilistic programming language is?-->You will see the <code>rstanarm</code> package is created to mirror functions like <code>lm()</code> and it simply requires adding a <code>stan_</code> prefix before common functions like <code>lm()</code>. We will focus on <code>stan_glm()</code> for the remainder of this section.</p>
</div>
<div id="bayesian-regression-with-a-continuous-variable" class="section level2">
<h2>
<span class="header-section-number">10.7</span> Bayesian Regression with a Continuous Variable</h2>
<p>Let us take another look at the the data on the 463 courses at UT Austin, which can be found in the <code>evals</code> data frame included in the <strong>moderndive</strong> package. We will once again model the relationship between teaching scores and “beauty” to help us highlight the similarities and differences between <code>lm()</code> and <code>stan_glm()</code>.</p>
<p>We can obtain the values of the intercept <span class="math inline">\(b_0\)</span> and the slope for <code>btg_avg</code> <span class="math inline">\(b_1\)</span> in two steps:</p>
<ol style="list-style-type: decimal">
<li>“Fit” the Bayesian regression model using <code>stan_glm()</code> function and save it in <code>bayes_score</code>.</li>
<li>Applying the <code>print()</code> function and indicating how many siginificant digits to include.</li>
</ol>
<div class="sourceCode" id="cb863"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb863-1"><a href="regression.html#cb863-1"></a><span class="kw">library</span>(rstanarm)</span>
<span id="cb863-2"><a href="regression.html#cb863-2"></a></span>
<span id="cb863-3"><a href="regression.html#cb863-3"></a>bayes_score &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch11, <span class="dt">refresh =</span> <span class="dv">0</span>)</span>
<span id="cb863-4"><a href="regression.html#cb863-4"></a></span>
<span id="cb863-5"><a href="regression.html#cb863-5"></a><span class="kw">print</span>(bayes_score, <span class="dt">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## stan_glm
##  family:       gaussian [identity]
##  formula:      score ~ bty_avg
##  observations: 463
##  predictors:   2
## ------
##             Median MAD_SD
## (Intercept) 3.88   0.08  
## bty_avg     0.07   0.02  
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 0.54   0.02  
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg</code></pre>
<p>First, we “fit” the bayesian regression model to the <code>data</code> using the <code>stan_glm()</code>  function and saved this as <code>bayes_score</code>. Notice <code>stan_glm()</code> is used just as <code>lm()</code> was: <code>stan_glm(y ~ x, data = data_frame_name)</code> where:</p>
<ul>
<li>
<code>y</code> is the outcome variable, followed by a tilde <code>~</code>.</li>
<li>
<code>x</code> is the explanatory variable.</li>
<li>The model formula is <code>score ~ bty_avg</code>.</li>
<li>
<code>data_frame_name</code> is the name of the data frame that contains the variables <code>y</code> and <code>x</code>. In our case, <code>data_frame_name</code> is the <code>evals_ch11</code> data frame.</li>
<li>
<code>refresh = 0</code> is optional. Setting refresh equal to 0 suppresses the printing of the sampling algorithm from the model. We will touch more on this later.</li>
</ul>
<div id="interpreting-regression-coefficients-1" class="section level3">
<h3>
<span class="header-section-number">10.7.1</span> Interpreting Regression Coefficients</h3>
<p>Notice, in the <code>print()</code> output, the estimates are referred to as the <code>Median</code> and the uncertainty as the <code>MAD_SD</code>. This is due to the fact that Bayesian Regression does not provide just a point estimate, but a distribution. The <code>Median</code> is the chosen summary statistic for the distribution because median-based-summaries are more stable over simulations. However, even though the estimates and uncertainty of the parameters are calculated differently, the coefficients of the <code>stan_glm()</code> model can be intepretted in the same manner as they were previously when using <code>lm()</code>.</p>
<p>The intercept <span class="math inline">\(b_0\)</span> = 3.8804777 is the average teaching score <span class="math inline">\(\widehat{y}\)</span> = <span class="math inline">\(\widehat{\text{score}}\)</span> for those courses where the instructor had a “beauty” score <code>bty_avg</code> of 0. Again, while the  intercept of the regression line has a mathematical interpretation, it has no <em>practical</em> interpretation here, since observing a <code>bty_avg</code> of 0 is impossible.</p>
<!-- DK: Add discussion of "hat". Recall the p hat which we estimated last time. y hat is like that! It is not a variable that we can observe. It is an estimate. (Indeed, it is an estimate of a potential outcome!) This is different form x, which has no hat, because it is real data, something we can see. Side note: not sure if the hat versus no hat distinction works well with b_0/b_1 being things we can't see and need to estimate, but for which we do not use hat notation. -->
<p>The slope <span class="math inline">\(b_1\)</span> = <span class="math inline">\(b_{\text{bty_avg}}\)</span> of 0.0667736. The “bty_avg” subscript indicates that this number summarizes the relationship between the teaching and average beauty score variables. The slope’s interpretation is different:</p>
<!-- DK: Let's make sure that these interpretations are highly consistent with Gelman and across the book. -->
<!-- EG: To make sure these interpretations are consistent, who should I talk to/what sections should I look at? -->
<blockquote>
<p>For every increase of 1 unit in <code>bty_avg</code>, there is an <em>associated</em> increase of, <em>on average</em>, 0.0667736 units of <code>score</code>.</p>
</blockquote>
<p>What the slope of 0.0667736 is saying is that across all possible courses, the <em>average</em> difference in teaching score between two instructors whose “beauty” scores differ by one is 0.0667736 when holding all other things equal.</p>
<p>In the output, we are also given another parameter, <span class="math inline">\(sigma\)</span> = 0.54. <span class="math inline">\(sigma\)</span> represents the residual standard error, the deviation between the predicted value and the observed value. Thus, it shows us the uncertainity when our model predicts the <code>score</code> using <code>bty_avg</code>. Our <span class="math inline">\(sigma\)</span>
tells us that a teacher’s score will be between plus or minus 0.54 the prediction for 68% of the data and between plus or minus two sigma, 1.08, of the prediction for about 95% of the data. Here is a visual representation:</p>
<p><img src="book_temp_files/figure-html/unnamed-chunk-564-1.png" width="672"></p>
<p>Finally, for all of the coefficients, we are provided a <code>MAD_SD</code>. The <code>MAD SD</code>, which equals <span class="math inline">\(1.483 * (\mathbf{median}^n_{i=1} |z_i - M|)\)</span>, summarizes the uncertainty in the model parameters. Since we are used to using the standard deviations to measure variation, the MAD SD is rescaled to mirror the standard error of the normal distribution. The <code>MAD_SD</code> can be used to retrieve our coefficient’s credible intervals for each parameter. Thus, for <code>bty_avg</code>, the 95% credible interval is (0.0337722, 0.0987925). This tells us that there is a 95% probability that the true value of the parameter for <code>bty_avg</code> falls within the interval.</p>
<!-- EG: Should we include an explanation of why the language of posteriors is so important/so heavily used within Bayesian modeling? -->
<p>Instead of trying to calcualate these by hand, the easiest way to retrieve our model’s credible intervals is using the function <code>posterior_interval()</code> on our model object <code>bayes_score</code>.</p>
<div class="sourceCode" id="cb865"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb865-1"><a href="regression.html#cb865-1"></a><span class="kw">posterior_interval</span>(bayes_score)</span></code></pre></div>
<pre><code>##                     5%        95%
## (Intercept) 3.75830835 4.00540453
## bty_avg     0.03895089 0.09343083
## sigma       0.50691061 0.56664030</code></pre>
<p>The default for the function is a 90% interval, but that can be changed by adding the <code>prob</code> input. Thus, we can replicate the 95% intervals that we have worked with in the past:</p>
<div class="sourceCode" id="cb867"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb867-1"><a href="regression.html#cb867-1"></a><span class="kw">posterior_interval</span>(bayes_score, <span class="dt">prob =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##                   2.5%      97.5%
## (Intercept) 3.73160841 4.02765079
## bty_avg     0.03377216 0.09879252
## sigma       0.50248717 0.57308742</code></pre>
<p>Now, let us call <code>summary()</code> on our <code>bayes_score</code> model. Using the <code>summary()</code> is an alternative to <code>print</code> and it provides more information about our <code>stan_glm()</code> model.</p>
<div class="sourceCode" id="cb869"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb869-1"><a href="regression.html#cb869-1"></a><span class="kw">summary</span>(bayes_score)</span></code></pre></div>
<pre><code>## 
## Model Info:
##  function:     stan_glm
##  family:       gaussian [identity]
##  formula:      score ~ bty_avg
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help('prior_summary')
##  observations: 463
##  predictors:   2
## 
## Estimates:
##               mean   sd   10%   50%   90%
## (Intercept) 3.9    0.1  3.8   3.9   4.0  
## bty_avg     0.1    0.0  0.0   0.1   0.1  
## sigma       0.5    0.0  0.5   0.5   0.6  
## 
## Fit Diagnostics:
##            mean   sd   10%   50%   90%
## mean_PPD 4.2    0.0  4.1   4.2   4.2  
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).
## 
## MCMC diagnostics
##               mcse Rhat n_eff
## (Intercept)   0.0  1.0  4012 
## bty_avg       0.0  1.0  3994 
## sigma         0.0  1.0  3897 
## mean_PPD      0.0  1.0  3979 
## log-posterior 0.0  1.0  1906 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).</code></pre>
<p>In the Estimates section, we get the same information as we did from <code>print()</code>. However, there is a lot of new information that we have not seen before under Fit Diagnostics and MCMC Diagnostics. We can use this information provided to further assess the model.</p>
<p>In the Fit Diagnostics section, we are provided information about the <code>mean_ppd</code>, the sample mean of the posterior predictive distribution of the outcome variable. A quick check of our model is that we hope the <code>mean_ppd</code> is close to the mean of our depedent variable. The mean of the <code>score</code> variable is 4.17473. Thus, the <code>mean_ppd</code> is definitely on par with the mean of the <code>bayes_score</code> dependent variable.</p>
<p>In MCMC diagnostics, we are provided with:</p>
<ul>
<li>
<code>log-posterior</code>, which is the log of the combined posterior distributions.</li>
<li>
<code>mcse</code>, which stands for the Monte Carlo standard error. Markov Chain Monte Carlo is the algorithm used to draw from the posterior distribution.</li>
<li>
<code>Rhat</code>, which indicates whether or not the model converges.</li>
<li>
<code>n_eff</code>, a measure of the effective sample size.</li>
</ul>
<p>The only thing that you need to be concerned about is the <code>Rhat</code>. When we know whether or not the model converges, we know whether or not the results are reliable. We hope to get values for <code>Rhat</code> as close to 1 as possible. Rhat values less than 1.1 indicate model convergence and that the model is reliable. Thus, since all the <code>Rhat</code> for our model are 1.0, <code>bayes_score</code> is reliable.</p>
</div>
<div id="uncertainty-in-bayesian-inference" class="section level3">
<h3>
<span class="header-section-number">10.7.2</span> Uncertainty in Bayesian Inference</h3>
<p>You may be wondering, what exactly is <code>stan_glm()</code> doing? Let us take a look under the hood of <code>stan_glm()</code>. This will enable us to further understand the usefulness of Bayesian Regression. Remember that we included <code>refresh = 0</code> in the <code>bayes_score</code> model. Let us run the same model, but leave out the <code>refresh</code> input.</p>
<div class="sourceCode" id="cb871"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb871-1"><a href="regression.html#cb871-1"></a><span class="kw">stan_glm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch11)</span></code></pre></div>
<pre><code>## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.3e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.0557 seconds (Warm-up)
## Chain 1:                0.095667 seconds (Sampling)
## Chain 1:                0.151367 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.8e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.044944 seconds (Warm-up)
## Chain 2:                0.108341 seconds (Sampling)
## Chain 2:                0.153285 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 2.3e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.072573 seconds (Warm-up)
## Chain 3:                0.163461 seconds (Sampling)
## Chain 3:                0.236034 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4.4e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.061016 seconds (Warm-up)
## Chain 4:                0.09187 seconds (Sampling)
## Chain 4:                0.152886 seconds (Total)
## Chain 4:</code></pre>
<pre><code>## stan_glm
##  family:       gaussian [identity]
##  formula:      score ~ bty_avg
##  observations: 463
##  predictors:   2
## ------
##             Median MAD_SD
## (Intercept) 3.9    0.1   
## bty_avg     0.1    0.0   
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 0.5    0.0   
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg</code></pre>
<p>Without <code>refresh</code>, the output displays everything <code>stan_glm()</code> is doing in the background, which is a sampling algorithm known as Markov Chain Monte Carlo (MCMC). In fact, <code>stan_glm()</code> is producing thousands of simulations after sampling from the posterior distribution using this MCMC algorithm. We can conveniently access all of the simulations of the the model parameters (intercept, slope, and sigma) from the posterior distribution as a matrix. Let’s take a look at some of these simulations.</p>
<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb874-1"><a href="regression.html#cb874-1"></a>sims &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(bayes_score)</span>
<span id="cb874-2"><a href="regression.html#cb874-2"></a></span>
<span id="cb874-3"><a href="regression.html#cb874-3"></a></span>
<span id="cb874-4"><a href="regression.html#cb874-4"></a><span class="kw">head</span>(sims)</span></code></pre></div>
<pre><code>##           parameters
## iterations (Intercept)    bty_avg     sigma
##       [1,]    3.872773 0.06436317 0.5522673
##       [2,]    3.835042 0.07806102 0.4997041
##       [3,]    3.947511 0.05505194 0.5368178
##       [4,]    3.906328 0.06023618 0.5074376
##       [5,]    3.856744 0.07283693 0.5596473
##       [6,]    3.899193 0.06312141 0.5480158</code></pre>
<div class="sourceCode" id="cb876"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb876-1"><a href="regression.html#cb876-1"></a><span class="kw">nrow</span>(sims)</span></code></pre></div>
<pre><code>## [1] 4000</code></pre>
<p>As you can see, each row or iteration has a slightly different value for our <code>bayes_score</code> model’s three parameters. Each combination of possible values of the parameters was used to try to fit the data. Also, looking at the dimensions of the matrix, there are 4000 rows. Thus, <code>stan_glm()</code> tried <code>4000</code> different combinations of possible values of the parameters to try to model the data.</p>
<p>Now, you may be wondering what is going on because there are <code>4000</code> simulations, but when we print the model, we get a single value for each parameter. Recall that what <code>lm()</code> refers to as the estimate, when we printed our <code>bayes_score</code> model, the column was headed as <code>Median</code>. Let’s calculate the median value for each column of our <code>sims</code> matrix. We can do this using the <code>apply()</code> function, which takes form: <code>apply(Object, Margin, Function)</code>. We will be applying the <code>median</code> function to the <code>sims</code> matrix and the <code>Margin</code> will be 2, which indicates columns instead of rows.</p>
<div class="sourceCode" id="cb878"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb878-1"><a href="regression.html#cb878-1"></a><span class="kw">apply</span>(sims, <span class="dv">2</span>, median)</span></code></pre></div>
<pre><code>## (Intercept)     bty_avg       sigma 
##  3.88047767  0.06677362  0.53605775</code></pre>
<p>These are the exact values of the coefficients of the printed <code>bayes_score</code> model. Thus, <code>stan_glm()</code> tried 4000 iterations to model the data. All <code>4000</code> were summarized using the <code>Median</code> to produce point estimates of the parameters.</p>
<p>Now, we understand where the point-estimate comes from when we call <code>print()</code> on a <code>stan_glm()</code> model; however, a simple linear regression using <code>lm()</code> provides us with a point-estimate, or in other words, a single line of best fit. The power of Bayesian Regression comes from the ability to analyze uncertainty using the entire posterior distribution, all 4000 simulations cumulatively. Again, the posterior distribution is a set of plausible values for each parameter and each observation or row is referred to as a posterior draw. Let us take a look at the posterior distrubtion for the <code>bty_avg</code> variable, the slope, by graphing it. Once again we can use the <code>sims</code> matrix, but we must convert it into a tibble using <code>as_tibble()</code> to graph.</p>
<div class="sourceCode" id="cb880"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb880-1"><a href="regression.html#cb880-1"></a>posterior_draws&lt;-sims<span class="op">%&gt;%</span><span class="kw">as_tibble</span>()</span>
<span id="cb880-2"><a href="regression.html#cb880-2"></a></span>
<span id="cb880-3"><a href="regression.html#cb880-3"></a></span>
<span id="cb880-4"><a href="regression.html#cb880-4"></a>posterior_draws<span class="op">%&gt;%</span></span>
<span id="cb880-5"><a href="regression.html#cb880-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg)) <span class="op">+</span></span>
<span id="cb880-6"><a href="regression.html#cb880-6"></a><span class="st">  </span><span class="kw">geom_histogram</span>()<span class="op">+</span></span>
<span id="cb880-7"><a href="regression.html#cb880-7"></a><span class="st">   </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span><span class="kw">median</span>(posterior_draws<span class="op">$</span>bty_avg), <span class="dt">color=</span><span class="st">"red"</span>, <span class="dt">size=</span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Frequency"</span>, <span class="dt">title =</span> <span class="st">"Posterior Distribution of bty_avg Parameter"</span>, <span class="dt">subtitle =</span> <span class="st">"The Red Line Represents the Median"</span>)</span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="book_temp_files/figure-html/unnamed-chunk-573-1.png" width="672"></p>
<p>If you noticed, <code>stan_glm()</code> does not have any p-values, t-values or degrees of freedom like <code>lm()</code>. The crux of Bayesian Modeling is that everything we need to know can be found within the posterior distribution. We can get a point estimate using the <code>Median</code> and we saw in the previous section that we can get a 95% credible interval for <code>bty_avg</code> using the <code>posterior_interval</code> function. We can also graph the posterior distribution of all the other parameters from the model, the intercept and sigma.</p>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="book_temp_files/figure-html/unnamed-chunk-574-1.png" width="672"></p>
<p>Another way to think of each posterior draw, each row of the <code>sims</code> matrix, is that each creates a regression line. This is something that may get lost in translation with the posterior distrubution and the 4000 simulations; however, the goal of <code>stan_glm()</code> is still to fit lines to data.</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb883-1"><a href="regression.html#cb883-1"></a>medians&lt;-<span class="kw">tibble</span>(<span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span> =<span class="st"> </span><span class="kw">median</span>(posterior_draws<span class="op">$</span><span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span>), <span class="dt">bty_avg =</span> <span class="kw">median</span>(posterior_draws<span class="op">$</span>bty_avg))</span>
<span id="cb883-2"><a href="regression.html#cb883-2"></a></span>
<span id="cb883-3"><a href="regression.html#cb883-3"></a>posterior_draws<span class="op">%&gt;%</span></span>
<span id="cb883-4"><a href="regression.html#cb883-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span>, <span class="dt">y =</span> bty_avg)) <span class="op">+</span></span>
<span id="cb883-5"><a href="regression.html#cb883-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">data =</span> medians, <span class="dt">color =</span> <span class="st">"red"</span>, <span class="dt">size =</span> <span class="dv">5</span>)<span class="op">+</span><span class="st"> </span></span>
<span id="cb883-6"><a href="regression.html#cb883-6"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Every Combination of Intercept and bty_avg From Posterior Distribution"</span>, <span class="dt">x =</span> <span class="st">"Intercept"</span>, <span class="dt">subtitle =</span> <span class="st">"Red Dot Represents the Median for Each Parameter"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-575-1.png" width="672">
Each intercept and slope (bty_avg) combination will create a different line. Furthermore, we can view all of these various lines. In doing so, we can again express and understand uncertainty.</p>
<p>We will take our first look at the <strong>tidybayes</strong> package. The purpose of the <strong>tidybayes</strong> package is to aid in formating Bayesian modeling outputs in a tidy manner. It also provides <strong>ggplot</strong> geoms to easily plot Bayesian Models.</p>
<p>Now back to the matter at hand of the various posterior draws and the lines they create. If you recall the <code>augment()</code> function we used for <code>lm()</code>, we were able to retrieve the fitted values yˆ. The <code>add_fitted_draws()</code> function from the <strong>tidybayes</strong> package is very similar, but for <code>stan_glm()</code>. The <code>add_fitted_draws()</code> takes a <code>stan_glm()</code> object, such as <code>bayes_score</code>, and a <code>n</code> parameter, which stands for the number of posterior draws to calculate the fitted values for. Let us try <code>n=5</code>.</p>
<div class="sourceCode" id="cb884"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb884-1"><a href="regression.html#cb884-1"></a>five_draws&lt;-evals_ch11<span class="op">%&gt;%</span></span>
<span id="cb884-2"><a href="regression.html#cb884-2"></a><span class="st">  </span><span class="kw">add_fitted_draws</span>(bayes_score, <span class="dt">n =</span> <span class="dv">5</span>)</span>
<span id="cb884-3"><a href="regression.html#cb884-3"></a></span>
<span id="cb884-4"><a href="regression.html#cb884-4"></a><span class="kw">nrow</span>(five_draws)</span></code></pre></div>
<pre><code>## [1] 2315</code></pre>
<p>If you recall, the <code>evals_ch11</code> dataset has 463 rows. <code>2315/5 = 463</code>. Thus, we can see that <code>add_fitted_draws()</code> calculated the fitted values 5 times. Let us see what <code>five_draws</code> looks like.</p>
<div class="sourceCode" id="cb886"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb886-1"><a href="regression.html#cb886-1"></a><span class="kw">head</span>(five_draws, <span class="dt">n =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## # A tibble: 10 x 9
## # Groups:   ID, score, bty_avg, age, .row [2]
##       ID score bty_avg   age  .row .chain .iteration .draw .value
##    &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
##  1     1   4.7       5    36     1     NA         NA  1130   4.21
##  2     1   4.7       5    36     1     NA         NA  1761   4.11
##  3     1   4.7       5    36     1     NA         NA  2817   4.20
##  4     1   4.7       5    36     1     NA         NA  3190   4.18
##  5     1   4.7       5    36     1     NA         NA  3603   4.20
##  6     2   4.1       5    36     2     NA         NA  1130   4.21
##  7     2   4.1       5    36     2     NA         NA  1761   4.11
##  8     2   4.1       5    36     2     NA         NA  2817   4.20
##  9     2   4.1       5    36     2     NA         NA  3190   4.18
## 10     2   4.1       5    36     2     NA         NA  3603   4.20</code></pre>
<p><code>add_fitted_draws</code>reports:</p>
<ul>
<li>
<code>ID</code> and <code>.row</code> which correspond to the row or specific teacher from the <code>evals_ch11</code> dataset.</li>
<li>
<code>score</code>, <code>bty_avg</code> and <code>age</code> are the observed values for a given teacher.</li>
<li>
<code>.draw</code> is the randomly selected posterior draw used to fit the data. The number corresponds to the row from the <code>sims</code> matrix used.</li>
<li>
<code>.value</code> is the fitted value. In this case, it represents the prediction for the <code>score</code> based on a teacher’s <code>bty_avg</code> for the specific parameters from the <code>.draw</code>.</li>
</ul>
<p>We are able to plot these different draws.</p>
<div class="sourceCode" id="cb888"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb888-1"><a href="regression.html#cb888-1"></a>five_draws<span class="op">%&gt;%</span></span>
<span id="cb888-2"><a href="regression.html#cb888-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span></span>
<span id="cb888-3"><a href="regression.html#cb888-3"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .value, <span class="dt">group =</span> .draw), <span class="dt">color =</span> <span class="st">"blue"</span>) <span class="op">+</span></span>
<span id="cb888-4"><a href="regression.html#cb888-4"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-578-1.png" width="672"></p>
<p>Now, you can see how each posterior draw creates a different line. With just these 5 draws, there is a lot of variability. Let us see what <code>n=100</code> looks like.</p>
<div class="sourceCode" id="cb889"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb889-1"><a href="regression.html#cb889-1"></a>evals_ch11<span class="op">%&gt;%</span></span>
<span id="cb889-2"><a href="regression.html#cb889-2"></a><span class="st">  </span><span class="kw">add_fitted_draws</span>(bayes_score, <span class="dt">n =</span> <span class="dv">100</span>)<span class="op">%&gt;%</span></span>
<span id="cb889-3"><a href="regression.html#cb889-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span></span>
<span id="cb889-4"><a href="regression.html#cb889-4"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .value, <span class="dt">group =</span> .draw), <span class="dt">alpha =</span> <span class="fl">0.6</span>, <span class="dt">color =</span> <span class="st">"blue"</span>) <span class="op">+</span></span>
<span id="cb889-5"><a href="regression.html#cb889-5"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-579-1.png" width="672"></p>
<div class="sourceCode" id="cb890"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb890-1"><a href="regression.html#cb890-1"></a>  <span class="co">#transition_states(.draw, 0, 1)+</span></span>
<span id="cb890-2"><a href="regression.html#cb890-2"></a>  <span class="co">#shadow_mark(past = TRUE, future = TRUE, alpha = 1/20, color = "gray50")</span></span></code></pre></div>
<p>We see a the lines become more concentrated. The power of Bayesian Modeling comes from the shear number of draws. Now remember, <code>stan_glm()</code> by default produces 4000 simulations and we have only plotted 100 of them. The more draws there are, the better your estimation of the posterior distribution and the better we are able to assess the uncertainty in our parameters.</p>
<p>###Predictions</p>
<p>Finally, the last step of Bayesian Inference is being able to make prediction about new data using our <code>stan_glm()</code> model. Let us create a new dataset of teacher’s beauty scores that mirrors the range of values in the <code>eval_ch11</code> dataset. To do this, we will create a tibble of <code>bty_avg</code> and use <code>seq</code> to generate a row for each <code>bty_avg</code> from 1.5 to 8.5 by 0.5.</p>
<div class="sourceCode" id="cb891"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb891-1"><a href="regression.html#cb891-1"></a>new &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">bty_avg =</span> <span class="kw">seq</span>(<span class="fl">1.5</span>, <span class="fl">8.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb891-2"><a href="regression.html#cb891-2"></a></span>
<span id="cb891-3"><a href="regression.html#cb891-3"></a>new</span></code></pre></div>
<pre><code>## # A tibble: 15 x 1
##    bty_avg
##      &lt;dbl&gt;
##  1     1.5
##  2     2  
##  3     2.5
##  4     3  
##  5     3.5
##  6     4  
##  7     4.5
##  8     5  
##  9     5.5
## 10     6  
## 11     6.5
## 12     7  
## 13     7.5
## 14     8  
## 15     8.5</code></pre>
<p>Now, we will start with the simplest prediction, the point prediction. These are based on the fitted model or as we have come to know, the median value for each parameter (intercept, bty_avg, sigma). We will use the <code>predict</code> function to find predicted scores for each beauty rating from our <code>new</code> dataset.</p>
<div class="sourceCode" id="cb893"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb893-1"><a href="regression.html#cb893-1"></a>new<span class="op">%&gt;%</span></span>
<span id="cb893-2"><a href="regression.html#cb893-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(bayes_score, <span class="dt">newdata =</span> .))<span class="op">%&gt;%</span></span>
<span id="cb893-3"><a href="regression.html#cb893-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> y_hat)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-581-1.png" width="672"></p>
<p>Great! We have predicted values for each <code>bty_avg</code> from the <code>new</code> dataset and they follow line created by the fitted model. However, this is nothing special to <code>stan_glm()</code>. We can do the exact same thing with an <code>lm()</code> model. Recall the <code>score_model</code> we created at the beginning of Chapter 10. It is the same as the <code>bayes_score</code> model, but it uses <code>lm()</code> instead of <code>stan_glm()</code>. Here are the point predictions for the <code>score_model</code> at each value of <code>bty_avg</code>in the <code>new</code> dataset.</p>
<div class="sourceCode" id="cb894"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb894-1"><a href="regression.html#cb894-1"></a>new<span class="op">%&gt;%</span></span>
<span id="cb894-2"><a href="regression.html#cb894-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(score_model, <span class="dt">newdata =</span> .))<span class="op">%&gt;%</span></span>
<span id="cb894-3"><a href="regression.html#cb894-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> y_hat)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-582-1.png" width="672"></p>
<p>We get the exact same thing. This is because a point prediction ignores uncertainty. As we have been exploring in this section, where Bayesian Regression differs from Simple Linear Regression is in it’s ability to express uncertainty in not only the model, but also it’s parameters. Remember we had a <code>sims</code> matrix of 4000 rows of parameters. The posterior distributions serves us better and gives us a lot more information than a single point estimate ever could.</p>
<p>Thus, our predicitions should also take uncertainty into account. The first type is linear predictors. We perform this type of prediction using the <code>posterior_linepred</code> function. <code>posterior_linepred</code> takes a stan model like our <code>bayes_score</code> and a dataset of new points, <code>new</code>. With the function, we will be able to represent the distribution of uncertainty in regards to the parameters at each <code>bty_avg</code> from the <code>new</code> dataset. Let us take a look.</p>
<div class="sourceCode" id="cb895"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb895-1"><a href="regression.html#cb895-1"></a>linepred&lt;-<span class="kw">posterior_linpred</span>(bayes_score, <span class="dt">newdata =</span> new)</span>
<span id="cb895-2"><a href="regression.html#cb895-2"></a></span>
<span id="cb895-3"><a href="regression.html#cb895-3"></a></span>
<span id="cb895-4"><a href="regression.html#cb895-4"></a></span>
<span id="cb895-5"><a href="regression.html#cb895-5"></a><span class="kw">head</span>(linepred)</span></code></pre></div>
<pre><code>##           
## iterations        1        2        3        4        5        6        7
##       [1,] 3.969318 4.001500 4.033681 4.065863 4.098044 4.130226 4.162408
##       [2,] 3.952134 3.991164 4.030195 4.069225 4.108256 4.147286 4.186317
##       [3,] 4.030089 4.057615 4.085141 4.112667 4.140193 4.167719 4.195245
##       [4,] 3.996683 4.026801 4.056919 4.087037 4.117155 4.147273 4.177391
##       [5,] 3.965999 4.002417 4.038836 4.075254 4.111673 4.148091 4.184510
##       [6,] 3.993875 4.025435 4.056996 4.088557 4.120118 4.151678 4.183239
##           
## iterations        8        9       10       11       12       13       14
##       [1,] 4.194589 4.226771 4.258952 4.291134 4.323315 4.355497 4.387679
##       [2,] 4.225347 4.264378 4.303408 4.342439 4.381469 4.420500 4.459530
##       [3,] 4.222770 4.250296 4.277822 4.305348 4.332874 4.360400 4.387926
##       [4,] 4.207509 4.237627 4.267745 4.297863 4.327982 4.358100 4.388218
##       [5,] 4.220928 4.257347 4.293765 4.330184 4.366602 4.403020 4.439439
##       [6,] 4.214800 4.246360 4.277921 4.309482 4.341042 4.372603 4.404164
##           
## iterations       15
##       [1,] 4.419860
##       [2,] 4.498561
##       [3,] 4.415452
##       [4,] 4.418336
##       [5,] 4.475857
##       [6,] 4.435725</code></pre>
<p><code>posterior_linepred</code> returns a matrix of posterior simulations wher each column of the <code>linepred</code> corresponds to each row of the <code>new</code> dataset. If we were to take the <code>mean</code> of each column, we would get the point prediction corresponding to that <code>bty_avg</code>. Also, if we were to take the <code>sd</code>, you would get the uncertainty in the fitted model. Thus <code>posterior_linepred</code> mimics what we were doing the previous section with the matrix of simulated parameters. In fact, we plot <code>posterior_linepred</code> using the same <code>add_fitted_draws</code> function without specifying n to factor in all 4000 posterior draws. <code>added_fitted_draws</code> from the <strong>tidybayes</strong> package puts the data in a much easier format than the matrix <code>posterior_linepred</code> outputs. First, we will recreate the point prediction plot using <code>add_fitted_draws</code> and use <code>mutate</code> to get the mean prediction for each <code>bty_avg</code>.</p>
<div class="sourceCode" id="cb897"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb897-1"><a href="regression.html#cb897-1"></a>new <span class="op">%&gt;%</span></span>
<span id="cb897-2"><a href="regression.html#cb897-2"></a><span class="st">  </span><span class="kw">add_fitted_draws</span>(<span class="dt">model =</span> bayes_score, .)<span class="op">%&gt;%</span></span>
<span id="cb897-3"><a href="regression.html#cb897-3"></a><span class="st">  </span><span class="kw">group_by</span>(bty_avg)<span class="op">%&gt;%</span></span>
<span id="cb897-4"><a href="regression.html#cb897-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">point_estimate =</span> <span class="kw">mean</span>(.value))<span class="op">%&gt;%</span></span>
<span id="cb897-5"><a href="regression.html#cb897-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> point_estimate)) <span class="op">+</span></span>
<span id="cb897-6"><a href="regression.html#cb897-6"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb897-7"><a href="regression.html#cb897-7"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb897-8"><a href="regression.html#cb897-8"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="fl">1.5</span>,<span class="fl">8.5</span>,<span class="fl">0.5</span>)) <span class="op">+</span></span>
<span id="cb897-9"><a href="regression.html#cb897-9"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Predicted Scores"</span>, <span class="dt">color =</span> <span class="st">"Uncertainty Level"</span>, <span class="dt">title =</span> <span class="st">"Posterior Linear Prediction Without Uncertainty"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-584-1.png" width="672"></p>
<p>Now, we will add to this plot using the <code>stat_interval</code> geom from the <strong>tidybayes</strong> package to add uncertainty to the visualization.</p>
<div class="sourceCode" id="cb898"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb898-1"><a href="regression.html#cb898-1"></a>new <span class="op">%&gt;%</span></span>
<span id="cb898-2"><a href="regression.html#cb898-2"></a><span class="st">  </span><span class="kw">add_fitted_draws</span>(<span class="dt">model =</span> bayes_score, .)<span class="op">%&gt;%</span></span>
<span id="cb898-3"><a href="regression.html#cb898-3"></a><span class="st">  </span><span class="kw">group_by</span>(bty_avg)<span class="op">%&gt;%</span></span>
<span id="cb898-4"><a href="regression.html#cb898-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">point_estimate =</span> <span class="kw">mean</span>(.value))<span class="op">%&gt;%</span></span>
<span id="cb898-5"><a href="regression.html#cb898-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> point_estimate)) <span class="op">+</span></span>
<span id="cb898-6"><a href="regression.html#cb898-6"></a><span class="st">  </span><span class="kw">stat_interval</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> .value), <span class="dt">.width =</span> <span class="kw">c</span>(<span class="fl">0.68</span>, <span class="fl">0.95</span>)) <span class="op">+</span></span>
<span id="cb898-7"><a href="regression.html#cb898-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> point_estimate)) <span class="op">+</span></span>
<span id="cb898-8"><a href="regression.html#cb898-8"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb898-9"><a href="regression.html#cb898-9"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="fl">1.5</span>,<span class="fl">8.5</span>,<span class="fl">0.5</span>))<span class="op">+</span></span>
<span id="cb898-10"><a href="regression.html#cb898-10"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Predicted Scores"</span>, <span class="dt">color =</span> <span class="st">"Uncertainty Level"</span>, <span class="dt">title =</span> <span class="st">"Posterior Linear Prediction With Uncertainty"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-585-1.png" width="672"></p>
<p>Linear Predictions can provide us the point estimate predictions while also telling us about the uncertainty in the parameters. As you can see, the intervals get a lot smaller towards the center of the <code>bty_avg</code> variable. This is because more data is concentrated towards the center. Furthermore, the predictions are more accurate/less varied because the posterior distribution has more data to draw from. Towards the extremes, we see the bands are a lot wider. Thus, the point predictions are a lot less reliable because the posterior distribution does not have a lot of data from the original <code>evals_ch11</code> to be built off of.</p>
<p>Finally, the last type of prediction we will cover is the predictive distribution. Whereas linear predictions focused on the uncertainty in the parameters coefficients, the predictive distribution represents uncertainty surrounding the predicted value. In other words, it “is the distribution of the outcome implied by the model after using the observed data to update our beliefs about the unknown parameters in the model.” To perform this, we will use the <code>posterior_predict()</code> function. It works in the same way as <code>posterior_linpred</code>.</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb899-1"><a href="regression.html#cb899-1"></a>pred&lt;-<span class="kw">posterior_predict</span>(bayes_score, <span class="dt">newdata =</span> new)</span>
<span id="cb899-2"><a href="regression.html#cb899-2"></a>  </span>
<span id="cb899-3"><a href="regression.html#cb899-3"></a><span class="kw">head</span>(pred)</span></code></pre></div>
<pre><code>##             1        2        3        4        5        6        7        8
## [1,] 3.737977 3.556061 3.681638 4.902959 4.102298 3.835430 2.700879 3.556280
## [2,] 4.869699 3.695933 4.135323 4.019350 4.751269 4.244312 4.641123 4.256357
## [3,] 3.813344 4.781656 3.648571 3.637705 3.924820 4.596784 3.690193 3.473641
## [4,] 4.307587 2.985194 3.997443 4.940027 4.396299 3.917390 4.627289 4.671745
## [5,] 4.485315 4.321578 3.668091 4.147269 2.996147 4.490390 4.846482 4.682812
## [6,] 5.022836 4.077453 4.474859 4.728042 3.548191 4.699912 3.933108 4.088809
##             9       10       11       12       13       14       15
## [1,] 4.550800 4.460460 3.186260 4.785808 4.584108 4.306250 5.354900
## [2,] 5.028321 3.626568 5.037153 4.755907 4.884801 4.947407 4.163441
## [3,] 4.354133 5.856471 4.307629 3.344771 3.940207 4.771565 3.529300
## [4,] 4.260463 3.839809 4.496967 3.254668 4.714164 3.789174 4.291987
## [5,] 4.588403 4.382113 4.519442 3.306825 3.659851 3.359948 4.047528
## [6,] 3.442891 4.342314 4.008765 3.502910 3.184268 4.829654 4.647246</code></pre>
<p>Again, in the matrix that is returned, each column corresponds to one new value of <code>bty_avg</code> from the <code>new</code> dataset. We can take a quick look at the predictive distribution for one of the <code>bty_avg</code> using the <code>hist()</code> function.</p>
<div class="sourceCode" id="cb901"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb901-1"><a href="regression.html#cb901-1"></a><span class="kw">hist</span>(pred[,<span class="dv">1</span>])</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-587-1.png" width="672"></p>
<p>However, <strong>tidybayes</strong> also has a function to make our lives easier for graphing the predictive distributions where we can compare all the distributions cumulatively in one plot. First, we have the <code>add_predicted_draws()</code> function which does the same thing as <code>posterior_predict()</code>, but <code>add_predicted_draws()</code> puts the data in a tidy format.</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb902-1"><a href="regression.html#cb902-1"></a>new <span class="op">%&gt;%</span></span>
<span id="cb902-2"><a href="regression.html#cb902-2"></a><span class="st">  </span><span class="kw">add_predicted_draws</span>(., <span class="dt">model =</span> bayes_score)<span class="op">%&gt;%</span></span>
<span id="cb902-3"><a href="regression.html#cb902-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> .prediction, <span class="dt">y =</span> bty_avg)) <span class="op">+</span></span>
<span id="cb902-4"><a href="regression.html#cb902-4"></a><span class="st">  </span><span class="kw">stat_halfeyeh</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb902-5"><a href="regression.html#cb902-5"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="fl">1.5</span>,<span class="fl">8.5</span>,<span class="fl">0.5</span>)) <span class="op">+</span></span>
<span id="cb902-6"><a href="regression.html#cb902-6"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Predicted Score"</span>, <span class="dt">title =</span> <span class="st">"Predictive Distribution for Score at Each Value of bty_avg"</span>)</span></code></pre></div>
<pre><code>## Warning: 'stat_halfeyeh' is deprecated.
## Use 'stat_halfeye' instead.
## See help("Deprecated") and help("tidybayes-deprecated").</code></pre>
<p><img src="book_temp_files/figure-html/unnamed-chunk-588-1.png" width="672"></p>
<p>With <code>stat_halfh()</code> we are able to view density plots for all the predictions from the <code>new</code> dataset together. Thus, we can compare all the predictive distributions. As you can see, there is a lot of overlap. This signifies there is a lot of uncertainty. Even though the <code>bty_avg</code> has a slightly postive slope, <code>bty_avg</code> does not seem to be very predictive when it comes to the teacher’s <code>score</code>.</p>
</div>
</div>
<div id="bayesian-regression-with-categorical-variable" class="section level2">
<h2>
<span class="header-section-number">10.8</span> Bayesian Regression with Categorical Variable</h2>
<p>In the previous section we focused on Bayesian Regression with one continuous variable. We will now take a look at a model wtih one categorical variable. We will follow many of the same steps as we did in the previous section, thus we will go through the steps of Bayesian Inference quicker in the section while highlighting the differences when regressing on a categorical variable. We will use the same data from the <code>gaminder</code> package on the 142 countries from 2007.</p>
<div class="sourceCode" id="cb904"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb904-1"><a href="regression.html#cb904-1"></a><span class="kw">library</span>(gapminder)</span>
<span id="cb904-2"><a href="regression.html#cb904-2"></a></span>
<span id="cb904-3"><a href="regression.html#cb904-3"></a>gapminder2007 &lt;-<span class="st"> </span>gapminder <span class="op">%&gt;%</span></span>
<span id="cb904-4"><a href="regression.html#cb904-4"></a><span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2007</span>) <span class="op">%&gt;%</span></span>
<span id="cb904-5"><a href="regression.html#cb904-5"></a><span class="st">  </span><span class="kw">select</span>(country, lifeExp, continent, gdpPercap)</span>
<span id="cb904-6"><a href="regression.html#cb904-6"></a></span>
<span id="cb904-7"><a href="regression.html#cb904-7"></a><span class="kw">head</span>(gapminder2007)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##   country     lifeExp continent gdpPercap
##   &lt;fct&gt;         &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;
## 1 Afghanistan    43.8 Asia           975.
## 2 Albania        76.4 Europe        5937.
## 3 Algeria        72.3 Africa        6223.
## 4 Angola         42.7 Africa        4797.
## 5 Argentina      75.3 Americas     12779.
## 6 Australia      81.2 Oceania      34435.</code></pre>
<p>As we did before, we will study the relationship between continents and life expectancy. We will “fit” the bayesian regression using the <code>stan_glm(y ~ x, data)</code> function and save it in <code>bayes_lifeExp</code>.</p>
<div class="sourceCode" id="cb906"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb906-1"><a href="regression.html#cb906-1"></a>bayes_lifeExp&lt;-<span class="kw">stan_glm</span>(lifeExp <span class="op">~</span><span class="st"> </span>continent, <span class="dt">data =</span> gapminder2007, <span class="dt">refresh =</span> <span class="dv">0</span>)</span>
<span id="cb906-2"><a href="regression.html#cb906-2"></a></span>
<span id="cb906-3"><a href="regression.html#cb906-3"></a><span class="kw">print</span>(bayes_lifeExp, <span class="dt">digits =</span> <span class="dv">2</span>, <span class="dt">detail =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>##                   Median MAD_SD
## (Intercept)       54.91   1.05 
## continentAmericas 18.63   1.77 
## continentAsia     15.78   1.68 
## continentEurope   22.77   1.70 
## continentOceania  25.00   5.36 
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 7.43   0.45</code></pre>
<div id="interpreting-coefficients" class="section level4">
<h4>
<span class="header-section-number">10.8.0.1</span> Interpreting Coefficients</h4>
<p>Remember, now that we are using a categorical explanatory variable <code>continent</code>, our model will not yield a “best-fitting” regression line, but rather offsets relative to a baseline for comparison. Remeber, also that each of these coefficients is the median of the posterior distribution for each parameter.</p>
<p>For example, we can take a look at the posterior distribution for the <code>continentAmericas</code> parameter. Calling <code>as.matrix</code> on the model, we can access all the posterior draws to then plot in a histogram:</p>
<div class="sourceCode" id="cb908"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb908-1"><a href="regression.html#cb908-1"></a>sims &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(bayes_lifeExp)</span>
<span id="cb908-2"><a href="regression.html#cb908-2"></a></span>
<span id="cb908-3"><a href="regression.html#cb908-3"></a></span>
<span id="cb908-4"><a href="regression.html#cb908-4"></a></span>
<span id="cb908-5"><a href="regression.html#cb908-5"></a>posterior_draws&lt;-sims<span class="op">%&gt;%</span><span class="kw">as_tibble</span>()</span>
<span id="cb908-6"><a href="regression.html#cb908-6"></a></span>
<span id="cb908-7"><a href="regression.html#cb908-7"></a></span>
<span id="cb908-8"><a href="regression.html#cb908-8"></a>Americas&lt;-posterior_draws<span class="op">%&gt;%</span></span>
<span id="cb908-9"><a href="regression.html#cb908-9"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> continentAmericas)) <span class="op">+</span></span>
<span id="cb908-10"><a href="regression.html#cb908-10"></a><span class="st">  </span><span class="kw">geom_histogram</span>()<span class="op">+</span></span>
<span id="cb908-11"><a href="regression.html#cb908-11"></a><span class="st">   </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span><span class="kw">median</span>(posterior_draws<span class="op">$</span>continentAmericas), <span class="dt">color=</span><span class="st">"red"</span>, <span class="dt">size=</span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Frequency"</span>, <span class="dt">title =</span> <span class="st">"Posterior Distribution"</span>, <span class="dt">subtitle =</span> <span class="st">"The Red Line Represents the Median"</span>)</span>
<span id="cb908-12"><a href="regression.html#cb908-12"></a></span>
<span id="cb908-13"><a href="regression.html#cb908-13"></a>Americas</span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="book_temp_files/figure-html/unnamed-chunk-591-1.png" width="672"></p>
<p>Now, let us break down the coefficients from the <code>print()</code> function:</p>
<div class="sourceCode" id="cb910"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb910-1"><a href="regression.html#cb910-1"></a><span class="kw">print</span>(bayes_lifeExp, <span class="dt">digits =</span> <span class="dv">2</span>, <span class="dt">detail =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>##                   Median MAD_SD
## (Intercept)       54.91   1.05 
## continentAmericas 18.63   1.77 
## continentAsia     15.78   1.68 
## continentEurope   22.77   1.70 
## continentOceania  25.00   5.36 
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 7.43   0.45</code></pre>
<pre><code>## # A tibble: 5 x 3
##   term              estimate std.error
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)           54.9      1.05
## 2 continentAmericas     18.6      1.77
## 3 continentAsia         15.8      1.68
## 4 continentEurope       22.8      1.70
## 5 continentOceania      25.0      5.36</code></pre>
<p>The coefficients can be interpreted as follows:</p>
<ol style="list-style-type: decimal">
<li>
<code>intercept</code> corresponds to the mean life expectancy of countries in Africa of 54.91.</li>
<li>
<code>continentAmericas</code> corresponds to countries in the Americas and the value +18.63 is the same as the difference in mean life expectancy relative to Africa. In other words, the mean life expectancy of countries in the Americas is 54.91 + 18.63= 73.54.</li>
<li>
<code>continentAsia</code> corresponds to countries in Asia and the value + 15.78 is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Asia is 70.69.</li>
<li>
<code>continentEurope</code> corresponds to countries in Europe and the value +22.77 is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Europe is 77.68.</li>
<li>
<code>continentOceania</code> corresponds to countries in Oceania and the value +25 is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Oceania is 79.91.</li>
</ol>
<p>The model also has a sigma, or residual standard error, of 7.43. Thus, sigma tells us that a country’s life expectancy will be between plus or minus 7.43 the prediction based on what continent the country is in for 68% of the data. Aslo, a country’s life expectancy will be between plus or minus two sigma, 14.86, of the prediction for about 95% of the data given which continent the country is in.</p>
<p>Finally, for all of the coefficients, we are provided a MAD_SD. We can use it to create each coefficient’s credible interval.</p>
<div class="sourceCode" id="cb913"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb913-1"><a href="regression.html#cb913-1"></a><span class="kw">posterior_interval</span>(bayes_lifeExp, <span class="dt">prob =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##                        2.5%     97.5%
## (Intercept)       53.021506 56.976819
## continentAmericas 15.245559 22.227278
## continentAsia     12.548909 19.092786
## continentEurope   19.434291 26.091442
## continentOceania  14.645149 35.200565
## sigma              6.607444  8.397343</code></pre>
<p>Each of these tell us there is a 95% probability that the true value of each parameter will fall within the respective interval.</p>
<p>Now, we can call <code>summary()</code> on our <code>bayes_lifeExp</code> model:</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb915-1"><a href="regression.html#cb915-1"></a><span class="kw">summary</span>(bayes_lifeExp)</span></code></pre></div>
<pre><code>## 
## Model Info:
##  function:     stan_glm
##  family:       gaussian [identity]
##  formula:      lifeExp ~ continent
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help('prior_summary')
##  observations: 142
##  predictors:   5
## 
## Estimates:
##                     mean   sd   10%   50%   90%
## (Intercept)       54.9    1.0 53.6  54.9  56.2 
## continentAmericas 18.7    1.8 16.4  18.6  21.0 
## continentAsia     15.8    1.7 13.6  15.8  17.9 
## continentEurope   22.7    1.7 20.6  22.8  24.9 
## continentOceania  25.0    5.3 18.3  25.0  31.7 
## sigma              7.5    0.5  6.9   7.4   8.0 
## 
## Fit Diagnostics:
##            mean   sd   10%   50%   90%
## mean_PPD 67.0    0.9 65.9  67.0  68.2 
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).
## 
## MCMC diagnostics
##                   mcse Rhat n_eff
## (Intercept)       0.0  1.0  3053 
## continentAmericas 0.0  1.0  3688 
## continentAsia     0.0  1.0  3873 
## continentEurope   0.0  1.0  3649 
## continentOceania  0.1  1.0  4021 
## sigma             0.0  1.0  4328 
## mean_PPD          0.0  1.0  4924 
## log-posterior     0.0  1.0  1644 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).</code></pre>
<p>Again, the few things to check from summary other than tht estimates are that the <code>mean_ppd</code> is in accords with the mean of the outcome variable, in our case <code>life_Exp</code>, and that all the <code>Rhat</code>s are close to 1. The mean of the outcome variable is 67.0074225, thus the <code>mean_ppd</code> is very close. Also all of the <code>Rhat</code>s are 1.0, signifying that our model converged and that it is reliable.</p>
<p>###Prediction</p>
<p>Finally, we will make prediction about new data using our <code>stan_glm()</code> model. Let us create a new dataset of continents. To do this, we will create a tibble containing the five various continents from the <code>gaminder2007</code> dataset.</p>
<div class="sourceCode" id="cb917"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb917-1"><a href="regression.html#cb917-1"></a>new &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">continent =</span> <span class="kw">c</span>(<span class="st">"Asia"</span>, <span class="st">"Africa"</span>, <span class="st">"Americas"</span>, <span class="st">"Europe"</span>, <span class="st">"Oceania"</span>))</span>
<span id="cb917-2"><a href="regression.html#cb917-2"></a></span>
<span id="cb917-3"><a href="regression.html#cb917-3"></a>new</span></code></pre></div>
<pre><code>## # A tibble: 5 x 1
##   continent
##   &lt;chr&gt;    
## 1 Asia     
## 2 Africa   
## 3 Americas 
## 4 Europe   
## 5 Oceania</code></pre>
<p>We know a regression with categorical variable does not yield a line, but rather offsets relative to a baseline for comparison. Thus, <code>posterior_linepred()</code> will help us quantify uncertainty in the paramets, but it cannot be interpreted using lines. Remember, we can apply <code>add_fitted_draws()</code> instead of <code>posterior_linepred</code> because it provides the data in a tidy manner. Also, remember that the <code>mean</code> for each group of predictions, <code>continent</code>, corresponds to the point estimate.</p>
<div class="sourceCode" id="cb919"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb919-1"><a href="regression.html#cb919-1"></a>new <span class="op">%&gt;%</span></span>
<span id="cb919-2"><a href="regression.html#cb919-2"></a><span class="st">  </span><span class="kw">add_fitted_draws</span>(<span class="dt">model =</span> bayes_lifeExp, .)<span class="op">%&gt;%</span></span>
<span id="cb919-3"><a href="regression.html#cb919-3"></a><span class="st">  </span><span class="kw">group_by</span>(continent)<span class="op">%&gt;%</span></span>
<span id="cb919-4"><a href="regression.html#cb919-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">point_estimate =</span> <span class="kw">mean</span>(.value))<span class="op">%&gt;%</span></span>
<span id="cb919-5"><a href="regression.html#cb919-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> continent, <span class="dt">y =</span> point_estimate)) <span class="op">+</span></span>
<span id="cb919-6"><a href="regression.html#cb919-6"></a><span class="st">  </span><span class="kw">stat_interval</span>(<span class="kw">aes</span>(<span class="dt">x =</span> continent, <span class="dt">y =</span> .value), <span class="dt">.width =</span> <span class="kw">c</span>(<span class="fl">0.68</span>, <span class="fl">0.95</span>)) <span class="op">+</span></span>
<span id="cb919-7"><a href="regression.html#cb919-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> continent, <span class="dt">y =</span> point_estimate)) <span class="op">+</span></span>
<span id="cb919-8"><a href="regression.html#cb919-8"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb919-9"><a href="regression.html#cb919-9"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Predicted Life Expectancy"</span>, <span class="dt">color =</span> <span class="st">"Uncertainty Level"</span>, <span class="dt">title =</span> <span class="st">"Posterior Linear Prediction With Uncertainty"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">coord_flip</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-598-1.png" width="672"></p>
<p>From the graph, we can see variation in the point estimates for each <code>continent</code> with Africa having a signicantly lower life expectancy. From the printed output, we continentOceania had the highest coefficient, meaning the greatest difference in mean life expectancy relative to Africa. Thus, we would expect Oceania to have highest life expectancy in comparison to the other continents; however, we see their is a very wide interval. This suggests their is a lot of uncertainty in the Oceania parameter. This is perhaps due to there not being as many countries on the continent of Oceania and there are vast differences in life expectancy between the countries in the dataset. Remember that the more data their is, the more accurate our posterior predictions will be and as we can see from above, the effect of the absence of data is shown in the uncertainty in the Oceania linear predictions.</p>
<p>Alternatively, we can use <code>posterior_predict()</code> to understand the posterior predicitive distribution for each continent. Remember we can perform <code>add_predicted_draws()</code> instead of <code>posterior_predict</code> because it provides the data in a tidy manner.</p>
<div class="sourceCode" id="cb920"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb920-1"><a href="regression.html#cb920-1"></a>new <span class="op">%&gt;%</span></span>
<span id="cb920-2"><a href="regression.html#cb920-2"></a><span class="st">  </span><span class="kw">add_predicted_draws</span>(., <span class="dt">model =</span> bayes_lifeExp)<span class="op">%&gt;%</span></span>
<span id="cb920-3"><a href="regression.html#cb920-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> .prediction, <span class="dt">y =</span> continent)) <span class="op">+</span></span>
<span id="cb920-4"><a href="regression.html#cb920-4"></a><span class="st">  </span><span class="kw">stat_halfeyeh</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb920-5"><a href="regression.html#cb920-5"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Predicted Life Expectancy"</span>, <span class="dt">title =</span> <span class="st">"Posterior Predictive Distribution For Each Continent's Life Expectancy"</span>)</span></code></pre></div>
<pre><code>## Warning: 'stat_halfeyeh' is deprecated.
## Use 'stat_halfeye' instead.
## See help("Deprecated") and help("tidybayes-deprecated").</code></pre>
<p><img src="book_temp_files/figure-html/unnamed-chunk-599-1.png" width="672"></p>
<p>Unlike our predictive distribution from <code>bayes_score</code> model in the previous section, we see a lot more differentiation between the categories. Although there is overlap at the tail, African Life Expectancy appears to be significantly lower than Life Expectancy on other continents. This plot shows the predictions are more stable. We can be more assured that there are actaul differences in predicted life expectancy between continents, which is something we would not be privy to using only point predictions.</p>
<p>Finally, one other way to display the posterior predictive distribution is using <code>stat_intervalh()</code>.</p>
<div class="sourceCode" id="cb922"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb922-1"><a href="regression.html#cb922-1"></a>new <span class="op">%&gt;%</span></span>
<span id="cb922-2"><a href="regression.html#cb922-2"></a><span class="st">  </span><span class="kw">add_predicted_draws</span>(., <span class="dt">model =</span> bayes_lifeExp)<span class="op">%&gt;%</span></span>
<span id="cb922-3"><a href="regression.html#cb922-3"></a><span class="st">  </span><span class="kw">group_by</span>(continent) <span class="op">%&gt;%</span></span>
<span id="cb922-4"><a href="regression.html#cb922-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">median_prediction =</span> <span class="kw">median</span>(.prediction)) <span class="op">%&gt;%</span></span>
<span id="cb922-5"><a href="regression.html#cb922-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> .prediction, <span class="dt">y =</span> continent)) <span class="op">+</span></span>
<span id="cb922-6"><a href="regression.html#cb922-6"></a><span class="st">  </span><span class="kw">stat_intervalh</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb922-7"><a href="regression.html#cb922-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> median_prediction, <span class="dt">y =</span> continent)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb922-8"><a href="regression.html#cb922-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Predicted Life Expectancy"</span>, <span class="dt">title =</span> <span class="st">"Posterior Predictive Distribution For Each Continent's Life Expectancy"</span>)</span></code></pre></div>
<pre><code>## Warning: 'stat_intervalh' is deprecated.
## Use 'stat_interval' instead.
## See help("Deprecated") and help("tidybayes-deprecated").</code></pre>
<p><img src="book_temp_files/figure-html/unnamed-chunk-600-1.png" width="672">
By default the uncertainty levels are <code>0.5,0.8, 0.95</code> but those can be altered with the <code>.width</code> input to your specification such as <code>.width = c(0.68, 0.95)</code>.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>
<span class="header-section-number">10.9</span> Conclusion</h2>
<p>In this chapter, you’ve studied the term <em>basic regression</em>, where you fit models that only have one explanatory variable. In Chapter <a href="multiple-regression.html#multiple-regression">11</a>, we’ll study <em>multiple regression</em>, where our regression models can now have more than one explanatory variable! In particular, we’ll consider two scenarios: regression models with one numerical and one categorical explanatory variable and regression models with two numerical explanatory variables. This will allow you to construct more sophisticated and more powerful models, all in the hopes of better explaining your outcome variable <span class="math inline">\(y\)</span>.</p>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="n-parameters.html"><button class="btn btn-default">Previous</button></a>
<a href="multiple-regression.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-07-08
</p>
</div>
</div>



</body>
</html>
