---
output_yaml:
  - _output.yml
---

# N Parameters {#n-parameters}


*This chapter is still very much a draft.* Come back in a few weeks for a better version.

<!-- At long last, Niel was here. -->


<!-- New idea! Start with non-causal data set and do the descriptive example first! It is simpler than the causal example. Can explain interaction effects more simply and then, when doing shaming, it is easier to understand what heterogeneous treatment effects mean. What what data set to use? Chapter 11 is now using nes. -->

<!-- Outline: The goal of this chapter is to replicate all of the things we have learned in chapter 8, except with models that allow for more than two parameters, which mostly involves having more than one right-hand side variable.  -->

<!-- For both data examples, do the following: -->

<!-- Wisdom: Start with Wisdom discussion, both of validity and, in the case of shaming, the specific estimand which is of interest. Part of this is: What question are you trying to answer? What problem are we solving?  -->

<!-- Then, we will be going through a series of models, always with greater complexity. For each model, we have: -->

<!-- Justice: each model discussion begins with Justice, which includes a Preceptor Table --- note how this changes for each model, because the Preceptor Table only needs columns for the covariates in the model, and only needs outcome columns for the outcomes you care about --- and then provides the math of the situation, which is also different each time.  A Preceptor Table has the rows and columns such that, if there are no missing values, answering your quetsion is trivial. I guess you might not have to give the Preceptor Table and the math every single time. But you do need to describe how they would differ from the last example. -->

<!-- Courage: Then, we have Courage, which means run the code and interpret it. This will include posterior_linpred(). Indeed, that is where this section finishes. In other words, each model includes a discussion of Justice and Courage, labeled as such. -->

<!-- Temperance: After you come to your final model, it is time for a discussion of Temperance: All the reasons why you should be careful about what you do with the model in the future. This section includes the use of posterior_predict(). -->


<!-- 1) EDA of `shaming`. I think that hh_size is a good variable to check for interactions with, especially with Control. Create a new variable called `solo` which is TRUE if the `hh_size` is equal to 1 and FALSE otherwise. In other words, does the state of living alone tell us anything? We will interact this term with treatment in our model. It is cool that the p value for the interaction of solo with Self is 0.06. A perfect example for the evils of testing!  -->



<!-- 2) `primary_06` as a function of `treatment`, `gender`, `solo`, `age` and (some of) their interactions. We will build up this model step-by-step, very similar to how we explored the effect of treatment in chapter 8. But we go deeper because  we are learning about interactions. Note that this situation is different from Chapter 8 in that fitted values and predicted values are not the same thing! The fitted value, for a combination of values for treatment and solo, is something 0.30, meaning that 30% of the people in this bucket votes. But the predicted value must be 0 or 1. Either you voted or you didn't. This example is clearly causal and so you need a Rubin Table with 5 potential outcome columns. The key difference in this chapter is that we are using lots of right hand side variables, both continuous and discrete. -->

<!-- 3) EDA of `nes`. This can be fairly quick. Again, we won't use all the variables. Only discuss the ones we do use. One of the variables should be year. We want to show off Gelman's trick by plotting things by year. -->

<!-- 4) We are making a predictive model of what as a function of what other stuff? Want to use a continuous variable and some discrete variables as well. Interactions too.  -->

<!-- 5) Discussion of the difference between predictive and causal, and how we can interpret a model as causal even if it uses observational data. Apply to the nes example model. -->

<!-- 6) Pitfalls. Does this belong? Discuss model selection and overfitting. Again, we are not solving these problems here. They are hard problems. Instead, we are motivating chapter 10, which should perhaps be re-titled.  -->


<!-- Thoughts -->

<!-- * Should we do any Bayesian stuff here. Maybe not? We build the dgm() function by hand, maybe simplifying things a bit by not taking parameter uncertainty seriously. We mentioned that there must be an easier way, and then we do that in chapter 10.  -->

<!-- * Should we discuss overfitting here? I think that the answer is Yes, that we at least have to mention it and why it is a problem. Indeed, we have at least two pages making sure the concepts are clear. Again, we hint at what solutions might look like and promise a resolution in chapter 10. Maybe we mention pooling. Maybe we mention cross-validation. But we don't yet solve the problem.  -->

<!-- * The key part of this chapter is showing the themes.Rmd all the way through two problems, one causal and one not. In that way, it is very similar to chapter 8. Indeed, perhaps we should make these two chapters as alike as possible. -->

Having created models with one parameter in Chapter \@ref(one-parameter) and two parameters in Chapter \@ref(two-parameters), you are now ready to make the jump to $N$ parameters. 

Imagine you are running for Governor and want to do a better job of getting your voters to vote. You recently read about a large-scale experiment showing the effect of sending out a voting reminder that "shames" citizens who do not vote. You are considering sending out a "shaming" voting reminder yourself. What will happen if you do? Will more voters show up to the polls? Additionally, on the day of the election a female citizen is randomly selected. What is the probability she will vote?    

In this chapter, we will consider models with multiple parameters and the complexities that arise from these additions. We will also learn how to make more accurate predictions using Bayesian methods that will provide us with answers to the questions posed above.

<!-- HV: Should this overarching question change since we are now starting with the governors data rather than shaming? -->

## Explorator Data Analysis of `governors`

Begin by loading the packages which we need.

```{r, message=FALSE}
library(PPBDS.data)
library(skimr)
library(broom.mixed)
library(rstanarm)
library(gtsummary)
library(tidyverse)
```

<!-- Talk about the data, as we usual do. Lots of interesting things. Explain how we only have data for people who are already dead. That is an interesting restriction, and effects the Preceptor Table in interesting ways. Make sure to mention all the variables we end up keeping below. Finish with some graphics which show how various things are related to our lefthand side variable: alive_post. -->

<!-- HV: Should I move my EDA introduction from the shaming EDA to here since this is now first? Same with the general descriptions of functions such as skim, glimpse, etc. -->

We will start off by using a subset of the `governors` data set from the `PPBDS.data` package. This data set looks at the lifespans of U.S. politicians from 1945 to 2012. It corresponds to the paper "Longevity Returns to Political Office" by Barfort, Klemmensen & Larsen (2019), and aims to find out whether winning an election influences the lifespan of politicians.

Let's explore the variables and observations contained within this data set by performing an Exploratory Data Analysis (EDA). EDAs help us summarize the data, identify patterns, spot outliers or anomalies, and more. We can start by running some summarizing commands such as `glimpse()`, `sample_n()`, and `skim()`.

```{r}
glimpse(governors)
```

As we can see, there are 11 variables and 1,092 observations. In this chapter, we will only be looking at the variables `state`, `year`, `last_name`, `sex`, `status`, `alive_post`, and `alive_pre`. We will also create a new variable called `won` that is `TRUE` when the win margin is greater than 0, and `FALSE` if not.

```{r}
ch9_gov <- governors %>% 
  mutate(won = ifelse(win_margin > 0, TRUE, FALSE)) %>% 
  select(last_name, year, state, sex, status, won, alive_post, alive_pre)
```

There are a few things to note when looking at this data. First, the data set includes the variables `alive_pre` and `alive_post`, which tell us how many days a candidate lived before the election took place and how many days a candidate lived after the election took place, respectively. Therefore, only politicians who are already deceased are included in this data set. Furthermore, candidates with unknown dates of death were not included in the dataset. 

Another caveat is that for a given election, only the top two candidates are included in the data set. If a politician did not receive the highest or second-highest number of votes, they are excluded. 

Finally, it is noted that for some observations, only the birth or death year of a candidate could be determined, in which case the date was taken as July 1st of that year.

Now, let's continue with the EDA by running `sample_n()` on the data. This will give us a random sample of observations to look at and may help us notice trends in the data, especially if we run it multiple times.

```{r}
sample_n(ch9_gov, 10)
```
From this sample, we can see some trends start to appear. For example, we see that `sex` is most often "Male", and status is most often "Challenger" rather than "Incumbent".

Another useful command in summarizing data sets during and EDA is `skim()`.

```{r}
skim(ch9_gov)
```

This output groups the variables together by type (character, logical, numeric, etc.). From this, we can see things like the number of unique last names, how many of the observations in our subset won versus how many lost their elections, and the mean number of days before and after a gubernatorial election that a candidate lived. We are also given histograms of the numerical data. In looking at the histogram for `year`, we see that it is skewed right, with half of the observations from election years between 1945 and 1962. This makes sense logically, because we are only looking at deceased candidates, and candidates from more recent elections are more likely to still be alive.

In using this data set, our left-side variable will be `alive_post`. We want to know how the election affected the lifespan of these gubernatorial candidates. Let's look at some graphs and plots showing the relationships between `alive_post` and some of the other variables in our subset.

```{r}
ch9_gov %>%
  ggplot(aes(x = year, y = alive_post)) +
  geom_point()
```
Starting with the relationship between `alive_post` and `year`, we can see that the data is skewed right and that there is a defined line that data points do not exist above. There are no data points in the top right portion of the graph because it is not possible to have run in 2011, lived 20,000 days after the election took place, and still have died before the data set was created. This line represents the most a candidate could have possibly lived - and still have died - to be included the data set. The reason this line is slanted downward is because the maximum value for this scenario is greater in earlier years. That is, those candidates who ran for governor in earlier years could live a longer time after the election and still have died prior to the data set creation, giving them higher `alive_post` values than those who ran for office in later years. 

Another note to make about this graph is that there are fewer observations in later years because less have died. Many governors who ran for office in the 21st century are still alive today and therefore excluded from this data set. 


```{r}
ch9_gov %>%
  ggplot(aes(x = won, y = alive_post)) +
  geom_boxplot()
```
This box plot is interesting in that it takes a direct look at the relationship we are trying to learn more about: whether winning an election influences the lifespan of politicians. In looking at these box plots, we can see that there are minor differences in the width of the boxes and length of the lines. Of the candidates who won, the 25% who lived the longest after the election lived longer than the top 25% of candidates who lost the election.

<!-- Clean up and add more discussion here -->

<!-- HV: What specific types of plots/graphs do you suggest adding? -->



<!-- HV: Where should the section/subsection of Wisdom go? -->
### Wisdom

Now that we have a general understanding of the data we are working with, let's talk about Wisdom.



## Model of `alive_post`

<!-- DK: Maybe instead of tables, we should just be showing graphics, using ggdist/tidybayes. -->

Let's explore descriptive, not causal, models of covariates associated with how long candidates for governor live after elections.

To begin, let's run a Bayesian regression of gubernatorial candidate lifespan prior to the election on candidate lifespan after the election. That is, we will regress `alive_pre` on `alive_post` using `stan_glm()`. 

```{r}
obj.gov.1 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ alive_pre,
                      family = gaussian(),
                      refresh = 0)
```

<!-- This is a simple example. Only three parameters are estimated: intercept, beta and sigma. But step through them all slowly, discussing what they mean. Not sure on the best way to display this. Maybe we just ignore sigma? -->

```{r}
tbl_regression(obj.gov.1, intercept = TRUE)
```

Now that we have this regression table, we will interpret it's meaning. This is a simple regression, similar to ones ran in Chapter 8. 

To start, let's interpret the intercept. Since our independent variable is `alive_pre`, the intercept is the `alive_post` value when `alive_pre` is zero. Here, we would interpret this intercept as the lifespan of a gubernatorial candidate after the election, if the candidate was alive for zero days prior to the election. Using the numbers included in the regression table, this means that a candidate who was alive for zero days prior to the election is expected to live 26,515 days after the election, on average. This interpretation does not make much sense, however, because newborn babies are not capable of running for governor. To make this more interpretable, we will introduce the topic of *centering*. But first, let's finish interpreting this model we have created.

Let's continue by discussing Beta for `alive_pre`. This value, -0.86, represents the slope of the model. For every unit increase in our independent variable, our dependent variable will change by this beta value. Putting this slope definition in terms of our model, this means that for every additional day a candidate is alive before an election, their lifespan after the election will decrease by 0.86 days on average. If we are given the number of days a candidate lived before the election and want to estimate how long they will live for after, we will multiply the days they were alive prior by this beta of -0.86, then subtract that from the intercept. Our 95% confidence interval here tells us that we are 95% confident that the true mean slope of this regression is contained between -0.93 and -0.78.

And now, back to centering. Centering is a tool that is used when the model's intercept does not make logical sense to interpret. To center a model, we pick a constant value, usually the mean of the independent variable, and subtract that constant from every value of the independent variable. This changes the zero point for the variable and shifts the model over.

In this example, we want to center the value for `alive_pre`, the predictor. First, we must pick the value that we will center by. Here, we will use the mean of `alive_pre`. Once we find this value, we will subtract it to every `alive_pre` value, creating a new variable called `alive_pre_centered`. We then regress this `alive_pre_centered` variable on `alive_post`.

```{r}
center <- mean(ch9_gov$alive_pre)
ch9_gov$alive_pre_centered <- ch9_gov$alive_pre - center
```

```{r}
obj.gov.1.centered <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ alive_pre_centered,
                      family = gaussian(),
                      refresh = 0)
```

```{r}
tbl_regression(obj.gov.1.centered, intercept = TRUE)
```

In this model, we can see that the intercept has increased while the slope has stayed the same. When we interpret this model, we only have to change the definition of the intercept. Rather than the intercept representing the lifespan after running of a candidate who was alive for zero days before running for governor, it now represents the post-election lifespan of a gubernatorial candidate who was alive for the mean number of days before running. In this instance, our center value is approximately 18892, so we will use this value in our interpretation. If a candidate was alive for 18892 days before running for governor (the mean value in this data set), they are expected to live for 10309 days after the election, on average. This interpretation makes much more sense in comparison to our first one, because the likelihood that a candidate lived for 18892 days before running for office is much more practical than their pre-election lifespan being 0.

Since all values of the predictor variable decreased by the same amount, the model shifted over but the slope of the linear model did not change. For this reason, the slope stays the same.

<!-- HV: I am not sure if I explained centering in the clearest/correct way. -->

Let's now regress `sex` on `alive_post` to see how sex affects candidates' post-election lifespans.

```{r}
obj.gov.2 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ sex - 1,
                      family = gaussian(),
                      refresh = 0)
```

<!-- Make these two the first models. Just like how we ended chapter 8. But you need to do more. Follow Modern Dive section: https://moderndive.com/5-regression.html#model2. In particular, explain fitted value, residual. Show the math.  Justice. The math plus the fact that the Preceptor is descriptive, meaning only one column for Y. Read the themes document.  -->

<!-- Do this in your preferred order. -->


```{r}
tbl_regression(obj.gov.2, intercept = TRUE)
```
In this regression, we use the -1 in the formula to make the output more straightforward, with no intercept to interpret. Now looking at the regression table output, let's interpret the Beta values for Female and Male, starting with a mathematical formula.

You may recall from algebra that the equation of a line is $$ y = a + b*x $$. It is defined by two coefficients *a* and *b*. The intercept coefficient *a* is the value of *y* when $$ x = 0 $$. The slope coefficient *b* for *x* is the increase in *y* for every increase of one in *x*.

However, when defining a regression line, we use slightly different notation: the equation of the regression line is $$ \hat{y} = {b_0} + {b_1} x  $$. The intercept coefficient is $$ {b_0} $$, so $$ {b_0} $$ is the value of $$ \hat{y} $$ when $$ x = 0 $$. The slope coefficient for *x* is $$ {b_1} $$, i.e., the increase in $$ \hat{y} $$ for every increase of one in *x*. Why do we put a "hat" on top of the *y*? It's a form of notation commonly used in regression to indicate that we have a "fitted value," or the value of *y* on the regression line for a given *x* value. This fitted value that falls on the regression line may differ from the observed value of *y* given that particular *x* value. The difference between the observed y-value and the predicted, or fitted, y-value is called the *residual*. 

<!-- HV: Are terms like fitted value and residual supposed to be in a certain font style to highlight them? -->

In many of the examples we will be discussing in this chapter, our regression equations will contain multiple beta values for all the different parameters we are interpreting. 

Now looking back to the regression model we just created, we see that there is no intercept. Instead of having a $$ {b_0} $$ value, we have $$ {b_m} $$ and $$ {b_f} $$ for male and female. This makes things easier to interpret. Without having to add or subtract anything from an intercept, this regression tells us that on average, women are expected to live 5840 days after running for governor, and men are expected to live 10395 days after running for governor.

This is a strange result, as men are expected to live twice as long as women. One explanation for this might be that women don't run for governor until later in life, and therefore are not expected to live as long.

<!-- HV: Should probably add more discussion of this result. -->

<!-- DK: Weird result, huh? Men live twice as long as women! Useful to discuss why that might be the case. -->

<!-- Perhaps of next is to understand what the intercept is. That is, this model is identical to the previous. -->

Now that we have interpreted the model using a -1 in the formula to get both a $$ {b_m} $$ value and a $$ {b_f} $$ value, let's take away the -1 and regress `sex` on `alive_post` to see how our equation changes.

```{r}
obj.gov.2a <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ sex,
                      family = gaussian(),
                      refresh = 0)
```

```{r}
tbl_regression(obj.gov.2a, intercept = TRUE)
```

From this result, we can see that we no longer have a value for female, however we do have an intercept. In this regression our mathematical regression formula is $$ \hat{alivepost} = {b_0} + {b_m}* male  $$. $$ {b_0} $$ is our intercept value which here would be 5875. You may notice that this is very similar to the female value from before. In this type of model, our intercept represents the characteristic of the variable that is left unrepresented in the model. Here our slope, or $$ {b_m} $$ value is for when the candidate is male. Therefore, we can infer that the intercept value represents those who are not male: females.

When the candidate is a male, we add the beta for male to the intercept value, which gives us the average lifespan of a male gubernatorial candidate after an election. As we can see from adding $${b_m}$$ and $${b_0}$$, this value is the same as what we got for males in the previous model.

<!-- HV: Shouldn't the intercept be the same as the female value from the previous regression? Is the difference here due to running stan_glm? Kind of confused by this result. -->

```{r}
sex_means <- 
  governors %>% 
  group_by(sex) %>% 
  summarize(avg = mean(alive_post), .groups = "drop")

governors %>% 
  ggplot(aes(sex, alive_post)) + 
    geom_jitter(width = 0.1, alpha = 0.2) + 
    geom_point(data = sex_means, aes(y = avg), color = "red")
```

<!-- use posterior lin pred instead of sex_means to get fitted values -->

<!-- DK: All the above happens before this -->

```{r}
obj.gov.3 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ sex + alive_pre,
                      family = gaussian(),
                      refresh = 0)
```

```{r}
tbl_regression(obj.gov.3, intercept = TRUE)
```

<!-- DK: Cool, eh? It is (mostly?) an age effect! Controlling for age at the election, women live about as long as men, but they are much more likely be candidates as an older age. Again, what we are doing is slowly building more complex models, understanding what they mean. -->

<!-- Also, note that this is a "parallel slopes" model. Explain slowly what that means.  Create a plot. -->

<!-- Side note: I would have thought that overlapping CIs for male/female would equate to insignificance of sex dummy when the regression includes a intercept. But that does not seem to be the case. Am I missing something? -->

<!-- Make the graphic which shows two parallel lines, by calling two layers of geom_smooth().  -->

<!-- With this as warm-up, we are now in a position to explore interactions. -->

```{r}
obj.gov.4 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ sex*alive_pre,
                      family = gaussian(),
                      refresh = 0)
```

```{r}
tbl_regression(obj.gov.4, intercept = TRUE)
```

<!-- DK: Take your time explaining what this means! It is not trivial. Note that this is a non-parallel slopes model. -->


```{r, cache=TRUE}
obj.gov.5 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ state + sex*alive_pre,
                      family = gaussian(),
                      refresh = 0,
                      iter = 10000)
```



```{r}
tbl_regression(obj.gov.5, intercept = TRUE)
```

<!-- DK: Need to modify this to only show of the more interesting state coefficients. (Although we might show all the state pdfs in the graphic.) This a good place to discuss shrinkage, which is what is happening here. And that is probaly enough models. -->

## Explorator Data Analysis of `shaming`

Consider a new data set, `shaming`, corresponding to an experiment carried out by Gerber, Green, and Larimer (2008) titled "Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment". This experiment used several hundred thousand registered voters and a series of mailings to determine the effect of social pressure on voter turnout. 

Let's now do another EDA, starting off by running `glimpse()`.

```{r}
glimpse(shaming)
```

Here we see that `glimpse()` gives us a look at the raw data contained within the `shaming` data set. At the very top of the output, we can see the number of rows and columns, or observations and variables respectively. We see that there are 344,084 observations, with each row corresponding to a unique respondent. The "Columns: 10" tells us that there are 10 variables within this data set. Below this, we see a cutoff version of the entire data set that has the variables on the left as rows and the observations as a list separated by commas, as compared to the tibble output that presents with the variables as columns and the observations as rows running horizontally.

<!-- It may be worth mentioning that general_04 is always "yes", unlike all the other voting history variables which have values of "yes" and "no". Why is that? I *think* that this is caused by their sampling plan. They found all the people who voted in the 2004 general election. Then, the authors found their history. As we would expect, those people had sometimes voted in the past and sometime not. Then, the authors sent the mailing. The key dependent variable, primary_06, is coding 0/1, since that makes doing the statistics easier. -->

From this summary, we get an idea about some of the variables we will be dealing with. Some variables that will be of interest to us are `sex`, `hh_size`, and `primary_06`. The variable `hh_size` tells us the size of the respondent's household, and `primary_06` tells us whether or not the respondent voted in the 2006 Primary election. 

There are a few things to note while exploring this data set. You may -- or may not -- have noticed that the only response to the `general_04` variable is "Yes". In their published article, the authors note that "Only registered voters who voted in November 2004 were selected for our sample" (Gerber, Green, Larimer, 2008). After this, the authors found their history then sent out the mailings.

It is also important to identify the dependent variable and its meaning. In this shaming experiment, the dependent variable is `primary_06`, which is a variable coded either 0 or 1 for whether or not the respondent voted in the 2006 primary election. This is the dependent variable because the authors are trying to measure the effect that the treatments have on the proportion of people who vote in the 2006 general election.

<!-- HV: Should I include discussion of the left-hand variable (treatment?) here? Or wait until we move into the regressions? -->

The voting results from other years, such as 2002 and 2004, are of less interest to us and can be removed from the abbreviated data set. In addition to removing `general_04`, `primary_02`, `general_02`, or `primary_04`, we also will not be taking particular interest in `birth_year`, or `no_of_names` within this chapter.
<!-- HV: should I explain why we are not using any of these variables? why they are not of great use to us? -->

By narrowing down the set of variables we are looking at and investigating, we will find more meaningful relationships among them. However, we have not yet discussed the most important variable of them all: `treatment`. The `treatment` variable is a factor variable with 5 levels, including the control. Since we are curious as to how sending mailings affects voter turnout, the treatment variable will tell us about the impact each type of mailing can make. Let's start off by taking a broad look at the different treatments.
<!-- HV: Is it okay to say the first sentence of this paragraph? -->

```{r}
shaming %>%
  count(treatment)
```

Four types of treatments were used in the experiment, with voters receiving one type of mailing. All of the mailing treatments carried the message, "DO YOUR CIVIC DUTY - VOTE!". 

The first treatment, Civic Duty, also read, “Remember your rights and responsibilities as a citizen. Remember to vote." This message acted as a baseline for the other treatments, since it carried a message very similar to the one displayed on all the mailings.

In the second treatment, Hawthorne, households received a mailing which told the voters that they were being studied and their voting behavior would be examined through public records. This adds a small amount of social pressure to the households receiving this mailing.

In the third treatment, Self, the mailing includes the recent voting record of each member of the household, placing the word "Voted" next to their name if they did in fact vote in the 2004 election or a blank space next to the name if they did not. In this mailing, the households were also told, “we intend to mail an updated chart" with the voting record of the household members after the 2006 primary. By emphasizing the public nature of voting records, this type of mailing exerts more social pressure on voting than the Hawthorne treatment.

The fourth treatment, Neighbors, provides the household members' voting records, as well as the voting records of those who live nearby. This mailing also told recipients, "we intend to mail an updated chart" of who voted in the 2006 election.

For now, let's focus on a subset of the data.

```{r}
ch9 <- shaming %>% 
  mutate(solo = ifelse(hh_size == 1, TRUE, FALSE)) %>% 
  mutate(age = 2006 - birth_year) %>% 
  select(primary_06, treatment, solo, sex, age)
```

We create the variable `solo`, which is TRUE for voters who live alone and FALSE for those that do not. We are curious to see if the treatment effect, if any, is the same for voters who live alone as it is for those who do not.

```{r}
ch9 %>% 
  skim()
```


<!-- DK: Add discussion of what you see here. No need to drop missing values since there aren't any. I think this next discussion can be dropped. -->


While some summarizing commands such as `glimpse()` gives us a good look at the possible values for the variables, it is difficult to read it in terms of individual observations. Recall that the *observational unit* is what is being measured. With the `shaming` data set, the observational unit would be the voter respondent. To get a better sense of some respondents' information, let's use `sample_n()` to gather a random sample of *n* observations from the data set.
<!-- HV: Does this belong here? -->

```{r}
ch9 %>% 
  sample_n(10)
```

Now we have a table with 5 random observations and the respondents' information in a regular table output. By taking a few random samples, we may start to see some patterns within the data. 

Now we have a table with 5 random observations and the respondents' information in a regular table output. By taking a few random samples, we may start to see some patterns within the data. Do you notice anything in particular about the variable `treatment`?

One other helpful summarizing technique we can use is `skim()`. To make the information it contains simpler, we will only be looking at three variables: `primary_06`, `treatment`, and `sex`. 

```{r}
shaming %>% 
  select(primary_06, treatment, sex) %>% 
  skim()
```

By running the `skim()` command, we get a summary of the data set as a whole, as well as the types of variables and individual variable summaries. At the top we see the number of columns and rows within the selected data set. Below this we are given a list with the different types of variables, or columns, and how often they appear within the data we are skimming. Following this, the variables are then separated by their column type, and we are given individual summaries based on the type. 

<!-- 2) `primary_06` as a function of `treatment` and `solo` and of their interaction. We will build up this model step-by-step, very similar to how we explored the effect of treatment in chapter 8. But we go deeper because  we are learning about interactions. Key thing is to go through all the themes.Rmd issues, at least until prediction. Note that this situation is different from Chapter 8 in that fitted values and predicted values are not the same thing! The fitted value, for a combination of values for treatment and solo, is something 0.30, meaning that 30% of the people in this bucket votes. But the predicted value must be 0 or 1. Either you voted or you didn't. This example is clearly causal and so you need a Rubin Table with 4 potential outcome columns. The key difference in this chapter is that we are using lots of right hand side variables, both continuous and discrete. -->

<!-- 3) EDA of `nes`. This can be fairly quick. Again, we won't use all the variables. Only discuss the ones we do use. One of the variables should be year. We want to show off Gelman's trick by plotting things by year. -->

<!-- 4) We are making a predictive model of what as a function of what other stuff? Want to use a continuous variable and some discrete variables as well. Interactions to.  -->

<!-- 5) I don't think we bother with a scenario which requires a bootstrap. Or should we?  -->

<!-- 6) Prediction. How do we use our model to make predictions? Bring the discussion back to the way that we opened the chapter with a problem. -->



Having created models with one parameter in Chapter \@ref(#one-parameter) and two parameters in Chapter \@ref(#two-parameters), you are now ready to make the jump to $N$ parameters.  The more parameters we include in our models, the more flexible they can become. But we must be careful of *overfitting*, of making models which are inaccurate because they don't use enough data to accurately estimate those parameters. The tension between overfitting and underfitting is central to the practice of data science.
<!-- HV: Where does this belong? -->

## Causal Effects of `treament`

We will now be looking into the causal effect of the treatment variable on the 2006 primary election. To start, let's build a model using `stan_glm()` followed by a regression table. 

```{r, cache=TRUE}
# obj.1 <- stan_glm(data = shaming, 
#                 formula = primary_06 ~ treatment - 1, 
#                 family = gaussian(), 
#                 refresh = 0)
```

```{r, echo=FALSE}
# tbl_regression(obj.1)
```

This table shows us each of the five treatments and their beta coefficients, along with a 95% Confidence Interval for these coefficients. The Control provides us with a baseline.

<!-- HV: Not exactly sure how to interpret this using causal effect... Should I be making preceptor tables here? -->



<!-- Talk about what these results mean.  Then, create the same model but with a different structure. -->


```{r, cache=TRUE}
# obj.2 <- stan_glm(data = shaming, 
#                 formula = primary_06 ~ treatment, 
#                 family = gaussian(), 
#                 refresh = 0)
```


```{r, echo=FALSE}
# tbl_regression(obj.2)
```

<!-- Explain how these two models are the same, except in how they define the parameters. Show us the math like Gelman does. Write down the math. For simple. -->



<!-- Once we talk about these things --- and, again, this is exactly what we have talked about in chapter 8 --- we can do a bit more. Like discuss how we are using 99%, because there is nothing magical aboyt 95%, other than convention. I also think it would be fun to show a nice graphic of this, highlighting how the estimates for Civic and Hawthorne overlap.  -->


### Interactions

<!-- This is new. With only two parameters, we can't really look at interaction effects. Need to discuss interaction effects in general. Also, note that heterogenous treatment effects are the same thing as interaction effects that involve a treatment effect as one of the variables.  -->

<!-- Feel free to build up this code, and other examples, more slowly than I am doing it here. -->

```{r, cache=TRUE}
# obj.3 <- stan_glm(data = shaming, 
#                 formula = primary_06 ~ sex + treatment + sex:treatment, 
#                 family = gaussian(), 
#                 refresh = 0)
```

```{r, echo=FALSE}
# tbl_regression(obj.3)
```

<!-- Takes a while to explain what all this means. -->

<!-- Two key issues: 1) Interpreting lots of parameters in a model. interactions, heterogenous treatment effects. shaming using lm().  -->

<!-- Treatment effect is not the same thing as a coefficient. -->

<!-- 
intercept

Interactions --- use: income ~ party*something

heterogeneous treatment effects --- use:  att_start ~ treatment*something 
just a fancy way of saying interaction effects, but with a variable which us causal


What problems do we face? All the things that make modeling difficult. Why is this so hard? -->

<!-- Centering. -->

<!-- Might naively just take the value for each bucket. But that overfits! Need to put down some structure, like ordering. -->

<!-- income category, party id, pooling, age, -->

<!-- overfitting/underfitting bias/variance -->

<!-- We must have left bootstrapping behind by now. No more bootstraps, at least for the purpose of calculating uncertainty. (We will use it later for the purpose of out-of-sample testing and avoiding overfitting.) Key lesson is that overfitting is easy. You can't just estimate a value for each cell. You need to smooth and borrow strength. Of course, the only way to do that is with a Bayesian approach. Right? We don't want to get into hacked likelihood approaches. -->

<!-- cces looks perfect for this project. There are 400,000 rows, so it seems like you ought to have plenty of data for anything you want, but once you realize there are 51 states and 10 years, things get sparse fast. We only have 15 observations, for example, for Wyoming in 2007. Once you start subsetting by race and education, you have no choice but to start borrowing strength.  -->

<!-- So, just what will we use? rstanarm(). If so (and if we have not introduced it earlier), we can begin with seeing how it is similar to lm() and then expand. This means that, in one paramter chapter, we should be doing lm( ~ 1). In two parameter, lm( ~ treatment) --- if treatment is zero one --- or, perhaps better, lm( ~ -1 + treatment) if treatment is a factor/character with two levels. We might also have introduced  -->
