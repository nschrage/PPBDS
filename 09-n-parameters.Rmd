---
output_yaml:
  - _output.yml
---

# N Parameters {#n-parameters}

<!-- Outline: -->

<!-- The goal of this chapter is to replicate all of the things we have learned in chapter 8, except with models that allow for more than two parameters, which mostly involves have more than one right-hand side variable. We go through all of themes.Rmd two more times, once with a causal data set and once with a predictive one. So far, nothing new or different. But we are also going to highlight three major problems. We aren't going to solve those problems, but we are going to hint at them. The problems are: a) How do you decide which model is "better?" (Not an easy question! But the ability to predict new data is certainly a big part of the answer. b) How do you avoid overfitting, not least because it makes your predictions horrible? c) How do you establish causality if you don't have a randomized experiment? -->

<!-- 0) Start with a problem-->


<!-- 1) EDA of `shaming`. I think that hh_size is a good variable to check for interactions with, especially with Control. One approach: Create a new variable called `solo` which is TRUE if the `hh_size` is equal to 1 and FALSE otherwise. In other words, does the state of living alone tell us anything? We will interact this term with treatment in our model. It is cool that the p value for the interaction with Self is 0.06. A perfect example for the evils of testing!  -->

<!-- It may be worth mentioning that general_04 is always "yes", unlike all the other voting history variables which have values of "yes" and "no". Why is that? I *think* that this is caused by their sampling plan. They found all the people who voted in the 2004 general election. Then, the authors found their history. As we would expect, those people had sometimes voted in the past and sometime not. Then, the authors sent the mailing. The key dependent variable, primary_06, is coding 0/1, since that makes doing the statistics easier. -->

<!-- 2) `primary_06` as a function of `treatment` and `solo` and of their interaction. We will build up this model step-by-step, very similar to how we explored the effect of treatment in chapter 8. But we go deeper because  we are learning about interactions. Key thing is to go through all the themes.Rmd issues, at least until prediction. Note that this situation is different from Chapter 8 in that fitted values and predicted values are not the same thing! The fitted value, for a combination of values for treatment and solo, is something 0.30, meaning that 30% of the people in this bucket votes. But the predicted value must be 0 or 1. Either you voted or you didn't. This example is clearly causal and so you need a Rubin Table with 4 potential outcome columns. The key difference in this chapter is that we are using lots of right hand side variables, both continuous and discrete. Unlike last chapter, we go through the entire process in this section, all the way through dgm and predictive uncertainty. -->

<!-- 3) EDA of `nes`. Or maybe something else? Let's discuss. This can be fairly quick. Again, we won't use all the variables. Only discuss the ones we do use. One of the variables should be year. We want to show off Gelman's trick by plotting things by year. -->

<!-- 4) We are making a predictive model of what as a function of what other stuff? Want to use a continuous variable and some discrete variables as well. Interactions too.  -->

<!-- 5) Discussion of the difference between predictive and causal, and how we can interpret a model as causal even if it uses observational data. Apply to the nes example model. -->

<!-- 6) Pitfalls. Discuss model selection and overfitting. Again, we are not solving these problems here. They are hard problems. Instead, we are motivating chapter 10, which should perhaps be re-titled.  -->


<!-- Thoughts -->

<!-- * Should we do any Bayesian stuff here. Maybe not? We build the dgm() function by hand, maybe simplifying things a bit by not taking parameter uncertainty seriously. We mentioned that there must be an easier way, and then we do that in chapter 10.  -->

<!-- * Should we discuss overfitting here? I think that the answer is Yes, that we at least have to mention it and why it is a problem. Indeed, we have at least two pages making sure the concepts are clear. Again, we hint at what solutions might look like and promise a resolution in chapter 10. Maybe we mention pooling. Maybe we mention cross-validation. But we don't yet solve the problem.  -->

<!-- * The key part of this chapter is showing the themes.Rmd all the way through two problems, one causal and one not. In that way, it is very similar to chapter 8. Indeed, perhaps we should make these two chapters as alike as possible. -->

Having created models with one parameter in Chapter \@ref(#one-parameter) and two parameters in Chapter \@ref(#two-parameters), you are now ready to make the jump to $N$ parameters. 

Imagine you are running for Governor and want to do a better job of getting your voters to vote. You recently read about a large-scale experiment showing the effect of sending out a voting reminder that "shames" citizens who do not vote. You are considering sending out a "shaming" voting reminder yourself. What will happen if you do? Will more voters show up to the polls? Additionally, on the day of the election a female citizen is randomly selected. What is the probability she will vote?

In this chapter, we will consider models with multiple parameters and the complexities that arise from these additions. We will also learn how to make more accurate predictions using Bayesian methods that will provide us with answers to the questions posed above.


## Explorator Data Analysis of `shaming`

Begin by loading packages which we have used in previous chapters.

```{r}
library(PPBDS.data)
library(skimr)
library(broom)
library(tidyverse)
```

We will start off this chapter introducing a new data set from the **PPBDS.data** package: `shaming`. This data set corresponds to an experiment carried out by Gerber, Green, and Larimer (2008) titled "Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment". This experiment used several hundred thousand registered voters and a series of mailings to determine the effect of social pressure on voter turnout. 

<!-- DK: Do glimpse and maybe other discussion here -->

<!-- Then mention treatment and run shaming %>% count(treatment), and then you discuss what these are. -->

Four types of treatments were used in the experiment, with voters receiving one type of mailing. All of the mailing treatments carried the message, "DO YOUR CIVIC DUTY - VOTE!". 

The first treatment, Civic Duty, also read, “Remember your rights and responsibilities as a citizen. Remember to vote." This message acted as a baseline for the other treatments, since it carried a message very similar to the one displayed on all the mailings.

In the second type of treatment, households received a mailing called the Hawthorne effect that told the voters that they were being studied and their voting behavior would be examined through public records. This adds a small amount of social pressure to the households receiving this mailing.

The third type of treatment is Self, in which the mailing includes the recent voting record of each member of the household, placing the word "Voted" next to their name if they did in fact vote in the 2004 election or places a blank space next to the name if they did not. In this mailing, the households were also told, “we intend to mail an updated chart" with the voting record of the members after the 2006 election. By emphasizing the public nature of voting records, this type of mailing exerts more social pressure on voting than the Hawthorne treatment.

The final type of mailing is the Neighbors treatment, which follows the Self mailing by listing the household members' voting records, as well as the voting records of those who live nearby. This mailing also told recipients, "we intend to mail an updated chart" of who voted in the 2006 election. This "shaming" treatment exerts the most social pressure of all the treatments and broadcasts individuals' voting habits. 

In addition to the treatments, the data set also consists of variables such as `primary_02`, `general_02`, `primary_04`, and `general_04` which tell us whether or not respondents voted in these primary and general elections. We are also given information about the respondents' backgrounds such as `sex`, `birth_year`, and household size (`hh_size`).

Consider the `shaming` tibble from **PPBDS.data**. Let's explore the variables and observations contained within this data set by performing an Exploratory Data Analysis (EDA). EDAs help us summarize the data, identify patterns, spot outliers or anomalies, and more. We can start by running some summarizing commands such as `glimpse()`, `sample_n()`, and `summary()`.

<!-- Provide a EDA of the shaming tibble, perhaps restricting it to a subset of the provided columns. Follow the lead of chapter 10. Make sure to use `glimpse()`, `sample_n()` and `skim()`. Readers need to know understand what the dependent variable means, along with left-hand side variables like treatment. -->

```{r}
glimpse(shaming)
```

Here we see that `glimpse()` gives us a look at the raw data contained within the `shaming` data set. At the very top of the output, we can see the number of rows and columns, or observations and variables respectively. We see that there are 344,084 observations, with each row corresponding to a unique respondent. The "Columns: 10" tells us that there are 10 variables within this data set. Below this, we see a cutoff version of the entire data set that has the variables on the left as rows and the observations as a list separated by commas, as compared to the tibble output that presents with the variables as columns and the observations as rows running horizontally.

While `glimpse()` gives us a good look at the possible values for the variables, it is difficult to read it in terms of individual observations. Recall that the *observational unit* is what is being measured. With the `shaming` data set, the observational unit would be the voter respondent. To get a better sense of some respondents' information, let's use `sample_n()` to gather a random sample of *n* observations from the data set. 

```{r}
shaming %>% 
  sample_n(10)
```

Now we have a table with 5 random observations and the respondents' information in a regular table output. By taking a few random samples, we may start to see some patterns within the data. Do you notice anything in particular about the variable `treatment`?
<!-- HV: Is it okay to pose these questions here? -->
<!-- HV: The no_of_names variable is supposed to give an integer with the number of names listed *IF* the respondent was in the "Neighbors" group but for all other treatments it is listed as 21. Should I just mutate this or have it dealt with in the data set as a whole? -->

To get a more in-depth summary of the variables and their components, we can run `summary()` on the data set.

```{r}
summary(shaming)
```

For numerical variables, the `summary()` output provides us with the minimum, maximum, mean, and quartiles. For variables such as `treatment`, we are given a list of the various treatments along with counts of how many observations are within each group. In this case, the character variables do not provide us with any information other than the number of observations for each.

One other helpful summarizing technique we can use is `skim()`. To make the information it contains simpler, we will only be looking at three variables: `primary_06`, `treatment`, and `sex`. 

```{r}
shaming %>% 
  select(primary_06, treatment, sex) %>% 
  skim()
```

By running the `skim()` command, we get a summary of the data set as a whole, as well as the types of variables and individual variable summaries. At the top we see the number of columns and rows within the selected data set. Below this we are given a list with the different types of variables, or columns, and how often they appear within the data we are skimming. Following this, the variables are then separated by their column type, and we are given individual summaries based on the type. 

There are a few things to note while exploring this data set. You may -- or may not -- have noticed that the only response to the `general_04` variable is "Yes". In their published article, the authors note that "Only registered voters who voted in November 2004 were selected for our sample" (Gerber, Green, Larimer, 2008). After this, the authors found their history then sent out the mailings.

It is also important to identify the dependent variable and its meaning. In this shaming experiment, the dependent variable is `primary_06`, which is a variable coded either 0 or 1 for whether or not the respondent voted in the 2006 primary election. This is the dependent variable because the authors are trying to measure the effect that the treatments have on the proportion of people who vote.

<!-- HV: Should I include discussion of the left-hand variable (treatment?) here? Or wait until we move into the regressions? -->

<!-- It may be worth mentioning that general_04 is always "yes", unlike all the other voting history variables which have values of "yes" and "no". Why is that? I *think* that this is caused by their sampling plan. They found all the people who voted in the 2004 general election. Then, the authors found their history. As we would expect, those people had sometimes voted in the past and sometime not. Then, the authors sent the mailing. The key dependent variable, primary_06, is coding 0/1, since that makes doing the statistics easier. -->

<!-- 2) `primary_06` as a function of `treatment` and `solo` and of their interaction. We will build up this model step-by-step, very similar to how we explored the effect of treatment in chapter 8. But we go deeper because  we are learning about interactions. Key thing is to go through all the themes.Rmd issues, at least until prediction. Note that this situation is different from Chapter 8 in that fitted values and predicted values are not the same thing! The fitted value, for a combination of values for treatment and solo, is something 0.30, meaning that 30% of the people in this bucket votes. But the predicted value must be 0 or 1. Either you voted or you didn't. This example is clearly causal and so you need a Rubin Table with 4 potential outcome columns. The key difference in this chapter is that we are using lots of right hand side variables, both continuous and discrete. -->

<!-- 3) EDA of `nes`. This can be fairly quick. Again, we won't use all the variables. Only discuss the ones we do use. One of the variables should be year. We want to show off Gelman's trick by plotting things by year. -->

<!-- 4) We are making a predictive model of what as a function of what other stuff? Want to use a continuous variable and some discrete variables as well. Interactions to.  -->

<!-- 5) I don't think we bother with a scenario which requires a bootstrap. Or should we?  -->

<!-- 6) Prediction. How do we use our model to make predictions? Bring the discussion back to the way that we opened the chapter with a problem. -->



Having created models with one parameter in Chapter \@ref(#one-parameter) and two parameters in Chapter \@ref(#two-parameters), you are now ready to make the jump to $N$ parameters.  The more parameters we include in our models, the more flexible they can become. But we must be careful of *overfitting*, of making models which are inaccurate because they don't use enough data to accurately estimate those parameters. The tension between overfitting and underfitting is central to the practice of data science.
<!-- HV: Where does this belong? -->

## Causal Effects of `treament`

<!-- This follows the discussion in chapter 8 very closely. See below for some key R code, which must be explained. (Obviously, you can use other code as well.) -->

```{r}
shaming %>% 
  lm(data = ., primary_06 ~ treatment - 1)
```

<!-- Talk about what these results mean.  Then, create the same model but with a different structure. -->


```{r}
shaming %>% 
  lm(data = ., primary_06 ~ treatment)
```

<!-- Explain how these two models are the same, except in how they define the parameters. Show us the math like Gelman does. Write down the math. For simple. -->


```{r}
shaming %>% 
  lm(data = ., primary_06 ~ treatment - 1) %>% 
  tidy(conf.int = TRUE, conf.level = 0.99) %>% 
  select(term, conf.low, estimate, conf.high)
```


<!-- Once we talk about these things --- and, again, this is exactly what we have talked about in chapter 8 --- we can do a bit more. Like discuss how we are using 99%, because there is nothing magical aboyt 95%, other than convention. I also think it would be fun to show a nice graphic of this, highlighting how the estimates for Civic and Hawthorne overlap.  -->


### Interactions

<!-- This is new. With only two parameters, we can't really look at interaction effects. Need to discuss interaction effects in general. Also, note that heterogenous treatment effects are the same thing as interaction effects that involve a treatment effect as one of the variables.  -->

<!-- Feel free to build up this code, and other examples, more slowly than I am doing it here. -->

```{r}
shaming %>% 
  lm(data = ., primary_06 ~ sex*treatment - 1) %>% 
  tidy() %>% 
  select(term, estimate)
```

<!-- Takes a while to explain what all this means. -->




<!-- Two key issues: 1) Interpreting lots of parameters in a model. interactions, heterogenous treatment effects. shaming using lm(). 




2) Estimate support for the president among small demographic. How do female, Hispanics in Alabama feel about Obama? You don't want to just use the overall mean, which would underfitting. You don't want to use the mean for just that small group, which would be overfitting.  Using nes data, for 2012, approval for Obama and stan_glm().  -->


<!-- 
intercept

Interactions --- use: income ~ party*something

heterogeneous treatment effects --- use:  att_start ~ treatment*something 
just a fancy way of saying interaction effects, but with a variable which us causal


What problems do we face? All the things that make modeling difficult. Why is this so hard? -->

<!-- Centering. -->

<!-- Might naively just take the value for each bucket. But that overfits! Need to put down some structure, like ordering. -->

<!-- income category, party id, pooling, age, -->

<!-- overfitting/underfitting bias/variance -->

<!-- We must have left bootstrapping behind by now. No more bootstraps, at least for the purpose of calculating uncertainty. (We will use it later for the purpose of out-of-sample testing and avoiding overfitting.) Key lesson is that overfitting is easy. You can't just estimate a value for each cell. You need to smooth and borrow strength. Of course, the only way to do that is with a Bayesian approach. Right? We don't want to get into hacked likelihood approaches. -->

<!-- cces looks perfect for this project. There are 400,000 rows, so it seems like you ought to have plenty of data for anything you want, but once you realize there are 51 states and 10 years, things get sparse fast. We only have 15 observations, for example, for Wyoming in 2007. Once you start subsetting by race and education, you have no choice but to start borrowing strength.  -->

<!-- So, just what will we use? rstanarm(). If so (and if we have not introduced it earlier), we can begin with seeing how it is similar to lm() and then expand. This means that, in one paramter chapter, we should be doing lm( ~ 1). In two parameter, lm( ~ treatment) --- if treatment is zero one --- or, perhaps better, lm( ~ -1 + treatment) if treatment is a factor/character with two levels. We might also have introduced  -->


## Formatting Linear Models in R
In the previous chapter, linear models were created using the general format of `dependent variable ~ -1 + independent variable`. After running the regression on a model of this type, the resulting table would show coefficients for both parameters of the independent variable, for example Democrat and Republican. This method works fine when there are only one or two parameters in consideration, however now we will be moving onto *n* parameters and we will have to change the formula we use; specifically, we will be removing the -1 from the lm formula.

In order to remove the -1, we first have to understand the function of it in the linear regression formulas that you have seen previously. Using the Enos (2014) `trains` data, when we use -1 in our formula and regress party on income, the table output lists both parameters and estimates their mean income values.

```{r}

lm(data = trains, income ~ -1 + party) %>% tidy(conf.int = TRUE)

```
<!-- HV: Is there a way to make this output neater in the knit document? More of a typical regression table? -->
<!-- HV: Should I start off introducing rstanarm and use stan_glm for all models in this chapter? -->

This table tells us that Democrats have an average income of $136,755.20, while Republicans have an average income of $167,368.40. 

Once we remove the -1 from the formula, the the regression table replaces one of the parameter terms with "(Intercept)". For example, if we use the same regression as above but remove the -1, the output table changes to this:
```{r}

lm(data = trains, income ~ party) %>% tidy(conf.int = TRUE)

```

Although the coefficients in this regression table are different than the ones above, the interpretation stays the same. The intercept still represents the average income of democrats, the default parameter, however the partyRepulican coefficient is now the difference between the mean income of republicans and democrats. The value of the mean republican income remains the same, however, but it is now calculated by adding the partyRepublican coefficient to the intercept estimate.

```{r}
136755.21 + 30613.21
```


## Adding parameters
We will now begin adding parameters to our regression models. The simplest way to do so is by adding another independent variable to the regression formula. For example, we can add gender to the same formula we used previously.

```{r}

lm(data = trains, income ~ party + gender) %>% tidy(conf.int = TRUE)

```

### Understanding the Model
Models with multiple predictors can become complicated to interpret, since the interpretation of a coefficient somewhat relies on the other variables of the model. The model we have just created summarizes the difference in average income of subjects based on both their political party affiliation and their gender. Using just these two predictors and their coefficients, we have three parameters to describe four types of people: democratic females, democratic males, republican females, and republican males.

We can start off by converting the resulting output into mathematical notation using the simple linear regression equation of	$Y =\beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \epsilon $. Using this notation, this model could be written as Income = 121086.46 + 31621.14*Republican + 27855.54*Male. Both of these predictors, party and gender, are binary and given the output coefficient terms, if a person is Republican it is coded as 1, whereas if they are a Democrat it is coded as 0. The same intuition applies to gender, with male coded as 1 and female coded as 0. Now, we must interpret this model.

*Intercept*: If we think about the intercept in terms of the linear equation we formulated above, it is the value of Y when x is zero, or the income when Republican and Male are both coded zero. Therefore in this model, the intercept represents female democrats and tells us that the mean income for female democrats is $121,086.46. 

*Party coefficient*: Alone, this variable compares the incomes of people with different political affiliations. It tells us that Republicans have a mean income that is $31,621.14 more than Democrats. If we look at the model altogether and want the mean income for a female republican, we would add the partyRepublican coefficient to the intercept estimate by coding a 1 to the Republican variable. This gives us a mean income of $152,707.60 for female republicans, compared to the mean income of $121,086.46 for female democrats.

*Gender coefficient*: The genderMale coefficient tells us the difference between the income of a female and a male. If we take out the partyRepublican term, the genderMale coefficient looks specifically at the difference in income of a female democrat versus a male democrat. By adding this coefficient to the intercept value, we find that male democrats have a mean income of $148,942, or $27,855.54 more than female democrats. 

*Using both coefficients*: In order to find the mean income of a male republican, we would have to code a 1 to both variables which would lead us to add both the partyRepublican coefficient *and* the genderMale coefficient to the intercept, since both parameters apply. From this we find the mean income of a republican male is $180,563.10. 
<!-- Fix choppy writing in this whole section. Maybe change formatting. -->

It is important to note, however, that adding predictors brings about a range of complexities, including which predictors to omit and which to include, interpretations of multiple coefficients, and the uncertainty associated with our models.

## Interaction terms
Another way to add parameters to a linear regression formula is by incorporating interaction terms. 



## heterogeneous treatment effect
fancy way of saying interactions but with a variable that you believe is causal 



## 5 sources of uncertainty
style.Rmd

#### 3. Parameter Uncertainty
estimate of income coefficient
confidence interval gives the range of uncertainty
  - when dealing with a new observation, it is not the confidence interval
      - this is where you use posterior predict
      - difference between uncertainty about parameyer mean estimate but confidence interval for the mean is not equal to the estimate for the new observation

#### 4. Unmodeled Variation




## Rubin Causal Model
new female democrat shows up


## cces data

In this chapter we will be using the cces data, or Cooperative Congress Election Survey. The CCES is a 50,000+ person national stratified sample survey that consists of a pre- and post- election wave. In the pre-election wave, respondents complete two-thirds of the survey that asks about general political attitudes, various demographic factors, assessment of roll call voting choices, political information, and vote intentions. In the post-election wave, respondents complete the final third of the survey that consists mostly of items related to the election that just occurred.

Some of the key variables that are included in the data set are approval ratings of the elected officials, from the governor to the president, on a scale from "Strongly Disapprove" to "Strongly Approve". To quantify this data, the new variables `approval_pres_num`, `approval_rep_num`, `approval_sen1_num`, `approval_sen2_num`, and `approval_gov_num` have all been created and quantify the approval scale by ordering the responses from a 1 to a 5. Those who answered with "Strongly Disapprove" are a 1 on the approval scale, while those who answered with "Strongly Approve" are a 5 on the numerical approval scale. Those who are neutral answered with "Neither Approve Nor Disapprove" and are quantified as a 3 on the scale. Respondents who answered with "Never Heard / Not Sure" have been removed in order to improve the accuracy of the approval ratings.

```{r, include=FALSE}
ch9 <- PPBDS.data::cces



```


Other variables in the cces data include state, race, age, education level, gender, and ideology. The data taken spans from 2006 to 2018, although it should be noted that there are more observations in years with general elections.

## presidential approval; overall; by year; by state; by state x year x educ

## need rstanarm

## Rubin Causal Model

<!-- create numeric `rating` 1 to 4. Leave out Never heard. Might use percentage strongly approve. -->

<!-- discuss overall rating for entire date set. One parameter. Discuss. For each year. For each state. -->

<!-- basic lm -->

<!-- lm(data = cces, age ~ 1) %>% tidy(conf.int = TRUE) -->
<!-- lm(data = cces, age ~ -1 + state) %>% tidy(conf.int = TRUE) -->
<!-- lm(data = cces, age ~ -1 + as.factor(year)) %>% tidy(conf.int = TRUE) -->

<!-- obj <- lm(data = cces, age ~ -1 + state*as.factor(year)) %>% tidy(conf.int = TRUE) -->


<!-- Connecting parameters to real world concepts. What are we measuring? validity. -->

<!-- estimands -->




