---
output_yaml:
  - _output.yml
---

# Functions

<!-- 0. Add 30 list-column and mapping questions, generally with each set of questions grouped into a mini two or three question series. -->

<!-- 1. Make clear the mapping from chapter sections to tutorial panels. Fix the easily fixable from style.Rmd. -->

<!-- 2. Along the way, there are some key functions we want to teach students and this is as good a place as any to do it. Relevant posts (no need to read) include: -->

<!-- https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/ -->
<!-- https://aosmith.rbind.io/2018/06/05/a-closer-look-at-replicate-and-purrr/ -->

<!-- We need to explain how to use `runif()` and `rnorm()`, providing the actual formula for the normal distribution in the side margin, or perhaps just copying all the text/code from [ModernDive](https://moderndive.com/A-appendixA.html#appendix-normal-curve). We also need to teach them about `replicate()`. Neither of these truly fit here, but they don't fit anywhere else either. So, use these functions for as many of your examples as possible.  -->

<!-- Example: -->

<!-- tibble(id = 1:3) %>%  -->
<!--   mutate(draws = map(id, ~ rnorm(5))) %>% -->
<!--   mutate(big_minus_little = map_dbl(draws, ~ max(.) - min(.))) -->

<!-- Make sure to highlight how you need the map() to do the draws, otherwise you get the same five random numbers in each row. -->

<!-- That is not the fanciest thing, but many/most our examples, in the chapter and the tutorial, should make use of rnorm() and runif(). Besides being useful in themselves, we want to get students used to the idea of using simulations to answer questions. If I draw five standard normal numbers, what is the most likely value of the biggest of the 5 minus the smaller? -->

<!-- 3. Read the probability chapter in bok. Change the current example of a function which works with gapminder data to something else, maybe involving stuff from dice, coins and so on, setting the stage for chapter 5. The real key is that we want students to be very comfortable when we ask them to start constructing data generating mechanisms in chapters 7+. How do we set them up for success in that? By constructing some easy DGMs here! And some hard ones. Indeed, we could go from some simple to hard, discussing the trade-offs along the way. In this course, they won't ever (?) write a function which takes a tibble as an input. Or maybe they will --- if the results of some statistical procedure or simulation is in the form of a tibble. -->

<!-- 4. Some thoughts on DGMs in general. First, should they be designed to produce one result, and then be called with replicate to run again and again? Or should they take N as the first argument, so that they behave similarly to rnorm()? Second, what other arguments should they take? Simplest approach might be that they take none, that all the things needed are built into the function itself. Better (?) would be to, at least, pass in a new_data type argument, since that is so standard in functions like `posterior_predict()`. How are these issues handled in **tidymodels**? -->

<!-- 5. No scoping rules. -->


The goal of this chapter is to reveal the __process__ a long-time useR employs for writing functions. We also want to illustrate why the process is the way it is. Merely looking at the finished product, e.g. source code for R packages, can be extremely deceiving. Reality is generally much uglier ... but more interesting! Powerful packages like **dplyr** and **purrr** are ready and waiting to apply your purpose-built functions to various bits of your data. If you can express your analytic wishes in a function, these tools will give you great power.

## Introduction to Functions

First, let's introduce the concept of functions. A function is a piece of code that is packaged in a way that makes it easy to reuse. Functions make it easy for you to `filter()`, `sort()`, `select()`, and create a `data.frame()`, as you have seen in the last few chapters. Functions also allow you to transform variables and perform a bunch of useful mathematical calculations, such as finding the `min()`, `max()`, and `quantile()`s of a dataset. Functions are also important in generating data and values, such as using `rnorm()` to randomly generate a normal distribution.

Note that every time we mention a function, we include the parentheses. This is because you call a function by including its parentheses and any necessary arguments in those parentheses. If you run the function name without its parentheses, R will return the code that makes up the function. 

```{r echo = TRUE}
Sys.Date
```

```{r echo = TRUE}
Sys.Date()
```

Every R function has three parts:

* A body of code
* A set of formal arguments
* An environment (where the function will look up the values of the objects within it)

There are plenty of built-in functions in R, such as the ones mentioned baove. You can also create your own custom functions, which may look something like this:

```{r eval=FALSE}
my_function <- function(x) x + 1
```

This code creates a function named `my_function()` that takes in in a value `x` and return a value `x + 1`. Alternatively, you could leave the function unnamed, as such:

```{r eval=FALSE}
function(x) x + 1
```

These nameless functions are called [anonymous functions.](https://coolbutuseless.github.io/2019/03/13/anonymous-functions-in-r-part-1/) We can create functions that do operations "on the fly" without bothering to give them a name.

We will dive into functions more in depth soon, but next we will discuss of list-columns and `map_*` functions. 

## List-columns and `map_*` functions

We are now going to learn about [**list columns**](https://r4ds.had.co.nz/many-models.html#list-columns-1) and [**`map_*` functions**](https://jennybc.github.io/purrr-tutorial/ls01_map-name-position-shortcuts.html), powerful tools that we will be using throughout the book.^[If I were going to give a lecture on these topics, it would look like [this one](https://resources.rstudio.com/webinars/how-to-work-with-list-columns-garrett-grolemund).]


### What are list columns?

<!-- DK: maybe start with "What are lists?" The chapter from R4DS with the pepper shaker pictures is excellent. -->

First, we need to know: What are lists? Imagine a `jar` full of 10 individually wrapped candies, with a number on each wrapper representing its candy number.^[This example is directly inspired by R for Data Science's pepper shaker example, found [here](https://r4ds.had.co.nz/vectors.html)] In order to obtain the `jar` with the first individually wrapped candy inside, you would need `jar[1]`. Similarly, `jar[2]` will return the jar with only the second individually wrapped candy inside. Whenever you are "reaching" into the jar to grab one certain candy, that is called indexing. In R, you can use square brackets to index through lists.

What if you wanted the second individually wrapped candy, but without the jar? Then you would need `jar[[2]]`. And lastly, in order to obtain the second candy itself without its wrapped, you would need `jar[[2]][[1]]`, where the second set of square brackets, or the second *index*, "reaches" inside the jar and the wrapper to extract the candy. This unwrapped and extracted candy is called a list *element*.

Now that we understand what a list is, what is a list-column? A list column is a column of your data which is a [list](https://adv-r.hadley.nz/vectors-chap.html#lists) rather than an atomic vector. Atomic vectors are familiar to us: each element of the vector has one value, and thus if an atomic vector is a column in your dataset, each observation gets a single value.  Lists, however, can contain vectors as elements. The best way to understand this is by creating a list column yourself and inspecting the results.  `str()` is a helpful function for looking at the contents of a tibble with list columns:

```{r simple_list_col, message=FALSE}
library(tidyverse)
library(gapminder)

tibble(new_col = list(1:2, 3:5)) %>%
  str()
```

Here we see that our tibble has one column (`new_col`) and one observation, which is a list with two elements.  The first element in the list is a vector of the integers 1 and 2 and the second element is a vector of the integers 3, 4, and 5. The tibble only has one row because there is only one value for `new_col`. 

Note that this is a case where it is crucial to use `tibble()`, not `data.frame()`!  If we had used `data.frame()` in the last example, it wouldn't have worked:

```{r data_frame, error = TRUE}
data.frame(new_col = list(1:2, 3:5)) %>%
  str()
```

Lists are very flexible.  For example, each element of a list can have its own data type:

```{r mulitple_types}
tibble(new_col = list(1:2,
                      c("Alice", "Bob"))) %>%
  str()
```

Now the first element consists of the integers 1 and 2 while the second is a character vector containing "Alice" and "Bob".

We wrapped "Alice" and "Bob" in `c()` in order to make it clear that this is a vector.  If we hadn't done that, we would have three elements in our list: the vector with 1 and 2, "Alice", and "Bob":

```{r no_c}
tibble(new_col = list(1:2,
                      "Alice",
                      "Bob")) %>%
  str()
```

### Creating list columns with `mutate()`

Any function that returns multiple values can be used to create a list column.  For example, consider the following tibble:

```{r alice_and_bob, eval = FALSE}
tibble(col_1 = c("Alice and Bob", "Carol and Dan"))
```

`col_1` is a character vector with two observations: "Alice and Bob" and "Carol and Dan". The tibble has two rows. It may be more useful to present these as vectors of names, without the annoying "and" in between.  That's exactly what we can do with `str_split()`:

```{r function_list_col}
tibble(col_1 = c("Alice and Bob", "Carol and Dan")) %>%
  mutate(col_2 = str_split(col_1, " and ")) %>%
  str()
```

After using `str_split()` within `mutate()`, we have created a new column, and that new column is a list column. 

This is often how we will go about creating list columns.  Let's practice with the `gapminder` dataset.  How could we add a column to the dataset that included the quantiles of the `lifeExp` variable?

```{r gapminder_list_column}
gapminder %>%
  group_by(year) %>%
  mutate(lifeExpQuantile = list(quantile(lifeExp)))
```

*Note*: `str_split()` was a particularly easy function for using with `mutate()` and `summarize()` because it returns a list.  You can check this by running `typeof()` on the output of `str_split()`: e.g., `str_split("Alice and Bob", " and ") %>% typeof()`.  If a function returns multiple values as a vector, like `quantile()` does, you can't use it directly in `mutate()` or `summarize()`, but you can wrap `list()` around it in order to get the same behavior.

<!-- DK: The above bit should be explained at the beginning when we review lists. -->

Or let's say that we wanted 1) to subset the dataset to the most recent year, 2) group by continent, and 3) get a `summary()` of the `gdpPercap` variable by continent:

```{r continent_summary}
gapminder %>%
  filter(year == max(year)) %>%
  group_by(continent) %>%
  summarize(gdpSummary = list(summary(gdpPercap)))
```

### `map_*` functions

What can we do with a list column? That is tricky! Lists are hard to work with. But the most natural thing to do with a list is to feed it to a `map_*` function.  What are these functions?

Let's say that you want to find the square root of a variable in a tibble, storing the result in `x`.  You could accomplish this with `mutate`:

```{r for_loop_example}
tibble(start = c(3, 2.5, 6)) %>% 
  mutate(new = sqrt(start))

```

Or, we can use `map_dbl()` to apply this to our input variable.  `map_dbl()` --- pronounced "map double" --- comes from the **purrr** package, which you will have loaded if you have loaded **tidyverse**.  `map_dbl()` is just one member of a family of `map_*` functions which applies the same function to every row in a tibble. The "dbl" suffix indicates that the function returns a "double," meaning a numeric value.


```{r map_simple}
tibble(start = c(3, 2.5, 6)) %>% 
  mutate(new = map_dbl(start, sqrt))
```

`map_dbl()` took the function `sqrt()` and applied it to each element of `start`. Note that, when we passed the function `sqrt()` to `map_dbl()`, we passed in just its name, without the closing parentheses. This code fails:

```{r, error=TRUE}
tibble(start = c(3, 2.5, 6)) %>% 
  mutate(new = map_dbl(start, sqrt()))
```

`sqrt()` with the parentheses is a call to the function. So, the first thing R tries to do is to run the function. Doing so fails because `sqrt()` requires an argument `x`, which is empty. 

```{marginfigure, echo = TRUE, fig.margin = TRUE}
Lesson: When passing a function in to `map_*` functions, pass just the name.
```

We called these `map_*` *functions* (plural) before.  If you know the expected output of your function, you can specify that kind of vector:

- `map()`: list  
- `map_lgl()`: logical
- `map_int()`: integer
- `map_dbl()`: double (numeric)
- `map_chr()`: character
- `map_df()`: data frame

So, since our example produces numeric output, we use `map_dbl()` instead of `map()`.

Now, you may be wondering, what's the difference between using `mutate()` and `map_*` functions? After all, the example above demonstrated a use case in which both were used interchangeably. The answer lies in iteration, which is the specialization of the **purrr** package. `map_*` functions are powerful because of their ability to create list-columns and to apply functions to every single element of a list, which `mutate()` cannot handle. We'll show you exactly what we mean in the next example.

### Using `map_*` functions to create list-columns

For this example, we will use the `nhanes` dataset from the `PPBDS.data` package to demonstrate how to use `map_*` functions to create list columns.

First, let's wrangle the data so each observation is grouped by age and gender.  We'll create a list column `all_weights` that consists of every subject's weight, grouped by age and gender.

```{r eval = FALSE, message=FALSE}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight))
```

Now that we have a list column, we can use it as the input to `map()`, outputting another list column.  Let's say we wanted a new list column, `log_weights`, which takes the logarithmic value of each weight observation.

```{r eval = FALSE, message=FALSE}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight)) %>%
  mutate(log_weights = map(all_weights, log))
```

Note that we took the list column `all_weights` and, by applying an anonymous function to it with `map()`, created another list column `log_weights`.  This is a very common process.  It is similar to taking a tibble and piping it into a `dplyr` function (such as `mutate()`) which gives you a new tibble that you can work with.

If we try to replace the `map()` function in the last line with a simple `mutate()`, we will get an error.

```{r echo=FALSE, error=TRUE}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight)) %>%
  mutate(log_weights = log(all_weights))
```

Again, this is because the `mutate()` function is ill-equipped to handle lists. We could have avoided creating the list-columns in the first place and found the log weight of each observation, but then we would be losing out on the grouping ability of lists. Do you see how `map_*` functions are equipped to handle inputs different from `mutate()`?

You can also use `map_*` functions to take a list column as an input and return an atomic vector -- a column with a single value per observation -- as an output.  For instance, let's say we now wanted the mean of the log weights:

```{r eval = FALSE, message=FALSE}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight)) %>%
  mutate(log_weights = map(all_weights, log),
         mean_log_weights = map_dbl(log_weights, mean, na.rm = TRUE))
```

Here, we also see that the `map_*` functions have the `...` argument, which allows `na.rm = TRUE` to be passed along to `mean()`.

What if we wanted to extract the top five log weights per row?

```{r top_5, eval = FALSE}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight)) %>%
  mutate(log_weights = map(all_weights, log),
         sorted_log_weights = map(log_weights, ~ sort(., decreasing = TRUE)),
         top5_log_weights = map(sorted_log_weights, ~ .[1:5]))
```

See how we chained the `map_*` functions:

1) `all_weights` was used as the input to `map()` to create `log_weights`
2) `log_weights` was used as the input to `map()` to create `sorted_log_weights`
3) `sorted_log_weights` was used as the input to `map()` to create `top5_log_weights`

Later on, we will practice using map functions to create list columns with custom functions, not just built-in R functions. You will see that `map_*` functions are highly versatile!


<!-- This is where we need to fill in the gap between map functions and functions in general -->

## Built-In Functions and Custom Functions

Load gapminder.

```{r start_func1}
library(gapminder)
str(gapminder)
```

Say you've got a numeric vector, and you want to compute the difference between its max and min. `lifeExp` or `pop` or `gdpPercap` are great examples of a typical input. You can imagine wanting to get this statistic after we slice up the Gapminder data by year, country, continent, or combinations thereof.

### Get something that works

First, develop some working code for interactive use, using a representative input. Use Gapminder's life expectancy variable. R functions that will be useful: `min()`, `max()`, `range()`. 

```{r}
# Get to know the functions mentioned above

min(gapminder$lifeExp)
max(gapminder$lifeExp)
range(gapminder$lifeExp)

# Some natural solutions

max(gapminder$lifeExp) - min(gapminder$lifeExp)
with(gapminder, max(lifeExp) - min(lifeExp))
range(gapminder$lifeExp)[2] - range(gapminder$lifeExp)[1]
with(gapminder, range(lifeExp)[2] - range(lifeExp)[1])
diff(range(gapminder$lifeExp))
```

Internalize this "answer" because our informal testing relies on you noticing departures from this.

### Skateboard >> perfectly formed rear-view mirror

This image --- widely attributed to the Spotify development team --- conveys an important point.

```{r spotify-howtobuildmvp, echo = FALSE, out.width = "60%", fig.cap = "From [Your ultimate guide to Minimum Viable Product (+great examples)](https://blog.fastmonkeys.com/2014/06/18/minimum-viable-product-your-ultimate-guide-to-mvp-great-examples/)"}
knitr::include_graphics("04-functions/images/mvp.jpg")
```

Build that skateboard before you build the car or some fancy car part. A limited-but-functioning thing is very useful. It also keeps the spirits high.

This is related to the valuable Telescope Rule:

> It is faster to make a four-inch mirror and then a six-inch mirror than it is to make a six-inch mirror.

### Turn the working interactive code into a function

Add NO new functionality! Just write your very first R function.

```{r}
max_minus_min <- function(x) max(x) - min(x)
max_minus_min(gapminder$lifeExp)
```

Check that you're getting the same answer as you did with your interactive code. Test it eyeball-o-metrically at this point.

One thing to note about functions is that functions have selective memory. In fact, functions only "remember" what happens within the function itself.

For example, I have saved value `x` as 1.

```{r}
x <- 1
```

Defining `x` inside a function will not change its value outside of the function.

```{r}
function(x) x <- 2

x
```

### Test your function

#### Test on new inputs

Pick some new artificial inputs where you know (at least approximately) what your function should return.

```{r}
max_minus_min(1:10)
max_minus_min(runif(1000))
```

I know that 10 minus 1 is 9. I know that random uniform [0, 1] variates will be between 0 and 1. Therefore max - min should be less than 1. If I take LOTS of them, max - min should be pretty close to 1.

It is intentional that I tested on integer input as well as floating point. Likewise, I like to use valid-but-random data for this sort of check.

#### Test on real data but *different* real data

Back to the real world now. Two other quantitative variables are lying around: `gdpPercap` and `pop`. Let's have a go.

```{r}
max_minus_min(gapminder$gdpPercap)
max_minus_min(gapminder$pop)
```

Either check these results "by hand" or apply the "does that even make sense?" test.

#### Test on weird stuff

Now we try to break our function. Don't get truly diabolical (yet). Just make the kind of mistakes you can imagine making at 2am when, 3 years from now, you rediscover this useful function you wrote. Give your function inputs it's not expecting.

```{r error = TRUE}
max_minus_min(gapminder)

# Hey, sometimes things "just work" on data.frames!

max_minus_min(gapminder$country)

# Factors are kind of like integer vectors, no?

max_minus_min("eggplants are purple")

# I have no excuse for this one
```

How happy are you with those error messages? You must imagine that some entire __script__ has failed and that you were hoping to just `source()` it without re-reading it. If a colleague or future you encountered these errors, do you run screaming from the room? How hard is it to pinpoint the usage problem?


#### I will scare you now

Here are some great examples where the function __should break but it does not.__

```{r}
max_minus_min(gapminder[c('lifeExp', 'gdpPercap', 'pop')])
max_minus_min(c(TRUE, TRUE, FALSE, TRUE, TRUE))
```

In both cases, R's eagerness to make sense of our requests is unfortunately successful. In the first case, a tibble containing just the quantitative variables is eventually coerced into numeric vector. We can compute max minus min, even though it makes absolutely no sense at all. In the second case, a logical vector is converted to zeroes and ones, which might merit an error or at least a warning.

### Check the validity of arguments

For functions that will be used again -- which is not all of them! -- it is good to check the validity of arguments. This implements a rule from the Unix philosophy:

> Rule of Repair: When you must fail, fail noisily and as soon as possible.

#### stop if not

`stopifnot()` is the entry level solution. We use it here to make sure the input `x` is a numeric vector.

```{r error = TRUE}
mmm <- function(x) {
  stopifnot(is.numeric(x))
  max(x) - min(x)
}
mmm(gapminder)
mmm(gapminder$country)
mmm("eggplants are purple")
mmm(gapminder[c('lifeExp', 'gdpPercap', 'pop')])
mmm(c(TRUE, TRUE, FALSE, TRUE, TRUE))
```

And we see that it catches all of the self-inflicted damage we would like to avoid.

#### if then stop

`stopifnot()` doesn't provide a very good error message. The next approach is very widely used. Put your validity check inside an `if()` statement and call `stop()` yourself, with a custom error message, in the body.

```{r error = TRUE}
mmm2 <- function(x) {
  if(!is.numeric(x)) {
    stop('I am so sorry, but this function only works for numeric input!\n',
         'You have provided an object of class: ', class(x)[1])
  }
  max(x) - min(x)
}
mmm2(gapminder)
```

In addition to a gratuitous apology, the error also contains two more pieces of helpful info:
  
* *Which* function threw the error.
* Hints on how to fix things: expected class of input vs actual class.

If it is easy to do so, we highly recommend this template: "you gave me THIS, but I need THAT".

The tidyverse style guide has a very useful [chapter on how to construct error messages](https://style.tidyverse.org/error-messages.html).

#### Sidebar: non-programming uses for assertions

Another good use of this pattern is to leave checks behind in data analytical scripts. Consider our repetitive use of Gapminder in this book. Every time we load it, we inspect it, hoping to see the usual stuff. If we were loading from file (vs. a stable data package), we might want to formalize our expectations about the number of rows and columns, the names and flavors of the variables, etc. This would alert us if the data suddenly changed, which can be a useful wake-up call in scripts that you re-run *ad nauseam* on auto-pilot or non-interactively.

### Using functions to generate data randomly

<!-- EC:  Added section using rnorm(), runif(), sample(), and replicate(). Review?-->

As data scientists, there are many reasons to create your own data. Perhaps you'd like to run a simulation as a baseline for comparison to another dataset that contains observed values. Or, perhaps you want to create a quick "fake" dataset to model real world phenomena. Either way, you will realize that at one point, you would like to generate data, which will induce some element of *randomness*. See, there is no point in manually creating a dataset in which you already *know* what every observation looks like. Real life is messy! Next, we will be introducing four functions that can be used to generate data ranomdly.

 If we really want to simulate the discrete uniform distribution of dice rolling, there is a better function to do so: `sample()`.

#### `sample()` and `replicate()`

`sample()` takes a vector of values and spits out a number of random values from that vector. You can specify the number of random values with the argument `size`. Let's take it for a spin.

```{r echo=TRUE}
sample(x = 1:6, size = 1)
```

What if we want to roll the dice 3 times? Make sure that the `replace` argument is set to `TRUE`. This will ensure that each number is able to be rolled, regardless of what the previous rolls resulted in.

```{r echo=TRUE}
sample(x = 1:6, size = 3, replace = TRUE)
```

Perfect! The last function we want to introduce is `replicate()`, which interacts really well with the above three functions. `replicate()` takes an expression and repeats it for `n` specified times. This is really useful when you want to generate many iterations of data. What if we replicated the rolling of two dice ten times?

```{r echo=TRUE}
replicate(10, sample(1:6, 2))
```

As you can see, the default output is an array.

```{r echo=TRUE}
replicate(10, sample(1:6, 2), simplify = FALSE)
```

Setting `simplify=FALSE` turns it into a list of `n = 10` elements.

### Wrap-up and what's next?

Here's the function we've written so far:

```{r end_func1}
mmm2
```

What we've accomplished:

* We've written our first function.
* We are checking the validity of its input, argument `x`.
* We've done a good amount of informal testing.
* We have been introduced to data generating functions.
  
## Argument Specifications and Default Values

In part 1, we wrote our first R function to compute the difference between the max and min of a numeric vector. We checked the validity of the function's only argument and, informally, we verified that it worked pretty well.

In this part, we generalize this function, learn more technical details about R functions, and set default values for some arguments.

### Load the Gapminder data

Load gapminder.

```{r start_func2}
library(gapminder)
```

### Restore our max minus min function

Let's keep our previous function around as a baseline.

```{r}
mmm <- function(x) {
  stopifnot(is.numeric(x))
  max(x) - min(x)
}
```

### Generalize our function to other quantiles

The max and the min are special cases of a __quantile__. Here are other special cases you may have heard of:

* median = 0.5 quantile
* 1st quartile = 0.25 quantile
* 3rd quartile = 0.75 quantile
  
If you're familiar with [box plots][wiki-boxplot], the rectangle typically runs from the 1st quartile to the 3rd quartile, with a line at the median.

If $q$ is the $p$-th quantile of a set of $n$ observations, what does that mean? Approximately $pn$ of the observations are less than $q$ and $(1 - p)n$ are greater than $q$. Yeah, you need to worry about rounding to an integer and less/greater than or equal to, but these details aren't critical here.

Let's generalize our function to take the difference between any two quantiles. We can still consider the max and min, if we like, but we're not limited to that.

### Get something that works, again

The eventual inputs to our new function will be the data `x` and two probabilities.

First, play around with the `quantile()` function. Convince yourself you know how to use it, for example, by cross-checking your results with other built-in functions.

```{r}
quantile(gapminder$lifeExp)
quantile(gapminder$lifeExp, probs = 0.5)
median(gapminder$lifeExp)
quantile(gapminder$lifeExp, probs = c(0.25, 0.75))
boxplot(gapminder$lifeExp, plot = FALSE)$stats
```

Now write a code snippet that takes the difference between two quantiles.

```{r}
the_probs <- c(0.25, 0.75)
the_quantiles <- quantile(gapminder$lifeExp, probs = the_probs)
max(the_quantiles) - min(the_quantiles)
```

### Turn the working interactive code into a function, again

Use `qdiff` as the base of our function's name. We copy the overall structure from our previous "max minus min" work but replace the guts of the function with the more general code we just developed.

```{r}
qdiff1 <- function(x, probs) {
  stopifnot(is.numeric(x))
  the_quantiles <- quantile(x = x, probs = probs)
  max(the_quantiles) - min(the_quantiles)
}
qdiff1(gapminder$lifeExp, probs = c(0.25, 0.75))
IQR(gapminder$lifeExp)

# Hey, we've reinvented IQR

qdiff1(gapminder$lifeExp, probs = c(0, 1))
mmm(gapminder$lifeExp)
```

Again we do some informal tests against familiar results and external implementations.

### Argument names: freedom and conventions

Understand the importance of argument names. We can name my arguments almost anything we like. Proof:

```{r}
qdiff2 <- function(zeus, hera) {
  stopifnot(is.numeric(zeus))
  the_quantiles <- quantile(x = zeus, probs = hera)
  max(the_quantiles) - min(the_quantiles)
}
qdiff2(zeus = gapminder$lifeExp, hera = 0:1)
```

While we can name my arguments after Greek gods, it's usually a bad idea. Take all opportunities to make things more self-explanatory via meaningful names.

If you are going to pass the arguments of your function as arguments of a built-in function, consider copying the argument names. Unless you have a good reason to do your own thing (some argument names are bad!), be consistent with the existing function. Again, the reason is to reduce your cognitive load. 

```{r}
qdiff1
```

We took this detour so you could see there is no *structural* relationship between our arguments (`x` and `probs`) and those of `quantile()` (also `x` and `probs`). The similarity or equivalence of the names __accomplishes nothing__ as far as R is concerned; it is solely for the benefit of humans reading, writing, and using the code. Which is very important!

### What a function returns

By default, a function returns the result of the last line of the body. We are just letting that happen with the line `max(the_quantiles) - min(the_quantiles)`. However, there is an explicit function for this: `return()`. We could just as easily make this the last line of my function's body:

```{r eval = FALSE}
return(max(the_quantiles) - min(the_quantiles))
```

You absolutely must use `return()` if you want to return early based on some condition, i.e. before execution gets to the last line of the body. Otherwise, you can decide your own conventions about when you use `return()` and when you don't.

### Default values: freedom to NOT specify the arguments

What happens if we call our function but neglect to specify the probabilities?

```{r error = TRUE}
qdiff1(gapminder$lifeExp)
```

Oops! At the moment, this causes a fatal error. It can be nice to provide some reasonable default values for certain arguments. In our case, it would be crazy to specify a default value for the primary input `x`, but very kind to specify a default for `probs`.

We started by focusing on the max and the min, so I think those make reasonable defaults. Here's how to specify that in a function definition.

```{r}
qdiff3 <- function(x, probs = c(0, 1)) {
  stopifnot(is.numeric(x))
  the_quantiles <- quantile(x, probs)
  max(the_quantiles) - min(the_quantiles)
}
```

Again we check how the function works, in old examples and new, specifying the `probs` argument and not.

```{r}
qdiff3(gapminder$lifeExp)
mmm(gapminder$lifeExp)
qdiff3(gapminder$lifeExp, c(0.1, 0.9))
```

### Wrap-up and what's next?

Here's the function we've written so far:

```{r end_func2}
qdiff3
```

What we've accomplished:

* We've generalized our first function to take a difference between arbitrary quantiles.
* We've specified default values for the probabilities that set the quantiles.
  
## Formal Testing

In part 2 we generalized our first R function so it could take the difference between any two quantiles of a numeric vector. We also set default values for the underlying probabilities, so that, by default, we compute the max minus the min.

In this part, we tackle `NA`s, the special argument `...` and formal testing.

### Load the Gapminder data

Load gapminder.

```{r start_func3}
library(gapminder)
```

### Restore our max minus min function

Let's keep our previous function around as a baseline.

```{r}
qdiff3 <- function(x, probs = c(0, 1)) {
  stopifnot(is.numeric(x))
  the_quantiles <- quantile(x, probs)
  max(the_quantiles) - min(the_quantiles)
}
```

### Be proactive about `NA`s

We are being gentle by letting you practice with the Gapminder data. In real life, missing data will make your life a living hell. If you are lucky, it will be properly indicated by the special value `NA`, but don't hold your breath. Many built-in R functions have an `na.rm =` argument through which you can specify how you want to handle `NA`s. Typically the default value is `na.rm = FALSE` and typical default behavior is to either let `NA`s propagate or to raise an error. Let's see how `quantile()` handles `NA`s:

```{r error = TRUE}
z <- gapminder$lifeExp
z[3] <- NA
quantile(gapminder$lifeExp)
quantile(z)
quantile(z, na.rm = TRUE)
```

So `quantile()` simply will not operate in the presence of `NA`s unless `na.rm = TRUE`. How shall we modify our function?

If we wanted to hardwire `na.rm = TRUE`, we could. Focus on our call to `quantile()` inside our function definition.

```{r}
qdiff4 <- function(x, probs = c(0, 1)) {
  stopifnot(is.numeric(x))
  the_quantiles <- quantile(x, probs, na.rm = TRUE)
  max(the_quantiles) - min(the_quantiles)
}
qdiff4(gapminder$lifeExp)
qdiff4(z)
```

This works but it is dangerous to invert the default behavior of a well-known built-in function and to provide the user with no way to override this.

We could add an `na.rm =` argument to our own function. We might even enforce our preferred default -- but at least we're giving the user a way to control the behavior around `NA`s.

```{r error = TRUE}
qdiff5 <- function(x, probs = c(0, 1), na.rm = TRUE) {
  stopifnot(is.numeric(x))
  the_quantiles <- quantile(x, probs, na.rm = na.rm)
  max(the_quantiles) - min(the_quantiles)
}
qdiff5(gapminder$lifeExp)
qdiff5(z)
qdiff5(z, na.rm = FALSE)
```

### The useful but mysterious `...` argument

You probably could have lived a long and happy life without knowing there are at least 9 different algorithms for computing quantiles. [Go read about the `type` argument][rdocs-quantile] of `quantile()`. TLDR: If a quantile is not unambiguously equal to an observed data point, you must somehow average two data points. You can weight this average different ways, depending on the rest of the data, and `type =` controls this.

Let's say we want to give the user of our function the ability to specify how the quantiles are computed, but we want to accomplish with as little fuss as possible. In fact, we don't even want to clutter our function's interface with this! This calls for the very special `...` argument. In English, this set of three dots is frequently called an "ellipsis".

```{r}
qdiff6 <- function(x, probs = c(0, 1), na.rm = TRUE, ...) {
  the_quantiles <- quantile(x = x, probs = probs, na.rm = na.rm, ...)
  max(the_quantiles) - min(the_quantiles)
}
```

The practical significance of the `type =` argument is virtually nonexistent, so we can't demo with the Gapminder data. Thanks to [\@wrathematics][twitter-wrathematics], here's a small example where we can (barely) detect a difference due to `type`.

```{r}
set.seed(1234)
z <- rnorm(10)
quantile(z, type = 1)
quantile(z, type = 4)
all.equal(quantile(z, type = 1), quantile(z, type = 4))
```

Now we can call our function, requesting that quantiles be computed in different ways.

```{r}
qdiff6(z, probs = c(0.25, 0.75), type = 1)
qdiff6(z, probs = c(0.25, 0.75), type = 4)
```

While the difference may be subtle, __it's there__. Marvel at the fact that we have passed `type = 1` through to `quantile()` *even though it was not a formal argument of our own function*.

The special argument `...` is very useful when you want the ability to pass arbitrary arguments down to another function, but without constantly expanding the formal arguments to your function. This leaves you with a less cluttered function definition and gives you future flexibility to specify these arguments only when you need to.

You will also encounter the `...` argument in many built-in functions -- read up on [`c()`][rdocs-c] or [`list()`][rdocs-list] -- and now you have a better sense of what it means. It is not a breezy "and so on and so forth."

There are also downsides to `...`, so use it with intention. In a package, you will have to work harder to create truly informative documentation for your user. Also, the quiet, absorbent properties of `...` mean it can sometimes silently swallow other named arguments, when the user has a typo in the name. Depending on whether or how this fails, it can be a little tricky to find out what went wrong.

The [ellipsis package](https://ellipsis.r-lib.org) provides tools that help package developers use `...` more safely. The in-progress tidyverse principles guide provides further guidance on the design of functions that take `...` in [Data, dots, details](https://principles.tidyverse.org/dots-position.html).

### Use testthat for formal unit tests

Until now, we've relied on informal tests of our evolving function. If you are going to use a function a lot, especially if it is part of a package, it is wise to use formal unit tests.

The [testthat][testthat-web] package ([CRAN][testthat-cran]; [GitHub][testthat-github]) provides excellent facilities for this, with a distinct emphasis on automated unit testing of entire packages. However, we can take it out for a test drive even with our one measly function.

We will construct a test with `test_that()` and, within it, we put one or more *expectations* that check actual against expected results. You simply harden your informal, interactive tests into formal unit tests. Here are some examples of tests and indicative expectations.

```{r eval = FALSE}
library(testthat)

test_that('invalid args are detected', {
  expect_error(qdiff6("eggplants are purple"))
  expect_error(qdiff6(iris))
})

test_that('NA handling works', {
  expect_error(qdiff6(c(1:5, NA), na.rm = FALSE))
  expect_equal(qdiff6(c(1:5, NA)), 4)
})
```

No news is good news! Let's see what test failure would look like. Let's revert to a version of our function that does no `NA` handling, then test for proper `NA` handling. We can watch it fail.

```{r end_func3, eval = FALSE}
qdiff_no_NA <- function(x, probs = c(0, 1)) {
  the_quantiles <- quantile(x = x, probs = probs)
  max(the_quantiles) - min(the_quantiles)
}

test_that('NA handling works', {
  expect_that(qdiff_no_NA(c(1:5, NA)), equals(4))
})
```

Similar to the advice to use assertions in data analytical scripts, I recommend you use unit tests to monitor the behavior of functions you (or others) will use often. If your tests cover the function's important behavior, then you can edit the internals freely. You'll rest easy in the knowledge that, if you broke anything important, the tests will fail and alert you to the problem. A function that is important enough for unit tests probably also belongs in a package, where there are obvious mechanisms for running the tests as part of overall package checks.

## Using custom and anonymous functions with `map_*` functions and list-columns

### Anonymous functions using `map_*` functions

There are two ways to create anonymous functions.  The first one, using base R, uses `function()` to create a function within `map_dbl()`.  

```{r}
tibble(start = c(3, 2.5, 6)) %>% 
  mutate(new = map_dbl(start, function(x) x + 1))
```

The second, using the **purrr** package (from where we get `map_dbl()`), starts with the `~` operator and then uses `.` to represent the current element.

```{r}
tibble(start = c(3, 2.5, 6)) %>% 
  mutate(new = map_dbl(start, ~ (. + 1)))
```

Note that the parantheses are not necessary. As long as everything after the `~` works as R code, the anonymous function should work, each time replace the `.` with the value of the `.x` variable --- which is `start` in this case --- with its value in that row.

```{r}
tibble(start = c(3, 2.5, 6)) %>% 
  mutate(new = map_dbl(start, ~ . + 1))
```

The `~ ` shorthand is very convenient once you get used to it.  Let's see it in an example. We'll use the `weather` dataset in the `nycflights13` package. Let's say that we want to return a list-column with all the recorded temperatures in a day in Celsius.

First, let's wrangle the data so each observation is a day rather than an hour.  We'll create a list column `temps_F` that consists of all the temperatures recorded that day at a particular origin.

```{r weather_temps, eval = FALSE}
weather %>%
  group_by(origin, year, month, day) %>%
  summarize(temps_F = list(c(temp)))
```

Now that we have a list column, we can use it as the input to `map()`, outputting another list column.  Let's say we wanted a new list column, `temps_C`, which records the temperature in Celsius:

```{r temps_C, eval = FALSE}
weather %>%
  group_by(origin, year, month, day) %>%
  summarize(temps_F = list(c(temp))) %>%
  mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9))
```

### Custom functions using `map_*` functions

Let’s practice `map_*` functions and list columns! We will give step by step instructions; we recommend that you follow along so that you understand how each part works.  We will introduce writing anonymous functions in this section. We'll also be introducing some important conditional functions (`ifelse()`, `any()`, `all()`, and `case_when()`). 

a. Write a function called `roll_dice()`, which will throw a pair of dice as many times as the user specifies.

<!-- EC: Added a few more intermediate steps in creating roll_dice() -->

We will begin by creating a minimally viable function, `starter_dice()`, which throws one die one time. 

```{r starter_dice}
starter_dice <- function() sample(1:6, 1)
```

From here, we can add a formal argument `n`, which requires the number of dice to be specified. Let's set the default value of `n` to 1. However, if more than one dice is thrown, we want to make sure that any number from `1-6` has the chance of being returned. To do so, make sure the `replace` argument in `sample()` is set to `TRUE`. Now we have generalized the function `starter_dice()`!

```{r starter_dice_generalized}
starter_dice <- function(n = 1) sample(1:6, n, replace = TRUE)
```

Now let's create an intermediate function `add_dice()` which throws n dice and adds the results:

```{r add_dice}
add_dice <- function(n = 1) {
  stopifnot(is.numeric(n))
  sum(sample(1:6, n, replace = TRUE))
}
```

Next, we will create `roll_dice()`, which calls `add_dice(n = 2)` as many times as the user specifies:

```{r roll_dice}
roll_dice <- function(n = 1) {
  stopifnot(is.numeric(n))
  map_int(rep(2, n), add_dice)
}
```

`rep()` is a useful input to `map_*` when you want to call a function with the same input multiple times. `rep(2, n)` creates a vector of length n where every element is 2. We use that as the input to `map_int()` because we want the input to `add_dice()` to be 2 every time (we are always throwing a pair of dice) and we want to perform the operation n times, chosen by the user.

b. Create a tibble named `x` with one variable: `throws`. `throws` is a list column, each element of which is three throws of the dice pair, i.e., the result of calling `roll_dice(n = 3)`.  Thus, our tibble will have ten rows and two columns.

```{r throw_dice}
x <- tibble(throws = map(rep(3, 10), roll_dice))
```

c. Add a variable to `x` called `first_seven` which is TRUE if the first roll in `throws` is a 7.

```{r first_seven, message = FALSE}
library(magrittr)

# The magrittr package allows us to use %<>%, the compound assignment pipe
# operator, which (as its name suggests) both assigns and pipes

x %<>% 
  mutate(first_seven = map_lgl(throws, ~ ifelse(.[[1]] == 7, TRUE, FALSE)))
```

We see here that `[[1]]` is how we extract the first element of a list, just like `[1]` extracts the first element of an atomic vector.  `ifelse()` takes as its first argument a condition; if it is TRUE it returns the second argument (here TRUE) and if not the third (here FALSE).

d. Add a variable to `x` called `a_winner` which is TRUE if at least one of the three throws is a 7 or an 11 and is FALSE otherwise.

```{r a_winner}
x %<>% 
  mutate(a_winner = map_lgl(throws, ~ ifelse(any(c(7, 11) %in% .), TRUE, FALSE)))
```

Here we use `any()`: `any()` checks if any element in its input is TRUE. So, let's say a particular throw was 4, 6, and 7: `c(7, 11) %in% c(4, 6, 7)` returns the vector `TRUE FALSE` (since 7 is in the vector but 11 isn't); then, `any(c(7, 11) %in% c(4, 6, 7))` returns TRUE because one of the conditions is TRUE.

e. Run `str()` on `x` and show the results.

```{r str_x}
str(x)
```

f. Calculate how “surpised” you should be if someone rolls three winners in a row. First, create a tibble with 10,000 rows. Include a `throws` list column with three throws of our dice, just as in part a). Second, create a column called `perfection` which is TRUE if all three of the throws are either 7 or 11.

```{r surprised}
surprised <- tibble(throws = map(rep(3, 10000), roll_dice)) %>% 
  mutate(perfection = map_lgl(throws, ~ ifelse(all(. %in% c(7, 11)), TRUE, FALSE)))

surprised %>%
  pull(perfection) %>%
  mean()
```

Here, we use `all()`, which has the same structure as `any()`, but checks whether *all* the elements in the input are TRUE.

Approximately 1.1% of three rolls of a pair of fair dice are all equal to either 7 or 11.

g. Your friend proposes the following bet. You will roll a pair of fair dice 10 times. Side A gets the second highest of the first 4 rolls. Side B gets 1 plus the the median of the remaining 6 rolls. Which side is more likely to win?  What's the chance of a tie?

```{r a_vs_b}
bet <- tibble(throws = map(rep(10, 10000), roll_dice)) %>% 
  mutate(A = map_dbl(throws, ~ sort(.[1:4])[3]),
         B = map_dbl(throws, ~ 1 + median(.[5:10])),
         winner = case_when(A > B ~ "A",
                            B > A ~ "B",
                            TRUE ~ "tie"))

case_when(sum(bet$winner == "A") > sum(bet$winner == "B") ~ "A",
          sum(bet$winner == "B") > sum(bet$winner == "A") ~ "B",
          TRUE ~ "Same # Victories")

sum(bet$winner == "tie")/length(bet$winner)
```

You can think of `case_when()` as a generalized `ifelse()`.  The syntax is a little more complicated.  Each expression is followed by `~` and what should be returned if that expression is TRUE.  The final expression, TRUE, is always true, and thus is the residual category if none of the above expression are TRUE.  Thus, the second `case_when()` in our above code will print "A" if A is the winner in more replications than B, "B" if B is the winner more than A, and "Same # Victories" otherwise.

Side B is more likely to win.

The chance of a tie is approximately 11%.
